! Last Change:  2020-11-12 17:43:44
!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels.  This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths.  It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!
!--------------------------------------------------------------------
program EcoSLIM
use mpi
use cudafor
use mrand
use thrust
use particle_loop
use mpiDeviceUtil
use variable_list
use hdf5_file_read
use hdf5_file_write
use subdomain_bound
use create_subdomain
use exchange_particles

implicit none

integer,parameter:: tPB = 256 ! block size, is it the larger the better?
integer:: rank, t_rank, ierr, status(MPI_STATUS_SIZE)
integer:: n_proc ! this is the total MPI processes and t_rank = n_proc - 1
integer:: fh0, fh1, fh2, fh3, fh4
character(len=MPI_MAX_PROCESSOR_NAME):: hostname
character(200):: loadf, restartf, exitedf, logf, message
integer:: deviceID, namelength, np_active_log
integer(MPI_OFFSET_KIND):: offset
!--------------------------------------------------------------------
integer:: np_ps, add_f, p_redis, path, nsub, cycle_f
! ??? probably these above line is not necessary anymore.
integer,allocatable:: nump(:),PME_tot(:,:,:),subid(:)
integer:: np_lo, np_ln, np_ro, np_rn
!--------------------------------------------------------------------
! IO control
! ipwrite controls an ASCII, .3D particle file not recommended due to poor performance
! this is left as a compiler option, currently disabled
!!!!!!
! icwrite controls VTK, binary grid based output where particle masses, concentrations,
! ages are mapped to a grid and written every N timesteps.  This is the most effiecient output
! but loses some accuracy or flexbility becuase individual particle locations are aggregated
! to the grid
!!!!!!!
! ibinpntswrite controls VTK, binary output of particle locations and attributes.  This is much faster
! than the .3D ASCII output but is still much slower than grid based output.  It provides the most
! information as particle locations are preserved
interface
subroutine vtk_write(time,x,conc_header,ixlim,iylim,izlim,icycle,n_constituents,pnts,vtk_file)
    real*8                 :: time
    real*8                 :: x(:,:,:,:)
    character (len=20)     :: conc_header(:)
    integer*4              :: ixlim
    integer*4              :: iylim
    integer*4              :: izlim
    real*8                 :: dx
    real*8                 :: dy
    real*8                 :: dz(izlim)
    real*8                 :: pnts(:,:)
    integer                :: icycle
    integer                :: n_constituents
    character (len=200)    :: vtk_file
end subroutine vtk_write

subroutine vtk_write_points(p,np_active, np,icycle,vtk_file,dx,dy,nx,ny,maxz,dem)
    real*8                 :: p(:,:)
    integer                :: icycle
    integer*4              :: np_active
    integer*4              :: np
    integer                :: n_constituents
    real*8                 :: dx
    real*8                 :: dy
    integer*4              :: nx
    integer*4              :: ny
    real*8                 :: maxz
    real*8                 :: dem(:,:)
    character (len=200)    :: vtk_file
end subroutine vtk_write_points
end interface

!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)

!--------------------------------------------------------------------
    ! This part has to be thought again for extra processes to do I/O
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'EcoSLIM_Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    ! I think how to do timing is also a new question due to asyn.
    ! Set up timing
    Total_time1 = 0.d0
    Total_time2 = 0.d0
    t1 = 0.d0
    t2 = 0.d0
    IO_time_read = 0.d0
    IO_time_write = 0.d0
    parallel_time = 0.d0
    sort_time = 0.d0

!--------------------------------------------------------------------
    ! (2) Read inputs, set up domain, write the log file, and
    ! initialize particles
!--------------------------------------------------------------------
    ! Note: The following file numbers refer to
    !
    !       - #10: slimin.txt
    !       - #11: runname_log.txt
    !       - #12: runname_particle.3D (visualizes particles in VisIT)
    !       - #13: runname_endparticle.txt

    ! open SLIM input .txt file
    open (10,file='slimin.txt')

    ! read SLIM run name
    read(10,*) runname

    ! read ParFlow run name
    read(10,*) pname

    ! read DEM file name
    read(10,*) DEMname

    if(rank == 0) then
        ! open/create/write the output log.txt file. If doesn't exist, it's created.
        open(11,file=trim(runname)//'_log.txt')
        write(11,*) '### EcoSLIM Log File'
        write(11,*)
        write(11,*) 'run name:',trim(runname)
        write(11,*)
        write(11,*) 'ParFlow run name:',trim(pname)
        write(11,*)
        if (DEMname /= '') then
        write(11,*) 'ParFlow DEM name:',trim(DEMname)
        else
        write(11,*) 'Not reading ParFlow DEM'
        end if
        write(11,*)
    endif ! rank = 0, write logfile through channel 11

    ! read domain number of cells and number of particles to be injected
    read(10,*) nx
    read(10,*) ny
    read(10,*) nz
    read(10,*) nCLMsoil
    read(10,*) ppx
    read(10,*) qqy
    ! read in number of particles for IC (if np_ic = -1 then restart from a file)
    read(10,*) np_ic

    ! read in the number of particles total
    read(10,*) np

    if(rank == 0) then
        ! check to make sure we don't assign more particles for IC than we have allocated
        ! in total
        if (np_ic > np) then
        write(11,*) 'warning NP_IC greater than IC'
        np = np_ic
        end if

        ! write nx, ny, nz, and np in the log file
        write(11,*) 'Grid information'
        write(11,*) 'nx:',nx
        write(11,*) 'ny:',ny
        write(11,*) 'nz:',nz
        write(11,*)
        write(11,*) 'Particle IC Information'
        write(11,*) 'np IC:',np_ic
        if (np_ic == -1) &
        write(11,*) 'Reading particle restart file:',trim(runname)//'_particle_restart.bin'
        write(11,*) 'np:',np
    endif ! rank=0

    ! nCLMsoil = 10 ! number of CLM soil layers over the root zone !this doesn't matter
    nzclm = 13 + nCLMsoil ! CLM output is 13+nCLMsoil layers for different variables not domain NZ,
            !  e.g. 23 for 10 soil layers (default) and 17 for 4 soil layers (Noah soil
            ! layer setup)
    n_constituents = 9

!--------------------------------------------------------------------
    call gridinfo(nx,ny,ppx,qqy,rank)
    ! then we get subdomain info
!--------------------------------------------------------------------
    ! allocate arrays
    allocate(DEM(-buff+1:nnx1+buff,-buff+1:nny1+buff))
    allocate(dz(nz), dz2(nz))
    allocate(Vx(-buff+1:nnx1+1+buff,-buff+1:nny1+buff,nz), &
            Vy(-buff+1:nnx1+buff,-buff+1:nny1+1+buff,nz), &
            Vz(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz+1))
    allocate(Saturation(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
            Porosity(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
            EvapTrans(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    Ind(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
    allocate(EvapTrans_da(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
    allocate(CLMvars(-buff+1:nnx1+buff,-buff+1:nny1+buff,nzclm))
    allocate(C(n_constituents,-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
    allocate(conc_header(n_constituents))

    ! Intialize everything to Zero
    Vx = 0.0d0
    Vy = 0.0d0
    Vz = 0.0d0

    Saturation = 0.0d0
    Porosity = 0.0d0
    EvapTrans = 0.0d0
    EvapTrans_da = 0.0d0
    C = 0.0d0

    allocate(out_np_cpu(1),ET_np_cpu(1))
    allocate(out_age_cpu(1),out_mass_cpu(1),out_comp_cpu(3))
    allocate(ET_age_cpu(1),ET_mass_cpu(1),ET_comp_cpu(3))

    allocate(neigh_list(t_rank))
!-------------------------------------
    ! allocate arrays on GPU
    allocate(dz_de(nz))
    allocate(Vx_de(-buff+1:nnx1+1+buff,-buff+1:nny1+buff,nz), &
            Vy_de(-buff+1:nnx1+buff,-buff+1:nny1+1+buff,nz), &
            Vz_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz+1), &
            Ind_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
    allocate(Saturation_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
            Porosity_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
            EvapTrans_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                Zone_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
    allocate(C_de(n_constituents,-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))

    allocate(out_np_de(1),ET_np_de(1))
    allocate(out_age_de(1),out_mass_de(1),out_comp_de(3))
    allocate(ET_age_de(1),ET_mass_de(1),ET_comp_de(3))

    allocate(d_isValid(np))

!-------------------------------------
    ! read dx, dy as scalars
    read(10,*) dx
    read(10,*) dy
    ! read dz as an array
    read(10,*) dz(1:nz)
    dz2 = dz
    ! read in (constant for now) ParFlow dt
    read(10,*) pfdt
    ! read in parflow start and stop times
    read(10,*) pft1
    read(10,*) pft2
    read(10,*) tout1
    read(10,*) n_cycle
    read(10,*) add_f

    pfnt=n_cycle*(pft2-pft1+1)
    outkk = tout1 + 1

    ! set ET DT to ParFlow one and allocate ET arrays accordingly
    ET_dt = pfdt
    allocate(PET_balance(pfnt,2))
    allocate(PET_balance_da(pfnt,2))
    PET_balance = 0.0d0
    PET_balance_da = 0.0d0

    ! clear out output particles
    npout = 0  !!! no use but need this info through a new way

    ! IO control, each value is a timestep interval, e.g. 1= every timestep, 2=every other, 0 = no writing
    read(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
    read(10,*) ibinpntswrite  !  controls VTK, binary output of particle locations and attributes
    read(10,*) etwrite        !  controls ASCII ET output
    read(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
                            ! ages are mapped to a grid and written every N timesteps

    ! allocate and assign timesteps
    ! this can be a scalar and update every timestep
    allocate(Time_Next(pfnt))
    Time_Next=0.d0
    do kk = outkk, pfnt
        Time_Next(kk) = float(kk)*pfdt
    end do

    Time_first = float(outkk-1)*pfdt

    ! read in velocity multiplier
    read(10,*) V_mult

    ! do we read in clm evap trans?
    read(10,*) clmtrans
    ! do we read in clm output file?
    read(10,*) clmfile

    ! read in IC number of particles for flux
    read(10,*) iflux_p_res

    ! read in density h2o
    read(10,*) denh2o

    ! read in diffusivity
    ! moldiff = (1.15e-9)*3600.d0
    read(10,*) moldiff

    ! fraction of dx/Vx
    read(10,*) dtfrac

    if(rank == 0) then
        ! wite out log file
        write(11,*)
        write(11,*) 'Grid Dimensions'
        write(11,'(" dx:",e12.5)') dx
        write(11,'(" dy:",e12.5)') dy
        write(11,'(" dz:",*(e12.5,", "))') dz(1:nz)
        write(11,*)
        write(11,*) 'Timestepping Information'
        write(11,'(" ParFlow delta-T, pfdt:",e12.5)') pfdt
        write(11,'(" ParFlow timesteps, pfnt:",i12)') pfnt
        write(11,'(" ParFlow start step, pft1:",i12)') pft1
        write(11,'(" ParFlow end step, pft2:",i12)') pft2
        write(11,'(" Output step start:",i12)') outkk
        write(11,'(" Time loops, cycles, n_cycle:",i12)') n_cycle
        write(11,'(" Total time steps:",i12)') pfnt

        write(11,*)
        write(11,*) 'V mult: ',V_mult,' for forward/backward particle tracking'
        write(11,*) 'CLM Trans: ',clmtrans,' adds / removes particles based on LSM fluxes'
        write(11,*)
        write(11,*) 'Physical Constants'
        write(11,*) 'denh2o: ',denh2o,' M/L^3'
        write(11,*) 'Molecular Diffusivity: ',moldiff,' '
        !write(11,*) 'Fractionation: ',Efract,' '
        write(11,*)
        write(11,*) 'Numerical Stability Information'
        write(11,'(" dtfrac: ",e12.5," fraction of dx/Vx")') dtfrac
    endif

    read(10,*) nind
    read(10,*) Indname
    if(rank == 0) then
        write(11,*)
        write(11,*) 'Indicator File'
        write(11,*) nind, 'Indicators'
    endif

    ! end of SLIM input
    close(10)

    Ind = 1.0d0
    ! read in Indicator file
    if (nind>0) then
    if (Indname /= '') then
        fname = trim(adjustl(Indname))
        call pfb_read(Ind,fname,nx,ny,nz)
        if(rank == 0) &
        write(11,*) 'Read Indicator File:', fname
    else
        if(rank == 0) &
        write(11,*) 'WARNING: indicator flage >0 but no indicator file provided'
    end if ! Ind
    end if

    if(rank == 0) then
        flush(11)
        close(11)
    endif

    allocate(P(np,17+nind*2))
    P = 0.d0    ! clear out all particle attributes
    P(1:np,7:9) = 1.d0
    ! make all particles active to start with and original from 1 = GW/IC

!-------------------------------------
    call global_xyz(nx, ny, nz, dx, dy, dz, rank, &
    Xgmin, Xgmax, Ygmin, Ygmax, Zgmin, Zgmax, fh1, &
    loadf, restartf, exitedf, logf)

    call local_xyz(nnx1, nny1, nz, dx, dy, dz, buff, &
    Xmin, Xmax, Ymin, Ymax, Zmin, Zmax)

    call DEM_for_visual(DEM,DEMname,ix1,iy1,nnx1,nny1,nz)

!-------------------------------------
    ! Read porosity values from ParFlow .pfb file
    fname=trim(adjustl(pname))//'.out.porosity.pfb'
    call pfb_read(Porosity,fname,nx,ny,nz)

    ! Read the initial Saturation from ParFlow
    if(tout1 == 0) then
        pfkk = pft1 - 1
        write(filenum,'(i5.5)') pfkk
        fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Saturation,fname,nx,ny,nz)
        ! this is not necessary if restart
    end if

    ! copy necessary data from host to device
    ! this part together with allocation should be done after update subdomain
    dz_de = dz2 ! can't understand why dz doesn't work.
    Ind_de = Ind
    Porosity_de = Porosity
    Saturation_de = Saturation
    Zone_de = Zone
    P_de = P
    !!! initialization pf P_de, probably need a new kernel
    ! think about initialization of some device variables
    ! since they are not updated each timestep.

    call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
    tPB>>>(Zone_de,nnx1,nny1,buff,d_isValid,rank,t_rank)
    ! ix2, iy2, nnx2, and nny2 are only used for read files

    neigh_list = d_isValid(1:t_rank)
    ! don't need to do this every timestep. Only after update of decomposition
    if(sum(neigh_list) > 0) allocate(N_recv_all(sum(neigh_list)))

    ! Intialize cuRand device API and this part need to check through again.
    allocate(h(nnx1*nny1*nz))
    call createRand_init<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>>(nx,ny,nz, &
    rank,np_ic,nnx1,nny1)
    ! each subdomain has different dimension, so we set a fixed offset of h using the whole domain

!-------------------------------------
    ! Define initial particles' locations and mass
    if (np_ic > 0)  then
        np_active = 0
        pid = 0
        if (np_active + np_ic*nnx1*nny1*nz >= np ) then
            write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
            call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

            write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
            call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
            ! now there is no restart after 9090

            goto 9090
        endif

        call add_init_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
        tPB >>> (P_de,C_de,Saturation_de,Porosity_de, &
        np_ic,nind,denh2o,outkk,nnx1,nny1,nz,ix1,iy1,dx,dy,dz)

        np_active = np_ic*nnx1*nny1*nz
        pid = np_ic*nnx1*nny1*nz

    ! if np_ic = -1 then we read a restart file
    else if (np_ic == -1) then

        call read_restarts (fh1,fh2,rank,restartf,np,nind,P)
        P_de = P

    end if

    ! Define initial particles' locations by surface water
    if (np_ic < -1)  then
        np_active = 0
        pid = np_active

        if(np_active + abs(np_ic)*nnx1*nny1 >= np) then
            write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
            call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

            write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
            call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

            goto 9090
        end if

        call river_init_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
        tPB >>> (P_de,C_de,Saturation_de,Porosity_de, &
        np_ic,np_active,pid,nind,nnx1,nny1,nz,dx,dy,dz, &
        pfdt,denh2o,outkk)

        np_active = abs(np_ic)*nnx1*nny1
        pid = abs(np_ic)*nnx1*nny1

    end if ! river IC

! Write out intial concentrations
! normalize ages by mass
! call MPI_ALLReduce(MPI_IN_PLACE,C,n_constituents*nx*ny*nz,MPI_DOUBLE_PRECISION, &
!                    MPI_SUM,MPI_COMM_WORLD,ierr)
! ! this may not be used for this version, we will do parallel output

where (C(3,:,:,:)>0.0)  C(2,:,:,:) = C(2,:,:,:) / C(3,:,:,:)
where (C(3,:,:,:)>0.0)  C(4,:,:,:) = C(4,:,:,:) / C(3,:,:,:)

! here need hdf5 output for initial C array

! ! Set up output options for VTK grid output
! ! icwrite = 1
! vtk_file=trim(runname)//'_cgrid'
! conc_header(1) = 'Concentration'
! conc_header(2) = 'Age'
! conc_header(3) = 'Mass'
! conc_header(4) = 'Comp'
! conc_header(5) = 'Delta'
! conc_header(6) = 'ET_Npart'
! conc_header(7) = 'ET_Mass'
! conc_header(8) = 'ET_Age'
! conc_header(9) = 'ET_Comp'
!
! if(icwrite > 0 .and. rank == 0)  &
! call vtk_write(Time_first,C,conc_header,nx,ny,nz,0,n_constituents,Pnts,vtk_file)

! keep the following output as it is. we can figure out a better way to handle them later.

call MPI_FILE_OPEN(MPI_COMM_SELF,logf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                   MPI_INFO_NULL,fh4,ierr)
    write(message,'(a,a)') ' **** Transient Simulation Particle Accounting ****', new_line(' ')
    call MPI_FILE_WRITE(fh4, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
    write(message,'(a,a)') ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
                             ETOut NP_PrecipIn NP_ETOut NP_QOut NP_active_old NP_filtered', new_line(' ')
    call MPI_FILE_WRITE(fh4, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

! open exited partile file and write header
call MPI_FILE_OPEN(MPI_COMM_SELF,exitedf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                   MPI_INFO_NULL,fh3,ierr)

if(rank == 0) then
    if(ipwrite < 0) then
    ! open/create/write the 3D output file which will write particles out each timestemp, very slowly in parallel
    open(214,file=trim(runname)//'_total_particle_trace.3D')
    write(214,*) 'X Y Z TIME'
    end if !! ipwrite < 0

    open(13,file=trim(runname)//'_ET_output.txt')
    write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

    open(15,file=trim(runname)//'_flow_output.txt')
    write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

    open(16,file=trim(runname)//'_PET_balance.txt')
    write(16,*) 'TIME P[kg] ET[kg]'
endif

!--------------------------------------------------------------------
! (3) For each timestep, loop over all particles to find and
!     update their new locations
!--------------------------------------------------------------------
! loop over timesteps
!
pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1
do kk = outkk, pfnt

! reset ParFlow counter for cycles
        if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1

        ! adjust the file counters
        pfkk = pfkk + 1

        ! Read the velocities computed by ParFlow
        write(filenum,'(i5.5)') pfkk
!----------------------------------------
        ! wait for hdf5 ready
        fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vx,fname,nx+1,ny,nz)

        fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vy,fname,nx,ny+1,nz)

        fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vz,fname,nx,ny,nz+1)

        fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Saturation,fname,nx,ny,nz)

        if (clmtrans) then
        ! Read in the Evap_Trans
        fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(EvapTrans,fname,nx,ny,nz)

        if (mod((kk-1),add_f) == 0) EvapTrans_da = 0.d0
        where (EvapTrans > 0.d0) EvapTrans_da = EvapTrans_da + EvapTrans

        ! check if we read full CLM output file
        if (clmfile) then
        ! Read in CLM output file @RMM to do make this input
        fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.pfb'
        call pfb_read(CLMvars,fname,nx,ny,nzclm)

        end if
        end if
!----------------------------------------
        ! Determine whether to perform forward or backward patricle tracking
        Vx = Vx * V_mult
        Vy = Vy * V_mult
        Vz = Vz * V_mult

        out_age_cpu = 0.d0; out_mass_cpu = 0.d0; out_comp_cpu = 0.d0; out_np_cpu = 0
        et_age_cpu  = 0.d0; et_mass_cpu  = 0.d0; et_comp_cpu  = 0.d0; et_np_cpu  = 0
        mean_age    = 0.d0; mean_comp    = 0.d0; total_mass   = 0.d0

        Vx_de = Vx; Vy_de = Vy; Vz_de = Vz
        Saturation_de = Saturation; EvapTrans_de = EvapTrans
        CLMvars_de = CLMvars(:,:,11) !probably this can be deallocated after add of particles
        ! If decomposition of the domain is updated, the matrix on GPU should be
        ! deallocated and then allocated after the dynamic DDC.
        ! Though hdf5 is not ready, at least we can assume the shape of each
        ! variable in the following parts.
        ! Now just think about the hourly add of particles.
        out_age_de = out_age_cpu; out_mass_de = out_mass_cpu
        out_comp_de = out_comp_cpu; out_np_de = out_np_cpu
        et_age_de = et_age_cpu; et_mass_de = et_mass_cpu
        et_comp_de = et_comp_cpu; et_np_de = et_np_cpu
        mean_age   = mean_age_de; mean_comp    = mean_comp_de; total_mass   = total_mass_de
        C = 0.d0; C_de = C
        ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
        ! the following add of particles.

        if (clmtrans) then
            if (np_active < np) then

                call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
                tPB >>> (EvapTrans_de,PET_balance_de,d_isValid,nnx1,nny1,nz, &
                dx,dy,dz,pfdt,denh2o)
                ! d_isValid has the length as P or of np which is assigned in input script
                ! so nnx1**nny1*nz should be smaller than np
                ! we don't care about the buffer zone for add of particles
                ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                ! in main program and pass arguments to other subroutines.

                PET_balance = PET_balance_de
                ! print to log file, or PET_balance can be a long array and do print at last

                ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                call thrustscan(d_isValid,nnx1*nny1,d_indices)

                i_added_particles = d_indices(nnx1*nny1) * iflux_p_res ! should be legal?
                if(np_active + i_added_particles >= np) then
                    write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                    write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                    goto 9090
                endif

                call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                tPB >>> (d_indices,P_de,EvapTrans_de,CLMvars_de,Ind_de, &
                iflux_p_res,np_active,pid,nind,nnx1,nny1,nz,dx,dy,dz,pfdt, &
                denh2o,outkk)

                np_active = np_active + d_indices(nnx1*nny1) * iflux_p_res
                pid = pid + d_indices(nnx1*nny1) * iflux_p_res

            end if
        end if

        ! this has to be checked later.
        ! call MPI_ALLReduce(MPI_IN_PLACE, PET_balance_da(kk,:), 2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, PET_balance(kk,:),    2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)

!-------------------------------------
        ! particle loop here
        call particles_independent <<< ceiling(dble(np_active)/tPB),tPB >>> ( &
            P_de,C_de,dz_de,EvapTrans_de,Vx_de,Vy_de,Vz_de,Saturation_de, &
            Porosity_de,Ind_de,zone_de,out_age_de,out_mass_de,out_comp_de, &
            et_age_de,et_mass_de,et_comp_de,out_np_de,et_np_de, &
            kk,np_active,nz,nind,pfdt,moldiff,dx,dy,denh2o,dtfrac, &
            ix1,iy1,buff,reflect,nnx1,nny1,rank, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin,ymin,zmin,xmax,ymax,zmax)
!-------------------------------------
        ! particle exchange here
        ! neigh_list is an 1D array of size t_rank. If an element is 1, it has this neighbor.
        ! sum(neigh_list) > 0, this means there must be at least one neighbor.
        ! sum(neigh_list) > 0 could be put in main to determine if call this subroutine
        if(sum(neigh_list) > 0) call MPI_particle_exchange(rank,t_rank,nind,tPB,np_active)
!-------------------------------------
        if(N_send > 0) then
            allocate(P_exit(N_send,17+2*nind))
            P_exit = P_send; deallocate(P_send)
        end if
        ! copy exited particles back and output later.
        ! considering asyn data copy.

        C              = C_de
        out_age_cpu    = out_age_de;    out_mass_cpu   = out_mass_de
        out_comp_cpu   = out_comp_de;   out_np_cpu     = out_np_de
        et_age_cpu     = et_age_de;     et_mass_cpu    = et_mass_de
        et_comp_cpu    = et_comp_de;    et_np_cpu      = et_np_de

!-------------------------------------
        ! for such reduce, we can use a specific MPI process to do.
        call MPI_ALLReduce(MPI_IN_PLACE, ET_age_cpu,  1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, ET_comp_cpu, 3,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, ET_mass_cpu, 1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, ET_np_cpu,   1,MPI_INTEGER,         MPI_SUM,MPI_COMM_WORLD,ierr)

        call MPI_ALLReduce(MPI_IN_PLACE, Out_age_cpu, 1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, Out_comp_cpu,3,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, Out_mass_cpu,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        call MPI_ALLReduce(MPI_IN_PLACE, Out_np_cpu,  1,MPI_INTEGER,         MPI_SUM,MPI_COMM_WORLD,ierr)

        ! call MPI_ALLReduce(MPI_IN_PLACE,C,n_constituents*nx*ny*nz,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        ! I think here it is not necessary. You just need to write into hdf5 if necessary.

        call MPI_Reduce(np_active, np_active_log, 1, MPI_INTEGER, MPI_SUM, 0, MPI_COMM_WORLD, ierr)

        if(rank == n_proc) then
            write(12,*) np_active_log, T2-T1, redis_time
            flush(12)

            if (ET_mass_cpu(1) > 0 ) then
                ET_age_cpu(1) = ET_age_cpu(1)/ET_mass_cpu(1)
                ET_comp_cpu(1) = ET_comp_cpu(1)/ET_mass_cpu(1)
                ET_comp_cpu(2) = ET_comp_cpu(2)/ET_mass_cpu(1)
                ET_comp_cpu(3) = ET_comp_cpu(3)/ET_mass_cpu(1)
            end if
            write(13,'(6(e12.5),i12)') float(kk)*ET_dt, ET_age_cpu(1), ET_comp_cpu(1), &
                                    ET_comp_cpu(2), ET_comp_cpu(3), ET_mass_cpu(1), &
                                    ET_np_cpu(1)
            flush(13)

            if (Out_mass_cpu(1) > 0 ) then
                Out_age_cpu(1) = Out_age_cpu(1)/Out_mass_cpu(1)
                Out_comp_cpu(1) = Out_comp_cpu(1)/Out_mass_cpu(1)
                Out_comp_cpu(2) = Out_comp_cpu(2)/Out_mass_cpu(1)
                Out_comp_cpu(3) = Out_comp_cpu(3)/Out_mass_cpu(1)
            end if
            write(15,64) float(kk)*ET_dt, Out_age_cpu(1), Out_comp_cpu(1), &
                        Out_comp_cpu(2), Out_comp_cpu(3), Out_mass_cpu(1), &
                        Out_np_cpu(1)
            flush(15)

            write(16,'(5(e12.5,2x))') float(kk)*ET_dt, PET_balance(kk,1), PET_balance(kk,2), &
                                    PET_balance_da(kk,1), PET_balance_da(kk,2)
            flush(16)

            if (total_mass > 0.0d0)  then
                mean_age =  mean_age / total_mass
                mean_comp = mean_comp / total_mass
                mean_mass = total_mass / float(np_active2)
            end if
            write(message,'(3(i10),7(1x,e12.5,1x),3(i8),2(i12),a)') kk,pfkk,outkk,Time_Next(kk),mean_age,mean_comp, &
                                                      mean_mass,total_mass,PET_balance(kk,1), &
                                                      PET_balance(kk,2),i_added_particles,ET_np_cpu(1), &
                                                      Out_np_cpu(1),np_active,np_active2,new_line(' ')
            call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)
        endif

write(filenumout,'(i10.10)') kk

! write all active particles at concentration in ASCII VisIT 3D file format
! as noted above, this option is very slow compared to VTK binary output
if(ipwrite > 0) then
if(mod(kk,ipwrite) == 0)  then
! open/create/write the 3D output file
open(14,file=trim(runname)//'_transient_particle.'//trim(adjustl(filenumout))//'.3D')
write(14,*) 'X Y Z TIME ID'
do ii = 1, np_active
if (P(ii,8) == 1) write(14,61) P(ii,1), P(ii,2), P(ii,3), P(ii,4), P(ii,13+2*nind)
end do
close(14)
end if
end if

! normalize ages by mass
!where (C(3,:,:,:)>0.0)  C(2,:,:,:) = C(2,:,:,:) / C(3,:,:,:)
!where (C(3,:,:,:)>0.0)  C(4,:,:,:) = C(4,:,:,:) / C(3,:,:,:)
!where (C(3,:,:,:)>0.0)  C(5,:,:,:) = C(5,:,:,:) / C(3,:,:,:)

! normalize ET ages by mass
!where (C(7,:,:,:)>0.0)  C(8,:,:,:) = C(8,:,:,:) / C(7,:,:,:)
!where (C(7,:,:,:)>0.0)  C(9,:,:,:) = C(9,:,:,:) / C(7,:,:,:)

! Write gridded ET outputs to text files
if(etwrite > 0) then
if (mod(kk,etwrite) == 0) then
! open/create/write the 3D output file
open(14,file=trim(runname)//'_ET_summary.'//trim(adjustl(filenumout))//'.txt')
write(14,*) 'X Y Z ET_npart, ET_mass, ET_age, ET_comp, EvapTrans_Rate, Saturation, Porosity'
do i = 1, nx
do j = 1, ny
do k = 1, nz
  if (EvapTrans(i,j,k) < 0.0d0)   &
  write(14,'(3(i6), 7(e13.5))')  i, j, k, C(6,i,j,k), C(7,i,j,k), C(8,i,j,k), &
       C(9,i,j,k), EvapTrans(i,j,k), Saturation(i,j,k), Porosity(i,j,k)
end do
end do
end do
close(14)
end if
end if

! write grid based values ("concentrations")
vtk_file=trim(runname)//'_cgrid'
if(icwrite > 0)  then
    !if(mod(kk,icwrite) == 0)  &
    !call vtk_write(Time_Next(kk),C,conc_header,nx,ny,nz,kk,n_constituents,Pnts,vtk_file)
    if(mod(kk,icwrite) == 0 .and. rank == 0)  then
        fname=trim(runname)//'.out.C1.'//trim(adjustl(filenumout))//'.pfb'
        call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,1)
        fname=trim(runname)//'.out.C2.'//trim(adjustl(filenumout))//'.pfb'
        call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,2)
        fname=trim(runname)//'.out.C3.'//trim(adjustl(filenumout))//'.pfb'
        call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,3)
        fname=trim(runname)//'.out.C4.'//trim(adjustl(filenumout))//'.pfb'
        call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,4)
        fname=trim(runname)//'.out.C5.'//trim(adjustl(filenumout))//'.pfb'
        call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,5)
        !fname=trim(runname)//'.out.C6.'//trim(adjustl(filenumout))//'.pfb'
        !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,6)
        !fname=trim(runname)//'.out.C7.'//trim(adjustl(filenumout))//'.pfb'
        !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,7)
        !fname=trim(runname)//'.out.C8.'//trim(adjustl(filenumout))//'.pfb'
        !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,8)
        !fname=trim(runname)//'.out.C9.'//trim(adjustl(filenumout))//'.pfb'
        !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,9)
    end if
end if

! write binary particle locations and attributes
vtk_file=trim(runname)//'_pnts'
if(ibinpntswrite > 0)  then
if(mod(kk,ibinpntswrite) == 0)  &
call vtk_write_points(P,np_active,np,kk,vtk_file, dx, dy,nx,ny, maxZ,dem)
end if

if (mod(kk,(pft2-pft1+1)/5) == 0 )  then
    call MPI_FILE_OPEN(MPI_COMM_SELF,restartf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                       MPI_INFO_NULL,fh2,ierr)
    call MPI_FILE_WRITE(fh2,np_active,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
    call MPI_FILE_WRITE(fh2,pid,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
    call MPI_FILE_WRITE(fh2,P(1:np_active,1:nind*2+17),np_active*(nind*2+17), &
                        MPI_DOUBLE_PRECISION,MPI_STATUS_IGNORE,ierr)
    call MPI_FILE_CLOSE(fh2, ierr)
end if

! LB criteria: if the difference between max- and min-number of particles over some limitation
if() then
    ! call decomposition
    ! Build neighbor list. do this after a particle loop
    call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
    tPB>>>(zone_de,nnx1,nny1,buff,d_isValid,rank,t_rank)
    neigh_list = d_isValid(1:t_rank)
    ! don't need to do this every timestep. Only after update of decomposition
    if(sum(neigh_list) > 0) allocate(N_recv_all(sum(neigh_list)))
    ! notice: make sure N_recv_all is deallocated before new allocation
    ! used to know how many particles this rank will receive
    ! N_recv_all should be set after update of decomposition.
    ! its size is the number of its neighbors. it will have elements of value 0,
    ! this means that neighbor will not send particles to this rank this time step.
end if

end do !! timesteps and cycles

9090 continue  !! continue statement for running out of particles when assigning precip flux.
               !!  code exits gracefully (writes files and possibly a restart file so the user can
               !!  re-run the simulation)

! close output file
call MPI_FILE_CLOSE(fh3, ierr)
call MPI_FILE_CLOSE(fh4, ierr)

! format statements for particle output
61  FORMAT(4(e12.5))
64  FORMAT(6(e12.5),i12)

if(rank == 0) then
    flush(13)
    close(13)

    if(ipwrite < 0) close(214)

    flush(15)
    close(15)

    flush(16)
    close(16)
endif

        call MPI_BARRIER(MPI_COMM_WORLD, ierr)
        Total_time2 = MPI_Wtime()

        write(message,'(a)') new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a,a)') '###  Execution Finished', new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a)') new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(i10,a,a)') npout,' particles exited the domain via outflow or ET.',new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a)') new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a,a)') 'Simulation Timing and Profiling:', new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'("Total Execution Time (s):",e12.5,a)') Total_time2-Total_time1,new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'("File IO Time Read (s):",e12.5,a)') IO_time_read,new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'("File IO Time Write (s):",e12.5,a)') IO_time_write,new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'("Time Sorting (s):",e12.5,a)') sort_time,new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'("Parallel Particle Time (s):",e12.5,a)') parallel_time,new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a)') new_line(' ')
        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        call MPI_FILE_CLOSE(fh1, ierr)
        call MPI_FINALIZE(ierr)

   end program EcoSLIM

