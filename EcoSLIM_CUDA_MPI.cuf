! Last Change:  2020-11-12 17:43:44
!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels.  This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths.  It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!--------------------------------------------------------------------
! 06/26/2021 GPU version, Chen Yang
!--------------------------------------------------------------------
program EcoSLIM
use mpi
use cudafor
use mrand
use thrust
use utilities
use particle_loop
use mpiDeviceUtil
use variable_list
use hdf5_file_read
use hdf5_file_write
use subdomain_bound
use create_subdomain
use exchange_zone
use exchange_particles

implicit none
!--------------------------------------------------------------------
! IO control
! ipwrite controls an ASCII, .3D particle file not recommended due to poor performance
! this is left as a compiler option, currently disabled
!!!!!!
! icwrite controls VTK, binary grid based output where particle masses, concentrations,
! ages are mapped to a grid and written every N timesteps.  This is the most effiecient output
! but loses some accuracy or flexbility becuase individual particle locations are aggregated
! to the grid
!!!!!!!
! ibinpntswrite controls VTK, binary output of particle locations and attributes.  This is much faster
! than the .3D ASCII output but is still much slower than grid based output.  It provides the most
! information as particle locations are preserved
interface
    subroutine vtk_write(time,x,conc_header,ixlim,iylim,izlim,icycle,n_constituents,pnts,vtk_file)
        real*8                 :: time
        real*8                 :: x(:,:,:,:)
        character (len=20)     :: conc_header(:)
        integer*4              :: ixlim
        integer*4              :: iylim
        integer*4              :: izlim
        real*8                 :: dx
        real*8                 :: dy
        real*8                 :: dz(izlim)
        real*8                 :: pnts(:,:)
        integer                :: icycle
        integer                :: n_constituents
        character (len=200)    :: vtk_file
    end subroutine vtk_write

    subroutine vtk_write_points(p,np_active, np,icycle,vtk_file,dx,dy,nx,ny,maxz,dem)
        real*8                 :: p(:,:)
        integer                :: icycle
        integer*4              :: np_active
        integer*4              :: np
        integer                :: n_constituents
        real*8                 :: dx
        real*8                 :: dy
        integer*4              :: nx
        integer*4              :: ny
        real*8                 :: maxz
        real*8                 :: dem(:,:)
        character (len=200)    :: vtk_file
    end subroutine vtk_write_points
end interface

!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)
    ! t_tank = ppx*qqy + 1
    ! ppx*qqy can be the total number of subdomains
    ! ppx*qqy can also be the number of manage rank
    ! rank number is 0 to ppx*qqy

    ! one more rank for DDC, so the number of sudomains is t_rank-1
    ! be careful about this rank when doing everything
    ! build new communicators for communication
    call MPI_COMM_GROUP(MPI_COMM_WORLD, world_group, ierr)
    manage_ranks = ppx*qqy ! only one rank used to manage
    call MPI_GROUP_EXCL(world_group, 1, manage_ranks, work_group, ierr)
    call MPI_GROUP_INCL(world_group, 1, manage_ranks, manage_group, ierr)
    call MPI_COMM_CREATE(MPI_COMM_WORLD, work_group, work_comm, ierr)
    call MPI_COMM_CREATE(MPI_COMM_WORLD, manage_group, manage_comm, ierr)

!--------------------------------------------------------------------
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'EcoSLIM_Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    ! how to do timing is also a new question due to asyn later.
    ! Set up timing
    Total_time1 = 0.d0
    Total_time2 = 0.d0
    t1 = 0.d0
    t2 = 0.d0
    IO_time_read = 0.d0
    IO_time_write = 0.d0
    parallel_time = 0.d0
    sort_time = 0.d0

!--------------------------------------------------------------------
    ! Read inputs, set up domain, write the log file
    ! no need to exclude the manage rank since only several scalar variables are read
    ! and these scalar variables have already been on the manange rank due to declaration
!--------------------------------------------------------------------
    ! open SLIM input .txt file
    open (10,file='slimin.txt')

    ! read SLIM run name
    read(10,*) runname

    ! read ParFlow run name
    read(10,*) pname

    ! read DEM file name
    read(10,*) DEMname

    if(rank == 0) then
        ! open/create/write the output log.txt file. If doesn't exist, it's created.
        open(11,file=trim(runname)//'_log.txt')
        write(11,*) '### EcoSLIM Log File'
        write(11,*)
        write(11,*) 'run name:',trim(runname)
        write(11,*)
        write(11,*) 'ParFlow run name:',trim(pname)
        write(11,*)
        if (DEMname /= '') then
            write(11,*) 'ParFlow DEM name:',trim(DEMname)
        else
            write(11,*) 'Not reading ParFlow DEM'
        end if
        write(11,*)
    endif ! rank = 0, write logfile through channel 11

    ! read domain number of cells and number of particles to be injected
    read(10,*) nx
    read(10,*) ny
    read(10,*) nz
    read(10,*) nCLMsoil
    read(10,*) ppx
    read(10,*) qqy
    ! read in number of particles for IC (if np_ic = -1 then restart from a file)
    read(10,*) np_ic

    ! read in the number of particles total
    read(10,*) np

    if(rank == 0) then
        ! check to make sure we don't assign more particles for IC than we have allocated
        ! in total
        if (np_ic > np) then
        write(11,*) 'warning NP_IC greater than IC'
        np = np_ic
        end if
        ! write nx, ny, nz, and np in the log file
        write(11,*) 'Grid information'
        write(11,*) 'nx:',nx
        write(11,*) 'ny:',ny
        write(11,*) 'nz:',nz
        write(11,*)
        write(11,*) 'Particle IC Information'
        write(11,*) 'np IC:',np_ic
        if (np_ic == -1) &
        write(11,*) 'Reading particle restart file:',trim(runname)//'_particle_restart.bin'
        write(11,*) 'np:',np
    endif ! rank=0

    ! nCLMsoil = 10 ! number of CLM soil layers over the root zone !this doesn't matter
    nzclm = 13 + nCLMsoil ! CLM output is 13+nCLMsoil layers for different variables not domain NZ,
                          ! e.g. 23 for 10 soil layers (default) and 17 for 4 soil layers (Noah soil
                          ! layer setup)
    n_constituents = 9

    allocate(dz(nz),dz2(nz),dz_de(nz))
    ! have to do this here since the following read of dz

    ! read dx, dy as scalars
    read(10,*) dx
    read(10,*) dy
    ! read dz as an array
    read(10,*) dz(1:nz)
    dz2 = dz
    ! read in (constant for now) ParFlow dt
    read(10,*) pfdt
    ! read in parflow start and stop times
    read(10,*) pft1
    read(10,*) pft2
    read(10,*) tout1
    read(10,*) n_cycle
    read(10,*) add_f

    pfnt = n_cycle*(pft2-pft1+1)
    outkk = tout1 + 1

    ! IO control, each value is a timestep interval, e.g. 1= every timestep, 2=every other, 0 = no writing
    read(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
    read(10,*) ibinpntswrite  !  controls VTK, binary output of particle locations and attributes
    read(10,*) etwrite        !  controls ASCII ET output
    read(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
                              ! ages are mapped to a grid and written every N timesteps

    ! allocate and assign timesteps
    ! this can be a scalar and update every timestep
    allocate(Time_Next(pfnt))
    Time_Next=0.d0
    do kk = outkk, pfnt
        Time_Next(kk) = float(kk)*pfdt
    end do
    Time_first = float(outkk-1)*pfdt

    ! read in velocity multiplier
    read(10,*) V_mult
    ! do we read in clm evap trans?
    read(10,*) clmtrans
    ! do we read in clm output file?
    read(10,*) clmfile
    ! read in IC number of particles for flux
    read(10,*) iflux_p_res
    ! read in density h2o
    read(10,*) denh2o
    ! read in diffusivity
    ! moldiff = (1.15e-9)*3600.d0
    read(10,*) moldiff
    ! fraction of dx/Vx
    read(10,*) dtfrac

    if(rank == 0) then
        ! wite out log file
        write(11,*)
        write(11,*) 'Grid Dimensions'
        write(11,'(" dx:",e12.5)') dx
        write(11,'(" dy:",e12.5)') dy
        write(11,'(" dz:",*(e12.5,", "))') dz(1:nz)
        write(11,*)
        write(11,*) 'Timestepping Information'
        write(11,'(" ParFlow delta-T, pfdt:",e12.5)') pfdt
        write(11,'(" ParFlow timesteps, pfnt:",i12)') pfnt
        write(11,'(" ParFlow start step, pft1:",i12)') pft1
        write(11,'(" ParFlow end step, pft2:",i12)') pft2
        write(11,'(" Output step start:",i12)') outkk
        write(11,'(" Time loops, cycles, n_cycle:",i12)') n_cycle
        write(11,'(" Total time steps:",i12)') pfnt

        write(11,*)
        write(11,*) 'V mult: ',V_mult,' for forward/backward particle tracking'
        write(11,*) 'CLM Trans: ',clmtrans,' adds / removes particles based on LSM fluxes'
        write(11,*)
        write(11,*) 'Physical Constants'
        write(11,*) 'denh2o: ',denh2o,' M/L^3'
        write(11,*) 'Molecular Diffusivity: ',moldiff,' '
        !write(11,*) 'Fractionation: ',Efract,' '
        write(11,*)
        write(11,*) 'Numerical Stability Information'
        write(11,'(" dtfrac: ",e12.5," fraction of dx/Vx")') dtfrac
    endif

    read(10,*) nind
    read(10,*) Indname
    if(rank == 0) then
        write(11,*)
        write(11,*) 'Indicator File'
        write(11,*) nind, 'Indicators'
    endif

    ! end of SLIM input
    close(10)

    !--------------------------------------------------------------------
    if(rank == ppx*qqy) then
        allocate(grid(ppx*qqy,4),grid_old(ppx*qqy,4))
        allocate(Zonet_old(-buff+1:nx+buff,-buff+1:ny+buff))
        allocate(Zonet_new(-buff+1:nx+buff,-buff+1:ny+buff))
        allocate(p_num(nx,ny),c_sum(max(nx,ny)),d_indices(np))
        grid = 0; Zonet_new = -1
        p_num = 1 ! check such an initialization. Checked and should be right.
    end if   ! set manage rank

    if(np_ic /= -1) then
        ! if not restart, we do decompostion and build topology
        call gridinfo()
        ! doing this on manager, and then we get subdomain info in grid array
        ! for init we don't have to build p_sum on each rank, just use that for
        ! the whole domain with init value of 1.
        ! if resart, no need to do this part, directly read necessary info from resart files
        call copy_grid() ! doing this on workers
        call allocate_arrays_temp() ! doing this on workers
        call receive_Zone_de() ! this needs all the ranks, both manager and workers

    end if

    if (rank /= ppx*qqy) then
        ! for the following, if there is [if (rank /= ppx*qqy)] in the subroutine, please
        ! just keep it there now, and later we can find a better way.
        call global_xyz() ! on workers
        ! restart file info inside, so call it before restart.
        call allocate_arrays_const() ! on workers, and P array needs nind

        if (np_ic == -1) then
            ! if np_ic = -1 then we read a restart file
            ! move restart here is due to we want to do the allocate of temp here which can keep
            ! a clear structure, and then we can read the necessary files such as Ind etc.
            ! But if we do allocate, it needs the subdomain range etc., so we read restart info here.

            call read_restarts ()
            ! this is in the subdomain_bound module, which is in the ecoslim_subdomain_bc file

            ! 1. read ix1 etc. on worker. specific files or in restarts with P
            ! 2. read zones on worker
            ! 3. read grid and Zonet_old on manager
            call allocate_arrays_temp()

        end if

        !-------------------------------------
        call DEM_for_visual()

        call local_xyz()

        ! read in Indicator file
        if (nind>0) then
            if (Indname /= '') then
                fname = trim(adjustl(Indname))
                call pfb_read(Ind,fname,nx,ny,nz)
                if(rank == 0) &
                write(11,*) 'Read Indicator File:', fname
            else
                if(rank == 0) &
                write(11,*) 'WARNING: indicator flage >0 but no indicator file provided'
            end if ! Ind
        end if

        if(rank == 0) then
            flush(11)
            close(11)
        endif

        ! Read porosity values from ParFlow .pfb file
        fname=trim(adjustl(pname))//'.out.porosity.pfb'
        call pfb_read(Porosity,fname,nx,ny,nz)

        ! Read the initial Saturation from ParFlow
        if(np_ic /= -1) then

            pfkk = pft1 - 1
            write(filenum,'(i5.5)') pfkk
            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Saturation,fname,nx,ny,nz)
            ! this is not necessary if restart

            ! Intialize cuRand device API and this part need to check through again.
            call createRand_init<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>>(nx,ny,nz, &
            rank,np_ic,nnx1,nny1)
            ! each subdomain has different dimension, so we set a fixed offset of h using the whole domain.

        end if

        ! copy necessary data from host to device
        ! this part together with allocation should be done after update of subdomain
        dz_de  = dz2 ! I can't understand why dz doesn't work.
        Ind_de = Ind
        Porosity_de   = Porosity
        Saturation_de = Saturation
        P_de = P
        !!! initialization of P_de, probably need a new kernel
        ! think about initialization of some device variables
        ! particularly those which are not updated each timestep.

        !-------------------------------------
        ! Define initial particles' locations and mass
        if (np_ic > 0)  then
            np_active = 0
            pid = 0
            if (np_active + np_ic*nnx1*nny1*nz >= np ) then
                write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                ! now there is no restart after 9090

                goto 9090
            endif

            call add_init_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
            tPB >>> (P_de,C_de,Saturation_de,Porosity_de, &
            np_ic,nind,denh2o,outkk,nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de)

            np_active = np_ic*nnx1*nny1*nz
            pid = np_ic*nnx1*nny1*nz

        end if

        ! Define initial particles' locations by surface water
        if (np_ic < -1)  then
            np_active = 0
            pid = np_active

            if(np_active + abs(np_ic)*nnx1*nny1 >= np) then
                write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                goto 9090
            end if

            call river_init_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
            tPB >>> (P_de,C_de,Saturation_de,Porosity_de, &
            np_ic,np_active,pid,nind,nnx1,nny1,nz,dx,dy,dz_de, &
            pfdt,denh2o,outkk)

            np_active = abs(np_ic)*nnx1*nny1
            pid = abs(np_ic)*nnx1*nny1

        end if ! river IC

        ! Write out intial concentrations
        ! normalize ages by mass

        ! call MPI_ALLReduce(MPI_IN_PLACE,C,n_constituents*nx*ny*nz,MPI_DOUBLE_PRECISION, &
        !                    MPI_SUM,MPI_COMM_WORLD,ierr)

        ! this may not be used for this version later, we will do parallel output
        if (np_ic /= -1) then
            where (C(3,:,:,:)>0.0)  C(2,:,:,:) = C(2,:,:,:) / C(3,:,:,:)
            where (C(3,:,:,:)>0.0)  C(4,:,:,:) = C(4,:,:,:) / C(3,:,:,:)
            ! can be done on GPU

            if(icwrite > 0)  &
            call vtk_write(Time_first,C,conc_header,nx,ny,nz,0,n_constituents,Pnts,vtk_file)
            ! each rank visualize its own subdomain
        end if

        ! keep the following output as it is. we can figure out a better way to handle them later.
        call MPI_FILE_OPEN(MPI_COMM_SELF,logf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                        MPI_INFO_NULL,fh4,ierr)

        write(message,'(a,a)') ' **** Transient Simulation Particle Accounting ****', new_line(' ')
        call MPI_FILE_WRITE(fh4, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        write(message,'(a,a)') ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
                                ETOut NP_PrecipIn NP_ETOut NP_QOut NP_active_old NP_filtered', new_line(' ')
        call MPI_FILE_WRITE(fh4, trim(message), len(trim(message)), &
        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

        ! open exited partile file and write header
        call MPI_FILE_OPEN(MPI_COMM_SELF,exitedf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                        MPI_INFO_NULL,fh3,ierr)

        if(rank == 0) then
            if(ipwrite < 0) then
            ! open/create/write the 3D output file which will write particles out each timestemp, very slowly in parallel
            open(214,file=trim(runname)//'_total_particle_trace.3D')
            write(214,*) 'X Y Z TIME'
            end if !! ipwrite < 0

            open(13,file=trim(runname)//'_ET_output.txt')
            write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

            open(15,file=trim(runname)//'_flow_output.txt')
            write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

            open(16,file=trim(runname)//'_PET_balance.txt')
            write(16,*) 'TIME P[kg] ET[kg]'
        endif

        call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
        tPB>>>(Zone_de,nnx1,nny1,buff,d_isValid,rank,t_rank-1)
        ! ix2, iy2, nnx2, and nny2 are only used for read files
        neigh_list = d_isValid(1:t_rank-1)
        if(sum(neigh_list) > 0) allocate(N_recv_all(sum(neigh_list)))
        ! don't have to do this every timestep, only after update of decomposition

        ! Intialize cuRand device API and this part need to check through again.
        call createRand_loop<<< ceiling(dble(np_active)/tPB),tPB >>>(np, rank, pfnt)
    end if
        !--------------------------------------------------------------------
        ! (3) For each timestep, loop over all particles to find and
        !     update their new locations
        !--------------------------------------------------------------------
        ! loop over timesteps
    pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1

    do kk = outkk, pfnt
        if (rank /= ppx*qqy) then
            ! reset ParFlow counter for cycles
            if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1
            ! adjust the file counters
            pfkk = pfkk + 1
            ! Read the velocities computed by ParFlow
            write(filenum,'(i5.5)') pfkk
            !----------------------------------------
            ! wait for hdf5 ready
            fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vx,fname,nx+1,ny,nz)

            fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vy,fname,nx,ny+1,nz)

            fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vz,fname,nx,ny,nz+1)

            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Saturation,fname,nx,ny,nz)

            if (clmtrans) then
                ! Read in the Evap_Trans
                fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.pfb'
                call pfb_read(EvapTrans,fname,nx,ny,nz)

                if (mod((kk-1),add_f) == 0) EvapTrans_da = 0.d0
                where (EvapTrans > 0.d0) EvapTrans_da = EvapTrans_da + EvapTrans

                ! check if we read full CLM output file
                if (clmfile) then
                    ! Read in CLM output file @RMM to do make this input
                    fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.pfb'
                    call pfb_read(CLMvars,fname,nx,ny,nzclm)

                end if
            end if
            !----------------------------------------
            ! Determine whether to perform forward or backward patricle tracking
            Vx = Vx * V_mult
            Vy = Vy * V_mult
            Vz = Vz * V_mult

            out_age_cpu = 0.d0; out_mass_cpu = 0.d0; out_comp_cpu = 0.d0; out_np_cpu = 0
            et_age_cpu  = 0.d0; et_mass_cpu  = 0.d0; et_comp_cpu  = 0.d0; et_np_cpu  = 0
            mean_age    = 0.d0; mean_comp    = 0.d0; total_mass   = 0.d0

            Vx_de = Vx; Vy_de = Vy; Vz_de = Vz
            Saturation_de = Saturation; EvapTrans_de = EvapTrans
            CLMvars_de = CLMvars(:,:,11) ! probably this can be deallocated after add of particles
            ! If decomposition of the domain is updated, the matrix on GPU should be
            ! deallocated and then allocated after the dynamic DDC.
            ! Though hdf5 is not ready, at least we can assume the shape of each
            ! variable in the following parts.
            ! Now just think about the hourly add of particles.
            out_age_de = out_age_cpu; out_mass_de = out_mass_cpu
            out_comp_de = out_comp_cpu; out_np_de = out_np_cpu
            et_age_de = et_age_cpu; et_mass_de = et_mass_cpu
            et_comp_de = et_comp_cpu; et_np_de = et_np_cpu
            mean_age   = mean_age_de; mean_comp = mean_comp_de; total_mass = total_mass_de
            C = 0.d0; C_de = C
            ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
            ! the following add of particles.
            !----------------------------------------
            if (clmtrans) then
                if (np_active < np) then

                    call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
                    tPB >>> (EvapTrans_de,PET_balance_de,d_isValid,nnx1,nny1,nz, &
                    dx,dy,dz,pfdt,denh2o)
                    ! d_isValid has the length as P or of np which is assigned in input script
                    ! so nnx1**nny1*nz should be smaller than np
                    ! we don't care about the buffer zone for add of particles
                    ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                    ! in main program and pass arguments to other subroutines.

                    ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                    call thrustscan(d_isValid,nnx1*nny1,d_indices)

                    i_added_particles = d_indices(nnx1*nny1) * iflux_p_res ! should be legal?
                    if(np_active + i_added_particles >= np) then
                        write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                        write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                        goto 9090
                    endif

                    call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                    tPB >>> (d_indices,P_de,EvapTrans_de,CLMvars_de,Ind_de, &
                    iflux_p_res,np_active,pid,nind,nnx1,nny1,nz,dx,dy,dz,pfdt, &
                    denh2o,outkk)

                    np_active = np_active + d_indices(nnx1*nny1) * iflux_p_res
                    pid = pid + d_indices(nnx1*nny1) * iflux_p_res

                end if
            end if

            ! this has to be checked later.
            ! call MPI_ALLReduce(MPI_IN_PLACE, PET_balance_da(kk,:), 2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, PET_balance,2,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            PET_balance = PET_balance_de
            ! print to log file, or PET_balance can be a long array and do print at last

            !-------------------------------------
            ! particle loop here
            call particles_independent <<< ceiling(dble(np_active)/tPB),tPB >>> ( &
                P_de,C_de,dz_de,EvapTrans_de,Vx_de,Vy_de,Vz_de,Saturation_de, &
                Porosity_de,Ind_de,Zone_de,out_age_de,out_mass_de,out_comp_de, &
                et_age_de,et_mass_de,et_comp_de,out_np_de,et_np_de, &
                kk,np_active,nz,nind,pfdt,moldiff,dx,dy,denh2o,dtfrac, &
                ix1,iy1,buff,reflect,nnx1,nny1,rank, &
                xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                xmin,ymin,zmin,xmax,ymax,zmax)
                ! P_de is initialized either after allocation of P, or
                ! read restart files
            !-------------------------------------
            ! particle exchange here
            ! neigh_list is an 1D array of size t_rank. If an element is 1, it has this neighbor.
            ! sum(neigh_list) > 0, this means there must be at least one neighbor.
            ! sum(neigh_list) > 0 could be put in main to determine if call this subroutine
            if(sum(neigh_list) > 0) call particle_exchange(rank,t_rank,nind,tPB,np_active)
            ! at this moment, np_active is not updated
            !-------------------------------------
            if(N_send > 0) then
                allocate(P_exit(N_send,17+2*nind))
                P_exit = P_send; deallocate(P_send)
            end if
            ! copy exited particles back and output later.
            ! considering asyn data copy.
            ! N_send is npout, make it clear later

            C              = C_de
            !-------------------------------------
            ! for such reduce, we can use a specific MPI process to do.
            ! we can do this using cuda-aware MPI, and then copy back.
            call MPI_ALLReduce(MPI_IN_PLACE, ET_age_cpu,  1,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, ET_comp_cpu, 3,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, ET_mass_cpu, 1,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, ET_np_cpu,   1,MPI_INTEGER,         MPI_SUM,work_comm,ierr)

            call MPI_ALLReduce(MPI_IN_PLACE, Out_age_cpu, 1,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, Out_comp_cpu,3,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, Out_mass_cpu,1,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            call MPI_ALLReduce(MPI_IN_PLACE, Out_np_cpu,  1,MPI_INTEGER,         MPI_SUM,work_comm,ierr)

            out_age_cpu    = out_age_de;    out_mass_cpu   = out_mass_de
            out_comp_cpu   = out_comp_de;   out_np_cpu     = out_np_de
            et_age_cpu     = et_age_de;     et_mass_cpu    = et_mass_de
            et_comp_cpu    = et_comp_de;    et_np_cpu      = et_np_de

            ! call MPI_ALLReduce(MPI_IN_PLACE,C,n_constituents*nx*ny*nz,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
            ! I think here it is not necessary. You just need to write into hdf5 if necessary.

            !-------------------------------------
            ! call MPI_Reduce(np_active, np_active_log, 1, MPI_INTEGER, MPI_SUM, 0, MPI_COMM_WORLD, ierr)
            call MPI_ALLGather(np_active,1,MPI_INTEGER,nump,1,MPI_INTEGER,work_comm,ierr)
            ! nump is an 1D array on host. it is allocated as const with dim of t_rank-1

            if(rank == 0) then
                write(12,*) sum(nump), T2-T1, redis_time
                flush(12)

                if (ET_mass_cpu(1) > 0 ) then
                    ET_age_cpu(1) = ET_age_cpu(1)/ET_mass_cpu(1)
                    ET_comp_cpu(1) = ET_comp_cpu(1)/ET_mass_cpu(1)
                    ET_comp_cpu(2) = ET_comp_cpu(2)/ET_mass_cpu(1)
                    ET_comp_cpu(3) = ET_comp_cpu(3)/ET_mass_cpu(1)
                end if
                write(13,'(6(e12.5),i12)') float(kk)*pfdt, ET_age_cpu(1), ET_comp_cpu(1), &
                                        ET_comp_cpu(2), ET_comp_cpu(3), ET_mass_cpu(1), &
                                        ET_np_cpu(1)
                flush(13)

                if (Out_mass_cpu(1) > 0 ) then
                    Out_age_cpu(1) = Out_age_cpu(1)/Out_mass_cpu(1)
                    Out_comp_cpu(1) = Out_comp_cpu(1)/Out_mass_cpu(1)
                    Out_comp_cpu(2) = Out_comp_cpu(2)/Out_mass_cpu(1)
                    Out_comp_cpu(3) = Out_comp_cpu(3)/Out_mass_cpu(1)
                end if
                write(15,64) float(kk)*pfdt, Out_age_cpu(1), Out_comp_cpu(1), &
                            Out_comp_cpu(2), Out_comp_cpu(3), Out_mass_cpu(1), &
                            Out_np_cpu(1)
                flush(15)

                write(16,'(5(e12.5,2x))') float(kk)*pfdt, PET_balance(kk,1), PET_balance(kk,2), &
                                        PET_balance_da(kk,1), PET_balance_da(kk,2)
                flush(16)

            endif

            if (total_mass > 0.0d0)  then
                mean_age =  mean_age / total_mass
                mean_comp = mean_comp / total_mass
                mean_mass = total_mass / dble(np_active)
            end if

            write(message,'(3(i10),7(1x,e12.5,1x),3(i8),2(i12),a)') kk,pfkk,outkk,Time_Next(kk),mean_age,mean_comp, &
                                                                    mean_mass,total_mass,PET_balance(kk,1), &
                                                                    PET_balance(kk,2),i_added_particles,ET_np_cpu(1), &
                                                                    Out_np_cpu(1),np_active,np_active2,new_line(' ')
            call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

            write(filenumout,'(i10.10)') kk

            ! write all active particles at concentration in ASCII VisIT 3D file format
            ! as noted above, this option is very slow compared to VTK binary output
            if(ipwrite > 0) then
            if(mod(kk,ipwrite) == 0)  then
            ! open/create/write the 3D output file
            open(14,file=trim(runname)//'_transient_particle.'//trim(adjustl(filenumout))//'.3D')
            write(14,*) 'X Y Z TIME ID'
            do ii = 1, np_active
            if (P(ii,8) == 1) write(14,61) P(ii,1), P(ii,2), P(ii,3), P(ii,4), P(ii,13+2*nind)
            end do
            close(14)
            end if
            end if

            ! normalize ages by mass
            !where (C(3,:,:,:)>0.0)  C(2,:,:,:) = C(2,:,:,:) / C(3,:,:,:)
            !where (C(3,:,:,:)>0.0)  C(4,:,:,:) = C(4,:,:,:) / C(3,:,:,:)
            !where (C(3,:,:,:)>0.0)  C(5,:,:,:) = C(5,:,:,:) / C(3,:,:,:)

            ! normalize ET ages by mass
            !where (C(7,:,:,:)>0.0)  C(8,:,:,:) = C(8,:,:,:) / C(7,:,:,:)
            !where (C(7,:,:,:)>0.0)  C(9,:,:,:) = C(9,:,:,:) / C(7,:,:,:)
            ! the normalization can be considered on GPU

            ! Write gridded ET outputs to text files
            if(etwrite > 0) then
            if (mod(kk,etwrite) == 0) then
            ! open/create/write the 3D output file
            open(14,file=trim(runname)//'_ET_summary.'//trim(adjustl(filenumout))//'.txt')
            write(14,*) 'X Y Z ET_npart, ET_mass, ET_age, ET_comp, EvapTrans_Rate, Saturation, Porosity'
            do i = 1, nx
            do j = 1, ny
            do k = 1, nz
            if (EvapTrans(i,j,k) < 0.0d0)   &
            write(14,'(3(i6), 7(e13.5))')  i, j, k, C(6,i,j,k), C(7,i,j,k), C(8,i,j,k), &
                C(9,i,j,k), EvapTrans(i,j,k), Saturation(i,j,k), Porosity(i,j,k)
            end do
            end do
            end do
            close(14)
            end if
            end if

            ! write grid based values ("concentrations")
            vtk_file=trim(runname)//'_cgrid'
            if(icwrite > 0)  then
                !if(mod(kk,icwrite) == 0)  &
                !call vtk_write(Time_Next(kk),C,conc_header,nx,ny,nz,kk,n_constituents,Pnts,vtk_file)
                if(mod(kk,icwrite) == 0 .and. rank == 0)  then
                    fname=trim(runname)//'.out.C1.'//trim(adjustl(filenumout))//'.pfb'
                    call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,1)
                    fname=trim(runname)//'.out.C2.'//trim(adjustl(filenumout))//'.pfb'
                    call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,2)
                    fname=trim(runname)//'.out.C3.'//trim(adjustl(filenumout))//'.pfb'
                    call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,3)
                    fname=trim(runname)//'.out.C4.'//trim(adjustl(filenumout))//'.pfb'
                    call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,4)
                    fname=trim(runname)//'.out.C5.'//trim(adjustl(filenumout))//'.pfb'
                    call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,5)
                    !fname=trim(runname)//'.out.C6.'//trim(adjustl(filenumout))//'.pfb'
                    !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,6)
                    !fname=trim(runname)//'.out.C7.'//trim(adjustl(filenumout))//'.pfb'
                    !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,7)
                    !fname=trim(runname)//'.out.C8.'//trim(adjustl(filenumout))//'.pfb'
                    !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,8)
                    !fname=trim(runname)//'.out.C9.'//trim(adjustl(filenumout))//'.pfb'
                    !call pfb_write(C,fname,nx,ny,nz,dx,dy,dy,n_constituents,9)
                end if
            end if

            ! write binary particle locations and attributes
            vtk_file=trim(runname)//'_pnts'
            if(ibinpntswrite > 0)  then
            if(mod(kk,ibinpntswrite) == 0)  &
            call vtk_write_points(P,np_active,np,kk,vtk_file, dx, dy,nx,ny, maxZ,dem)
            end if

            if (mod(kk,(pft2-pft1+1)/5) == 0 )  then
                call MPI_FILE_OPEN(MPI_COMM_SELF,restartf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                                MPI_INFO_NULL,fh2,ierr)
                call MPI_FILE_WRITE(fh2,np_active,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
                call MPI_FILE_WRITE(fh2,pid,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
                call MPI_FILE_WRITE(fh2,P(1:np_active,1:nind*2+17),np_active*(nind*2+17), &
                                    MPI_DOUBLE_PRECISION,MPI_STATUS_IGNORE,ierr)
                call MPI_FILE_CLOSE(fh2, ierr)

                ! output more, such as grid, zone_de etc.
            end if
            ! if writing restart, we need to copy P from device to host, if not, we don't need copy.

            ! LB criteria: if the difference between max- and min-number of particles over some limitation
            ! we assume this limitation as 0.5 here and later to figure out a better way
            if(dble(maxval(nump) - minval(nump))/maxval(nump) > 0.5d0) then
                decom = 1   ! since there is no nump on manager, so we use a flag 'decom'.
                ! build p_num on each worker, and this kernel is in utilities
                ! p_num is allocated after the dim of each subdomain is determined.
                call build_p_num(ix1,iy1)
            end if

        end if ! do this interval on workers.

        if (decom == 1) then
            call combine_p_num(grid)
            ! this is a collective communication, so using all the ranks

            call deallocate_temp() ! only on workers
            ! deallocate variables with old domension to save memory for DDC and particle transfer
            ! Zone_de is not deallocated since it will still be used

            call gridinfo()
            ! so grid on each worker has been updated

            call copy_grid_n()
            ! get nnx1n,nny1n,nnx2n,nny2n,ix1n,iy1n,ix2n,iy2n for temporary use

            call subzones_for_transfer()
            ! use the old and new suits ix1 etc. to get Zone_de and Zones_new

            call zone_exchange()
            ! exchange particles using Zone_de and Zones_new

            call copy_grid()
            ! get the new nnx1,nny1,nnx2,nny2,ix1,iy1,ix2,iy2

            call allocate_arrays_temp()

            call receive_Zone_de(grid)

            if(rank /= ppx*qqy) then

                call DEM_for_visual(DEM,DEMname,ix1,iy1,nnx1,nny1,nz)

                call local_xyz()

                ! initialize some arrays if necessary

                ! Build neighbor list. do this after a particle loop
                call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
                tPB>>>(zone_de,nnx1,nny1,buff,d_isValid,rank,t_rank)
                neigh_list = d_isValid(1:t_rank)
                ! don't need to do this every timestep. Only after update of decomposition
                if(sum(neigh_list) > 0) allocate(N_recv_all(sum(neigh_list)))
                ! notice: make sure N_recv_all is deallocated before new allocation
                ! used to know how many particles this rank will receive
                ! N_recv_all should be set after update of decomposition.
                ! its size is the number of its neighbors. it will have elements of value 0,
                ! this means that neighbor will not send particles to this rank this time step.
                call createRand_loop<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>>(nx,ny,nz, &
                rank,np_ic,nnx1,nny1)

            end if
        end if
    end do !! timesteps and cycles

    9090 continue  !! continue statement for running out of particles when assigning precip flux.
                !!  code exits gracefully (writes files and possibly a restart file so the user can
                !!  re-run the simulation)

    ! close output file
    call MPI_FILE_CLOSE(fh3, ierr)
    call MPI_FILE_CLOSE(fh4, ierr)

    ! format statements for particle output
    61  FORMAT(4(e12.5))
    64  FORMAT(6(e12.5),i12)

    if(rank == 0) then
        flush(13)
        close(13)

        if(ipwrite < 0) close(214)

        flush(15)
        close(15)

        flush(16)
        close(16)
    endif

    write(message,'(a)') new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(a,a)') '###  Execution Finished', new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(a)') new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(i10,a,a)') npout,' particles exited the domain via outflow or ET.',new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(a)') new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(a,a)') 'Simulation Timing and Profiling:', new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'("Total Execution Time (s):",e12.5,a)') Total_time2-Total_time1,new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'("File IO Time Read (s):",e12.5,a)') IO_time_read,new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'("File IO Time Write (s):",e12.5,a)') IO_time_write,new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'("Time Sorting (s):",e12.5,a)') sort_time,new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'("Parallel Particle Time (s):",e12.5,a)') parallel_time,new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    write(message,'(a)') new_line(' ')
    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

    call MPI_FILE_CLOSE(fh1, ierr)
    call MPI_FINALIZE(ierr)

end program EcoSLIM

