module exchange_particles
    use cudafor
    use variable_list, only: N_recv_all, N_send, neigh_list
    ! neigh_list is allocated after getting number of total GPUs
    ! N_recv_all is allocated or deallocated after initialization/update of grid

    use variable_list, only: P_exit, P_de, C_de, d_isValid, d_indices, &
    P_send, P_recv, Zone_de, holes

contains
    subroutine MPI_particle_exchange(rank,t_rank,nind,tPB,np_active)
        use mpi
        implicit none
        integer:: rank, t_rank, nind, tPB, np_active
        integer:: i, j, ierr, status(MPI_STATUS_SIZE)
        integer:: ncount1, ncount2, ncount3, ncount4, &
                  request1(t_rank), request2(t_rank), &
                  request3(t_rank), request4(t_rank)

        ncount2 = 0; ncount4 = 0
        do i = 0, t_rank-1
            !-----------------------------------------
            ! receive part. N_recv_all and P_recv are for all neighbors which send particles.
            !-----------------------------------------
            ! Rank by rank. For each rank, other ranks send particles to it while it must
            ! not send particles to itself. So the following actions can be reached.
            if(i == rank) then

                ncount1 = 0
                do j = 0, t_rank-1
                    if(neigh_list(j+1) == 1) then ! be careful j+1
                        ncount1 = ncount1 + 1
                        call MPI_IRECV(N_recv_all(ncount1),1,MPI_INTEGER,j,40,MPI_COMM_WORLD,request1(ncount1),ierr)
                    end if
                end do
                ! the location of each neighbor is fixed in N_recv_all, since there is the message envelope
                ! each neighbor will send number of particles that will be sent to this rank, even it is zero.
                ! finally, ncount1 is the number of neighbors. We set the size of request1 as t_rank.

                call MPI_WAITALL(ncount1,request1(1:ncount1),status,ierr)
                ! next step it needs the sum of N_recv_all, so we have to wait here.

                ncount1 = 0; ncount3 = 0
                if(sum(N_recv_all) > 0) then
                    ! at least one neighbor sent. if not, no need to do receive
                    allocate(P_recv(sum(N_recv_all),17+nind*2))
                    ! do sum of N_recv_all and allocate P_recv
                    do j = 0, t_rank-1
                        if(neigh_list(j+1) == 1) then
                            ncount1 = ncount1 + 1
                            if(N_recv_all(ncount1) > 0) then ! exist and send
                                ncount3 = ncount3 + 1
                                call MPI_IRECV(P_recv(sum(N_recv_all(1:ncount1-1))+1:sum(N_recv_all(1:ncount1)),:), &
                                N_recv_all(ncount1)*(17+nind*2),MPI_DOUBLE_PRECISION, &
                                j,41,MPI_COMM_WORLD,request2(ncount3),ierr)
                            endif
                        end if
                    end do
                    ! The following work doesn't care about if this step has been finished until the connection.
                end if

            end if

            !-----------------------------------------
            ! send part. one by one neighbor, N_send and P_send are repeatedly used.
            !-----------------------------------------
            if(neigh_list(i+1) == 1) then

                ncount2 = ncount2 + 1

                call prepare_neighbor<<<ceiling(dble(np_active)/tPB),&
                tPB>>>(P_de(:,13+2*nind),d_isValid,np_active,i) ! get d_isValid
                call thrustscan(d_isValid,np_active,d_indices)

                if(ncount2 /= 1) call MPI_WAIT(request3(ncount2-1),status,ierr)
                N_send = np_active - d_indices(np_active) ! this should be legal
                call MPI_ISEND(N_send,1,MPI_INTEGER,i,40,MPI_COMM_WORLD,request3(ncount2),ierr)

                if(N_send > 0) then
                    ncount4 = ncount4 + 1
                    if(ncount4 /= 1) then
                        call MPI_WAIT(request4(ncount4-1),status,ierr)
                        deallocate(P_send)
                    end if
                    allocate(holes(N_send),P_send(N_send,17+nind*2))

                    call prepare_holes<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,np_active)
                    ! block size cannot be reduced ？？

                    call compaction_inplcae<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)

                    call MPI_ISEND(P_send,N_send*(17+nind*2),MPI_DOUBLE_PRECISION,i,41,&
                                MPI_COMM_WORLD,request4(ncount4),ierr)
                    ! gpu send since it is a gpu buffer?

                    np_active = d_indices(np_active); deallocate(holes)
                end if

            end if

        end do

        ! then connect received particles
        if(ncount3 > 0) &
        call MPI_WAITALL(ncount3,request2(1:ncount3),status,ierr)
        ! if receive happened, we wait

        call connect_recv<<<ceiling(dble(sum(N_recv_all))/tPB),&
        tPB>>>(P_de,P_recv,np_active,sum(N_recv_all),17+nind*2)
        np_active = np_active + sum(N_recv_all)

        deallocate(P_recv) ! deallocate P_send after transfer back to host

        call Update_C_Array <<< ceiling(dble(np_active)/tPB),tPB >>> (P_de,C_de,dx,dy,dz, &
        nz,ix1,iy1,nnx1,nny1,buff,mean_age_de,mean_comp_de,total_mass_de,np_active)

        ! then separate inactive particles
        call prepare_exit<<<ceiling(dble(np_active)/tPB),tPB>>>(P_de(:,8),d_isValid,np_active)
        call thrustscan(d_isValid,np_active,d_indices)

        call MPI_WAIT(request3(ncount2),status,ierr) ! no if condition since send anyway
        N_send = np_active - d_indices(np_active)

        if(ncount4 > 0)then
            call MPI_WAIT(request4(ncount4),status,ierr)
            deallocate(P_send)
        end if

        if(N_send > 0) then

            allocate(holes(N_send),P_send(N_send,17+nind*2))
            ! P_send also be used for P_exit

            call prepare_holes<<<ceiling(dble(np_active)/tPB),&
            tPB>>>(holes,d_indices,d_isValid,np_active)

            call compaction_inplcae<<<ceiling(dble(np_active)/tPB),&
            tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)
            ! no need to deallocate P_send, it will be done next timestep before sending

            np_active = d_indices(np_active); deallocate(holes)

        end if

    end subroutine MPI_particle_exchange

    attributes(global) subroutine Update_C_Array(P,C,dx,dy,dz,nz,ix1,iy1, &
                                                nnx1,nny1,buff,mean_age, &
                                                mean_comp,total_mass,np_active)
        implicit none
        real(8),intent(in):: P(:,:),dz(:)
        real(8),intent(inout):: C(:,:,:,:)
        real(8),intent(inout):: mean_age(:),total_mass(:),mean_comp(:)
        integer,value:: nnx1,nny1,nz,ix1,iy1,buff,np_active
        real(8),value:: dx,dy
        integer:: Ploc(3), ii, k
        real(8):: lock, Z

        ii = (blockIdx%x - 1) * blockDim%x + threadIdx%x
        lock = 1.d0

        if(ii < np_active) then
            ! Transfer global to local
            P(ii,1) = P(ii,1) - ix1*dx
            P(ii,2) = P(ii,2) - iy1*dy

            ! Find the "adjacent" "cell corresponding to the particle's location
            Ploc(1) = floor(P(ii,1) / dx)
            Ploc(2) = floor(P(ii,2) / dy)
            if(Ploc(1) >= nnx1 + buff) Ploc(1) = nnx1 + buff - 1
            if(Ploc(2) >= nny1 + buff) Ploc(2) = nny1 + buff - 1

            Z = 0.d0
            do k = 1, nz
                Z = Z + dz(k)
                if (Z >= P(ii,3)) then
                    Ploc(3) = k - 1
                    exit
                end if
            end do

            if(P(ii,8) == 0. .and. P(ii,10) == 2.) then
                temp = atomicAdd(C(6,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),lock)
                temp = atomicAdd(C(7,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,6))  ! particle mass added to ET
                temp = atomicAdd(C(8,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,4)*P(ii,6))  ! mass weighted age
                temp = atomicAdd(C(9,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,7)*P(ii,6))  ! mass weighted contribution
            end if

            temp = atomicAdd(C(1,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,8)*P(ii,11)*P(ii,6))
            temp = atomicAdd(C(2,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,8)*P(ii,4)*P(ii,6))
            temp = atomicAdd(C(4,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,8)*P(ii,5)*P(ii,6))
            temp = atomicAdd(C(3,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,8)*P(ii,6))
            temp = atomicAdd(C(5,Ploc(1)+1,Ploc(2)+1,Ploc(3)+1),P(ii,8)*P(ii,12)*P(ii,6))
        !--------------------------------------
            ! increment mean age, composition and mass
            temp = atomicAdd(mean_age, P(ii,4)*P(ii,6))
            temp = atomicAdd(mean_comp, P(ii,7)*P(ii,6))
            temp = atomicAdd(total_mass, P(ii,6))
        !--------------------------------------
            P(ii,1) = P(ii,1) + ix1*dx
            P(ii,2) = P(ii,2) + iy1*dy
        end if

    end subroutine Update_C_Array

end module exchange_particles