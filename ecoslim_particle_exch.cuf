module exchange_particles
    ! subroutine particle_exchange

    use cudafor
    use thrust
    use variable_list, only: N_recv_all, N_send, neigh_list
    ! neigh_list is allocated after getting number of total GPUs
    ! N_recv_all is allocated or deallocated after initialization/update of grid
    use variable_list, only: P_exit, P_de, C_de, d_isValid, d_indices, &
    Zone_de, P_send, P_recv, holes
    use variable_list, only: dx,dy,dz_de,nz,mean_age_de,mean_comp_de,total_mass_de
    use create_subdomain, only: ix1,iy1,nnx1,nny1,buff

contains
    subroutine particle_exchange(rank,t_rank,nind,tPB,np_active)
        ! change sequence, send first and then recv
        use mpi
        implicit none
        integer,intent(in):: rank, t_rank, nind, tPB
        integer,intent(inout):: np_active
        integer:: i, j, ierr, status(MPI_STATUS_SIZE)
        integer:: ncount1, ncount2, ncount3, ncount4, &
                  request1(t_rank), request2(t_rank), &
                  request3(t_rank), request4(t_rank)

        ncount2 = 0; ncount4 = 0
        do i = 0, t_rank-1

            !-----------------------------------------
            ! send part. one by one neighbor, N_send and P_send are repeatedly used.
            !-----------------------------------------
            if(neigh_list(i+1) == 1) then

                ncount2 = ncount2 + 1

                call prepare_neighbor<<<ceiling(dble(np_active)/tPB),&
                tPB>>>(P_de(:,13+2*nind),d_isValid,np_active,i) ! get d_isValid
                call thrustscan(d_isValid,np_active,d_indices)

                if(ncount2 /= 1) call MPI_WAIT(request3(ncount2-1),status,ierr)
                N_send = np_active - d_indices(np_active) ! this should be legal
                call MPI_ISEND(N_send,1,MPI_INTEGER,i,40,MPI_COMM_WORLD,request3(ncount2),ierr)

                if(N_send > 0) then
                    ncount4 = ncount4 + 1
                    if(ncount4 /= 1) then
                        call MPI_WAIT(request4(ncount4-1),status,ierr)
                        deallocate(P_send)
                    end if
                    allocate(holes(N_send),P_send(N_send,17+nind*2))

                    call prepare_holes<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,np_active)
                    ! block size cannot be reduced ？？

                    call compaction_inplace<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)

                    call MPI_ISEND(P_send,N_send*(17+nind*2),MPI_DOUBLE_PRECISION,i,41,&
                                MPI_COMM_WORLD,request4(ncount4),ierr)
                    ! gpu send since it is a gpu buffer?

                    np_active = d_indices(np_active); deallocate(holes)
                end if

            end if

            !-----------------------------------------
            ! receive part. N_recv_all and P_recv are for neighbors which send particles.
            !-----------------------------------------
            ! Rank by rank. For each rank, other ranks send particles to it while it must
            ! not send particles to itself. So the following actions can be reached.
            if(i == rank) then

                ncount1 = 0
                do j = 0, t_rank-1
                    if(neigh_list(j+1) == 1) then ! be careful j+1
                        ncount1 = ncount1 + 1
                        call MPI_IRECV(N_recv_all(ncount1),1,MPI_INTEGER,j,40,MPI_COMM_WORLD,request1(ncount1),ierr)
                    end if
                end do
                ! the location of each neighbor is fixed in N_recv_all, since there is the message envelope
                ! each neighbor will send number of particles that will be sent to this rank, even it is zero.
                ! finally, ncount1 is the number of neighbors. We set the size of request1 as t_rank.

                call MPI_WAITALL(ncount1,request1(1:ncount1),status,ierr)
                ! next step it needs the sum of N_recv_all, so we have to wait here.

                ncount1 = 0; ncount3 = 0
                if(sum(N_recv_all) > 0) then
                    ! at least one neighbor sent. if not, no need to do receive
                    allocate(P_recv(sum(N_recv_all),17+nind*2))
                    ! do sum of N_recv_all and allocate P_recv
                    do j = 0, t_rank-1
                        if(neigh_list(j+1) == 1) then
                            ncount1 = ncount1 + 1
                            if(N_recv_all(ncount1) > 0) then ! exist and send
                                ncount3 = ncount3 + 1
                                call MPI_IRECV(P_recv(sum(N_recv_all(1:ncount1-1))+1:sum(N_recv_all(1:ncount1)),:), &
                                N_recv_all(ncount1)*(17+nind*2),MPI_DOUBLE_PRECISION, &
                                j,41,MPI_COMM_WORLD,request2(ncount3),ierr)
                            endif
                        end if
                    end do
                    ! The following work doesn't care about if this step has been finished until the connection.
                end if

            end if

        end do

        ! then connect received particles
        if(ncount3 > 0) then
            call MPI_WAITALL(ncount3,request2(1:ncount3),status,ierr)
            ! if receive happened, we wait

            call connect_recv<<<ceiling(dble(sum(N_recv_all))/tPB),&
            tPB>>>(P_de,P_recv,np_active,sum(N_recv_all),17+nind*2)
            np_active = np_active + sum(N_recv_all)

            deallocate(P_recv) ! deallocate P_send after transfer back to host
        endif

        call Update_C_Array <<< ceiling(dble(np_active)/tPB),tPB >>> (P_de,C_de,dx,dy,dz_de, &
        nz,ix1,iy1,nnx1,nny1,buff,mean_age_de,mean_comp_de,total_mass_de,np_active)

        ! then separate inactive particles
        call prepare_exit<<<ceiling(dble(np_active)/tPB),tPB>>>(P_de(:,8),d_isValid,np_active)
        call thrustscan(d_isValid,np_active,d_indices)
        ! active and ET-inactive particles in overlap zone are sent
        ! out of domain, all outfolw-inactive, received ET-inactive are scanned here

        if(ncount2 > 0) call MPI_WAIT(request3(ncount2),status,ierr) ! no if condition since send anyway
        N_send = np_active - d_indices(np_active)

        if(ncount4 > 0)then
            call MPI_WAIT(request4(ncount4),status,ierr)
            deallocate(P_send)
        end if

        if(N_send > 0) then

            allocate(holes(N_send),P_send(N_send,17+nind*2))
            ! P_send also be used for P_exit

            call prepare_holes<<<ceiling(dble(np_active)/tPB),&
            tPB>>>(holes,d_indices,d_isValid,np_active)

            call compaction_inplcae<<<ceiling(dble(np_active)/tPB),&
            tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)
            ! no need to deallocate P_send, it will be done next timestep before sending

            np_active = d_indices(np_active); deallocate(holes)

        end if

    end subroutine particle_exchange

end module exchange_particles