        ! Build neighbor list.
        call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
        tPB>>>(zone_de,nnx1,nny1,buff,d_isValid,rank,t_rank)
        neigh_list = d_isValid(1:t_rank)
        ! don't need to do this every timestep. Only after update of decomposition
        if(sum(neigh_list) > 0) allocate(N_recv_all(sum(neigh_list)))
        ! used to know how many particles this rank will receive
        ! N_recv_all should be set after update of decomposition.
        ! its size is the number of its neighbors. it will have elements of value 0,
        ! this means that neighbor will not send particles to this rank this time step.

        ncount2 = 0; ncount4 = 0
        do i = 0, t_rank-1
            !-----------------------------------------
            ! receive part. N_recv_all and P_recv are for all neighbors which send particles.
            !-----------------------------------------
            ! Rank by rank. For each rank, other ranks send particles to it while it must
            ! not send particles to itself. So the following actions can be reached.
            if(i == rank .and. sum(neigh_list) > 0) then
                ! neigh_list is an 1D array of size t_rank. If an element is 1, it has this neighbor.
                ! sum(neigh_list) > 0, this means there must be at least one neighbor.

                ncount = 0
                do j = 0, t_rank-1
                    if(neigh_list(j+1) == 1) then
                        ncount = ncount + 1
                        call MPI_IRECV(N_recv_all(ncount),1,MPI_INTEGER,j,40,MPI_COMM_WORLD,request1(ncount),ierr)
                    end if
                end do
                ! the location of each neighbor is fixed in N_recv_all, since there is the message envelope
                ! each neighbor will send number of particles that will be sent to this rank, even it is zero.
                ! finally, ncount is the number of neighbors. We set the size of request1 as t_rank.

                call MPI_WAITALL(ncount,request1(1:ncount),status,ierr)
                ! next step it needs the sum of N_recv_all, so we have to wait here.

                ncount = 0; ncount3 = 0
                if(sum(N_recv_all) > 0) then
                    ! at least one neighbor sent. if not, no need to do receive
                    allocate(P_recv(sum(N_recv_all),17+nind*2))
                    ! do sum of N_recv_all and allocate P_recv
                    do j = 0, t_rank-1
                        if(neigh_list(j+1) == 1) then
                            ncount = ncount + 1
                            if(N_recv_all(ncount) > 0) then ! exist and send
                                ncount3 = ncount3 + 1
                                call MPI_IRECV(P_recv(sum(N_recv_all(1:ncount-1))+1:sum(N_recv_all(1:ncount)),:), &
                                N_recv_all(ncount)*(17+nind*2),MPI_DOUBLE_PRECISION, &
                                j,41,MPI_COMM_WORLD,request2(ncount3),ierr)
                            endif
                        end if
                    end do
                    ! The following work doesn't care about if this step has been finished until the connection.
                end if

            end if

            !-----------------------------------------
            ! send part. one by one neighbor, N_send and P_send are repeatedly used.
            !-----------------------------------------
            if(neigh_list(i+1) == 1) then

                ncount2 = ncount2 + 1

                call prepare_neighbor<<<ceiling(dble(np_active)/tPB),&
                tPB>>>(P_de(:,8),d_isValid,np_active,i) ! get d_isValid
                call thrustscan(d_isValid,np_active,d_indices)

                if(ncount2 /= 1) call MPI_WAIT(request3(ncount2-1),status,ierr)
                N_send = np_active - d_indices(np_active) ! this should be legal
                call MPI_ISEND(N_send,1,MPI_INTEGER,i,40,MPI_COMM_WORLD,request3(ncount2),ierr)

                if(N_send > 0) then
                    ncount4 = ncount4 + 1
                    if(ncount4 /= 1) then
                        call MPI_WAIT(request4(ncount4-1),status,ierr)
                        deallocate(P_send)
                    end if
                    allocate(holes(N_send),P_send(N_send,17+nind*2))

                    call prepare_holes<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,np_active)
                    ! block size cannot be reduced ？？

                    call compaction_inplcae<<<ceiling(dble(np_active)/tPB),&
                    tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)

                    call MPI_ISEND(P_send,N_send*(17+nind*2),MPI_DOUBLE_PRECISION,i,41,&
                                MPI_COMM_WORLD,request4(ncount4),ierr)
                    ! gpu send since it is a gpu buffer?

                    np_active = d_indices(np_active); deallocate(holes)
                end if

            end if

        end do

        ! then separate inactive particles
        call prepare_exit<<<ceiling(dble(np_active)/tPB),tPB>>>(P_de(:,8),d_isValid,np_active)
        call thrustscan(d_isValid,np_active,d_indices)

        call MPI_WAIT(request3(ncount2),status,ierr) ! no if condition since send anyway
        N_send = np_active - d_indices(np_active)

        if(ncount4 > 0)then
            call MPI_WAIT(request4(ncount4),status,ierr)
            deallocate(P_send)
        end if
        allocate(holes(N_send),P_send(N_send,17+nind*2))
        ! P_send also be used for P_exit

        call prepare_holes<<<ceiling(dble(np_active)/tPB),&
        tPB>>>(holes,d_indices,d_isValid,np_active)

        call compaction_inplcae<<<ceiling(dble(np_active)/tPB),&
        tPB>>>(holes,d_indices,d_isValid,P_de,P_send,np_active,17+nind*2)
        ! no need to deallocate P_send, it will be done next timestep before sending

        np_active = d_indices(np_active); deallocate(holes)

        if(ncount3 > 0) &
        call MPI_WAITALL(ncount3,request2(1:ncount3),status,ierr)

        call connect_recv<<<ceiling(dble(sum(N_recv_all))/tPB),&
        tPB>>>(P_de,P_recv,np_active,sum(N_recv_all),17+nind*2)
        np_active = np_active + sum(N_recv_all)

        deallocate(P_send,P_recv)
        ! this should be considered. size of N_recv_all should not change
        ! if decomposition is not changed.