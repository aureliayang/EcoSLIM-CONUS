module utilities
    use cudafor

    use variable_list, only: Xgmin, Xgmax, Ygmin, Ygmax
    use variable_list, only: buff, ppx, qqy, rank

    use variable_list, only: Zonet_old, Zonet_new

    use variable_list, only: P_de,C_de,Vx_de,Vy_de,Vz_de,dz_de, &
    Ind_de,Saturation_de,Porosity_de,EvapTrans_de,CLMvars_de, &
    PET_balance_de,PET_balance_da_de
    use variable_list, only: out_np_de,ET_np_de,out_age_de,out_mass_de, &
    out_comp_de,ET_age_de,ET_mass_de,ET_comp_de
    use variable_list, only: mean_age_de,mean_comp_de,mean_mass_de,total_mass_de
    use variable_list, only: d_isValid,d_indices

    use variable_list, only: P,C,Vx,Vy,Vz,dz,dz2, &
    Ind,Saturation,Porosity,EvapTrans,CLMvars, &
    PET_balance,PET_balance_da
    use variable_list, only: out_np,ET_np,out_age,out_mass, &
    out_comp,ET_age,ET_mass,ET_comp
    use variable_list, only: mean_age,mean_comp,mean_mass,total_mass
    use variable_list, only: DEM,n_constituents

    use mrand, only: h

contains
    subroutine copy_grid(ix1,iy1,ix2,iy2,nnx1,nny1, &
                         nnx2,nny2,grid,rank)
        implicit none
        integer:: ix1, iy1, ix2, iy2, nnx1, nny1, nnx2, nny2
        integer:: grid(:,:), rank

        ix1  = grid(rank+1,1); nnx1 = grid(rank+1,3)
        iy1  = grid(rank+1,2); nny1 = grid(rank+1,4)

        ix2  = ix1 - buff; nnx2 = nnx1 + 2*buff
        iy2  = iy1 - buff; nny2 = nny1 + 2*buff

        if(ix1 == Xgmin) then
            ix2  = ix1
            nnx2 = nnx2 - buff
        endif
        if(ix1+nnx1 == Xgmax) nnx2 = nnx2 - buff

        if(iy1 == Ygmin) then
            iy2  = iy1
            nny2 = nny2 - buff
        endif
        if(iy1+nny1 == Ygmax) nny2 = nny2 - buff

    end subroutine copy_grid

    subroutine allocate_arrays_temp(nnx1,nnx2,nz,nzclm)
        use mpi
        implicit none
        !--------------------------------------------------------------------
        ! allocate arrays
        if (rank /= ppx*qqy) then
            allocate(DEM(-buff+1:nnx1+buff,-buff+1:nny1+buff))

            allocate(Vx(-buff+1:nnx1+1+buff,-buff+1:nny1+buff,nz), &
                    Vy(-buff+1:nnx1+buff,-buff+1:nny1+1+buff,nz), &
                    Vz(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz+1))

            allocate(Saturation(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    Porosity(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    EvapTrans(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                            Ind(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))

            allocate(EvapTrans_da(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
            allocate(CLMvars(-buff+1:nnx1+buff,-buff+1:nny1+buff,nzclm))
            allocate(C(n_constituents,-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))

            ! Intialize everything to Zero
            Vx = 0.0d0
            Vy = 0.0d0
            Vz = 0.0d0
            Saturation = 0.0d0
            Porosity = 0.0d0
            EvapTrans = 0.0d0
            EvapTrans_da = 0.0d0
            C = 0.0d0
            ! Ind is initialized when reading it.
            ! CLMvars is initialized when reading it.
            ! DEM is initialized when reading it.

            !-------------------------------------
            ! allocate arrays on GPU
            allocate(Vx_de(-buff+1:nnx1+1+buff,-buff+1:nny1+buff,nz), &
                    Vy_de(-buff+1:nnx1+buff,-buff+1:nny1+1+buff,nz), &
                    Vz_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz+1), &
                    Ind_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
            allocate(Saturation_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    Porosity_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    EvapTrans_de(-buff+1:nnx1+buff,-buff+1:nny1+buff,nz), &
                    CLMvars_de(-buff+1:nnx1+buff,-buff+1:nny1+buff), &
                        Zone_de(-buff+1:nnx1+buff,-buff+1:nny1+buff))
            allocate(C_de(n_constituents,-buff+1:nnx1+buff,-buff+1:nny1+buff,nz))
        end if
    end subroutine allocate_arrays_temp

    subroutine allocate_arrays_const(nz,np,t_rank)
        use mpi
        implicit none

        if (rank /= ppx*qqy) then
            allocate(out_np_cpu(1),ET_np_cpu(1))
            allocate(out_age_cpu(1),out_mass_cpu(1),out_comp_cpu(3))
            allocate(ET_age_cpu(1),ET_mass_cpu(1),ET_comp_cpu(3))
            allocate(mean_age(1),mean_comp(1),mean_mass(1),total_mass(1))

            allocate(out_np_de(1),ET_np_de(1))
            allocate(out_age_de(1),out_mass_de(1),out_comp_de(3))
            allocate(ET_age_de(1),ET_mass_de(1),ET_comp_de(3))
            allocate(mean_age_de(1),mean_comp_de(1),mean_mass_de(1),total_mass_de(1))

            allocate(neigh_listn(t_rank),neigh_listo(t_rank))
            allocate(neigh_list(t_rank),nump(t_rank))
            allocate(d_isValid(np),d_isValid(np),h(np))

            allocate(dz(nz),dz2(nz),dz_de(nz))
            allocate(conc_header(n_constituents))

            allocate(PET_balance(2),PET_balance_da(2))
            allocate(PET_balance_de(2),PET_balance_da_de(2))
            PET_balance = 0.0d0
            PET_balance_da = 0.0d0

            allocate(grid(t_rank-1,4))
        endif

    end subroutine allocate_arrays_const

    subroutine deallocate_temp()
        use mpi
        implicit none

        if (rank /= ppx*qqy) then
            deallocate(Vx_de,Vy_de,Vz_de,C_de,Ind_de,Saturation_de, &
            Porosity_de,EvapTrans_de,CLMvars_de)
            ! zone_de will be deallocated in exchange zone after the use
            deallocate(Vx,Vy,Vz,C,Ind,Saturation,Porosity,EvapTrans, &
            EvapTrans_da,CLMvars,DEM)
        endif

    end subroutine deallocate_temp

    subroutine combine_p_sum()
        ! this part to utilities
        ! build p_sum on each rank
        call build_p_sum<<< ceiling(dble(np_active)/tPB),tPB >>>()

        ! send p_sum to the p_sum on rank of ppx*qqy
        ! now we could use send and recv while we latter can change to scatter/gather
        ! p_sum is the same name but the size is the whole domain on rank ppx*qqy
    end subroutine combine_p_sum

    attributes(global) subroutine build_p_sum()
        implicit none
        integer:: Ploc(3), ii, lock, temp
        ! p_sum doesn't need buffer zone

        ii = (blockIdx%x - 1) * blockDim%x + threadIdx%x
        lock = 1

        if(ii < np_active) then
            P(ii,1) = P(ii,1) - ix1*dx
            P(ii,2) = P(ii,2) - iy1*dy

            Ploc(1) = floor(P(ii,1) / dx)
            Ploc(2) = floor(P(ii,2) / dy)

            temp = atomicAdd(p_sum(Ploc(1)+1,Ploc(2)+1),lock)

            P(ii,1) = P(ii,1) + ix1*dx
            P(ii,2) = P(ii,2) + iy1*dy
        endif

    end subroutine build_p_sum

    subroutine subzones_for_transfer()
        ! get Zone_de and Zones_new
        ! we use send and recv now, and will be changed to scatter and gather later
        implicit none
        integer:: ix1, iy1, ix2, iy2, nnx1, nny1, nnx2, nny2
        real(8)::

        if(rank == ppx*qqy) then
            do i = 0, ppx*qqy - 1
                ! build and send
                call MPI_ISEND(Zonet_new(:,:),N_send*(17+nind*2),MPI_DOUBLE_PRECISION,i,41,&
                MPI_COMM_WORLD,request4(ncount4),ierr)
            end do
        else
        endif
    end subroutine subzones_for_transfer

    subroutine receive_Zone_de(grid,rank)
        ! we use send and recv now, and will be changed to scatter and gather later
        use mpi
        implicit none
        integer:: ix1, iy1, nnx1, nny1
        integer:: ix1r, iy1r, nnx1r, nny1r
        integer:: ncount1, request1(ppx*qqy-1)
        integer:: ncount2, request2(ppx*qqy-1)
        integer:: rank, grid(:,:)
        integer:: i

        if(rank /= ppx*qqy) then
            ncount2 = rank
            ix1r = grid(rank+1,1);  iy1r = grid(rank+1,2)
            nnx1r = grid(rank+1,3); nny1r = grid(rank+1,4)
            call MPI_IRECV(Zone_de(1:nnx1r,1:nny1r),nnx1r*nny1r,MPI_INTEGER, &
            ppx*qqy,41,MPI_COMM_WORLD,request2(ncount2),ierr)
            ! Zone_de should be initialized as -1 before this call
        else
            ncount1 = 0
            do i = 0, ppx*qqy - 1
                ncount1 = ncount1 + 1
                 ix1 = grid(i+1,1);  iy1 = grid(i+1,2)
                nnx1 = grid(i+1,3); nny1 = grid(i+1,4)
                call MPI_ISEND(Zonet_new(ix1+1:ix1+nnx1,iy1+1:iy1+nny1),nnx1*nny1, &
                MPI_INTEGER,i,41,MPI_COMM_WORLD,request1(ncount1),ierr)
            end do
        endif
    end subroutine receive_Zone_de

end module utilities