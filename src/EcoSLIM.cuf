!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels. This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths. It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!--------------------------------------------------------------------
! 06/26/2021 GPU version, Chen Yang (cy15@princeton.edu)
!--------------------------------------------------------------------
program EcoSLIM
    ! use mpi
    ! use cudafor
    ! use thrust
    ! use variable_list
        #if _HDF5 == 1
    use hdf5_file_read
        #endif
    ! use hdf5_file_write
    use mrand
    use utilities
    use mpiDeviceUtil
    use subdomain_bound
    use create_subdomain
    use eco_compact_util
    use eco_updateC_sortP
    use eco_read_input
    use eco_working_comm
    use eco_particle_loop
    use eco_particle_add
    use eco_particle_pme
    use eco_particle_init
    use eco_particle_exch1
    use eco_particle_exch2
    use eco_particle_lb
    use eco_particle_separ

!--------------------------------------------------------------------
    implicit none

    integer:: istat
    logical:: separ_flag, pme_flag, fore_flag

!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)
    write(ranknum,'(i5.5)') rank

        #if _HDF5 == 1
    call open_hdf5_interface()
        #endif

!--------------------------------------------------------------------
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    call initialize_time()

    istat = cudaEventCreate(startEvent)
    istat = cudaEventCreate(stopEvent)

    Total_time1 = mpi_wtime()

    open(30,file='Debug.'//trim(adjustl(ranknum))//'.txt')

    call read_input()

!--------------------------------------------------------------------
    call alloc_arrays_const()
    ! now we know the total dimension

    call global_xyz()

    call working_comm()
!--------------------------------------------------------------------
    if(np_ic /= -1) call gridinfo()
    if(np_ic == -1) call read_grid_Zone()
    ! if not restart, we do decompostion and build topology
    ! gridinfo is to get grid and Zonet_new
    ! if restart, read grid and Zonet_new in
    ! we want to restart at n*add_f, and also 8760/m since we need results of a complete year

    if (work_comm /= MPI_COMM_NULL) then

        call copy_grid(map_sub) ! doing this on workers

        call alloc_arrays_temp(Porosity_cpu) ! doing this on workers

        call scan_zone<<< ceiling(dble((nnx1_c+2*buff)*(nny1_c+2*buff))/tPB), &
            tPB >>> (buff,neigh_list,rank,ppx*qqy)

        call local_xyz()
        !--------------------------------------------------------------------
        if(np_ic /= -1) then
            ! Saturation = 0.8d0
            write(filenum,'(i5.5)') pft1-1
            fname = trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Saturation_cpu,fname,nx_c,ny_c,nz_c)
            Saturation(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:) = &
            Saturation_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c,:)

            Saturation_de = Saturation

            call createRand_init<<< ceiling(dble(nnx1_c*nny1_c*nz_c)/tPB),tPB >>> (nx_c,ny_c,nz_c, &
                rank,np_ic,nnx1_c,nny1_c)

        endif

        ! Define initial particles' locations and mass
        if (np_ic > 0)  then
            fore_flag = .false.
            call initial_forward(fore_flag)
            if(fore_flag) goto 9090
        end if

        if(np_ic == -1) then
        end if

        if(np_ic < -1) then
        end if

    else
        call MPI_IRECV(map_sub,1,MPI_INTEGER,t_rank-1,40,MPI_COMM_WORLD,rq2,ierr)
    end if
    ! if the initial number of GPUs equals the total number, this will be skipped.

    call file_open()

    ! Intialize cuRand device API and this part need to check through again.
    call createRand_loop<<< ceiling(dble(np)/tPB),tPB >>> (np, rank, pfnt)

    call MPI_Barrier(MPI_COMM_WORLD,ierr)
!--------------------------------------------------------------------
    pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

    do kk = outkk, pfnt

        if(work_comm /= MPI_COMM_NULL) then

            ! reset ParFlow counter for cycles
            if (mod((kk-1),(pft2-pft1+1)) == 0)  pfkk = pft1 - 1

            ! adjust the file counters
            pfkk = pfkk + 1

                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif

            ! Read the velocities computed by ParFlow
            write(filenum,'(i5.5)') pfkk

                #if _HDF5 == 1
            fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vx_h5,nnx2_c+1,nny2_c,nz_c)
            Vx(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c+1,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,1:nz_c)=Vx_h5
                #else
            fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vx_cpu,fname,nx_c+1,ny_c,nz_c)
            Vx(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c+1,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,1:nz_c) = &
            Vx_cpu(ix2_c+1:ix2_c+nnx2_c+1,iy2_c+1:iy2_c+nny2_c,1:nz_c)
                #endif

                #if _HDF5 == 1
            fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vy_h5,nnx2_c,nny2_c+1,nz_c)
            Vy(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c+1,1:nz_c)=Vy_h5
                #else
            fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vy_cpu,fname,nx_c,ny_c+1,nz_c)
            Vy(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c+1,1:nz_c) = &
            Vy_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c+1,1:nz_c)
                #endif

                #if _HDF5 == 1
            fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vz_h5,nnx2_c,nny2_c,nz_c+1)
            Vz(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,1:nz_c+1)=Vz_h5
                #else
            fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Vz_cpu,fname,nx_c,ny_c,nz_c+1)
            Vz(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,1:nz_c+1) = &
            Vz_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c,1:nz_c+1)
                #endif

                #if _HDF5 == 1
            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Saturation_h5,nnx2_c,nny2_c,nz_c)
            Saturation(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:)=Saturation_h5
                #else
            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Saturation_cpu,fname,nx_c,ny_c,nz_c)
            Saturation(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:) = &
            Saturation_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c,:)
                #endif

            if (clmtrans) then
                ! Read in the Evap_Trans
                    #if _HDF5 == 1
                fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.h5'
                call read_h5_file(EvapTrans_h5,nnx2_c,nny2_c,nz_c)
                EvapTrans(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:)=EvapTrans_h5
                    #else
                fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.pfb'
                call pfb_read(EvapTrans_cpu,fname,nx_c,ny_c,nz_c)
                EvapTrans(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:) = &
                EvapTrans_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c,:)
                    #endif

                if (mod((kk-1),add_f) == 0) EvapTrans_da = 0.d0
                where (EvapTrans > 0.d0) EvapTrans_da = EvapTrans_da + EvapTrans
                ! LB has to be n*add_f??? yes!

                if (clmfile) then
                    ! Read in CLM output file @RMM to do make this input
                    ! fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.h5'
                    ! call read_h5_file(CLMvars,5)
                    CLMvars = 0.d0
                    ! fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.pfb'
                    ! call pfb_read(CLMvars_cpu,fname,nx,ny,nzclm)
                    ! CLMvars(ix2_c-ix1_c+1:ix2_c-ix1_c+nnx2_c,iy2_c-iy1_c+1:iy2_c-iy1_c+nny2_c,:) = &
                    ! CLMvars_cpu(ix2_c+1:ix2_c+nnx2_c,iy2_c+1:iy2_c+nny2_c,:)

                end if
            end if

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(IO_time_read,startEvent,stopEvent)
                #endif

            ! Determine whether to perform forward or backward patricle tracking
            Vx = Vx * V_mult
            Vy = Vy * V_mult
            Vz = Vz * V_mult

            call host2device()

            !----------------------------------------

            if (clmtrans) then
                if (np_active < np) then
                    pme_flag = .false.
                    call particle_pme(pme_flag)
                    if(pme_flag) exit
                end if
            end if

            !----------------------------------------
            ! If you don't physically separate the particles, the interior and peripheric particles
            ! are mixed in the array, the thrustscan and movement of interior particles may happen at
            ! the same time, but they are all read-only operations, so it doesn't matter.
            !!!!! ****since add_f is large, after new GPU is added, in the following add_f steps, in fact,
            !!!!! ****there are no particles on this new GPU, so we need if condition for the following work.

            if(separate .and. mod(kk,transfer) == 0) then
                ! do separation when we do transfer. Otherwise, the interior part will not be calculated.
                if(np_active > 0) then
                    separ_flag = .false.
                    call separ_particles(separ_flag)
                    if (separ_flag) exit
                else
                    N_peri = np_active
                    N_inte = 0
                end if
            else
                N_peri = np_active
                N_inte = 0
            endif
            !----------------------------------------
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif
            ! peripheric particles
            call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
                xmin2_c,ymin2_c,zmin2_c,xmax2_c,ymax2_c,zmax2_c, &
                N_inte,N_peri,reflect,map_sub,kk,transfer)

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(peri_time,startEvent,stopEvent)
                #endif
            !----------------------------------------
            if(transfer > 0) then
                if(mod(kk,transfer) == 0) then
                    call particle_exchange1()
                end if
            end if

            if(transfer < 0) then
                if(mod(kk,transfer) == 0) then
                    call particle_exchange2()
                    ! currently, LB not supported
                end if
            end if

            if(np_active > 0) call UpdateC_SortP()
            !----------------------------------------
            mean_age    = mean_age_de
            mean_comp   = mean_comp_de
            total_mass  = total_mass_de
            if(total_mass(1) > 0.d0) mean_age(1)  = mean_age(1)/total_mass(1)
            if(total_mass(1) > 0.d0) mean_comp(1) = mean_comp(1)/total_mass(1)
            mean_mass = total_mass/dble(np_active)
            ET_np_cpu   = ET_np_de
            out_np_cpu  = out_np_de
        end if

        write(20,'(3(i8),7(1x,e12.5,1x),3(i8),6(i12))') kk,pfkk,outkk,Time_Next(kk), &

        mean_age,mean_comp,mean_mass,total_mass,PET_balance(1), &
        !
        PET_balance(2),i_added_particles,ET_np_cpu(1),Out_np_cpu(1), &
        !
        np_active,N_peri,N_inte,N_exit,sum(N_send),sum(N_recv)
        !

        write(17,'(3(i8),35(1x,e12.5,1x))') kk,pfkk,np_active, &

        IO_time_read,h2d_time,scan_time,thrust_time,copy_time,add2_time,reduce_time, &
        ! 4,         5,       6,        7,          8,        9,        10
        sort_time1,sort_time2,sort_time3,sort_time4,sort_time5, &
        ! 11,      12,         13,        14,       15
        sort_time6,sort_time7,sort_time9,sort_time10, &
        ! 16,      17,         18,       19
        peri_time,tran_time1,tran_time2,tran_time3,tran_time4,tran_time5, &
        ! 20,     21,        22,        23,        24,        25
        tran_time6,tran_time7,tran_time8,tran_time9,inte_time,tran_time10, &
        ! 26,      27,        28,        29,        30,       31
        C_time,exit_time,tran_time11,tran_time12,tran_time13,tran_time14,tran_time15
        ! 32,  33

        flush(17);flush(20)

        if(LB > 0) then
            if(mod(kk,LB) == 0) then
                call load_balance(Porosity_cpu)
            end if
        end if

        call MPI_Barrier(MPI_COMM_WORLD,ierr)

    end do
    9090 continue

    call null_texture()

    Total_time2 = mpi_wtime()

    istat = cudaEventDestroy(startEvent)
    istat = cudaEventDestroy(stopEvent)

    write(11,*)
    write(11,*) '###  Execution Finished'
    write(11,*)
    write(11,*) 'Simulation Timing and Profiling:'
    write(11,'("Total Execution Time (s):",e12.5)') Total_time2 - Total_time1

    call file_close()

        #if _HDF5 == 1
    call close_hdf5_interface()
        #endif
    if(work_comm /= MPI_COMM_NULL) call MPI_COMM_FREE(work_comm, ierr)
    call MPI_GROUP_FREE(work_group, ierr)
    call MPI_GROUP_FREE(world_group, ierr)
    call MPI_FINALIZE(ierr)

end program EcoSLIM