!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels. This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths. It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!--------------------------------------------------------------------
! 06/26/2021 GPU version, Chen Yang (cy15@princeton.edu)
!--------------------------------------------------------------------
program EcoSLIM
    ! use mpi
    ! use cudafor
    use mrand
    ! use thrust
    use utilities
    use particle_loop
    use mpiDeviceUtil
    use variable_list
    ! use hdf5_file_read
    ! use hdf5_file_write
    use subdomain_bound
    use create_subdomain
    use add_particles
    use compact_array
    ! use exchange_zone
    use exchange_particles1
    use exchange_particles2
    use UpdateC_SortP

    implicit none
    ! integer(8),allocatable,pinned:: p_num_cpu(:,:)
    ! type(curandStateXORWOW),allocatable,pinned:: h_cpu(:)
    real(8),allocatable:: Saturation_cpu(:,:,:),Porosity_cpu(:,:,:)
    real(8),allocatable:: EvapTrans_cpu(:,:,:),CLMvars_cpu(:,:,:)
    real(8),allocatable:: Vx_cpu(:,:,:),Vy_cpu(:,:,:),Vz_cpu(:,:,:)
    integer,allocatable:: rq1(:)
    integer:: istat, temp, rq2
    logical:: flag
!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)
    write(ranknum,'(i5.5)') rank

    ! call open_hdf5_interface()
!--------------------------------------------------------------------
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    ! How to do timing is also a new question when we do asyn later.
    ! Set up timing
    Total_time1 = 0.d0
    Total_time2 = 0.d0
    t1 = 0.d0
    t2 = 0.d0

    IO_time_write = 0.d0
    parallel_time = 0.d0

    IO_time_read = 0.
    h2d_time = 0.
    d2h_time = 0.
    add_time = 0.

    sort_time = 0.
    peri_time = 0.
    inte_time = 0.
    exit_time = 0.
    C_time    = 0.

    scan_time   = 0.
    copy_time   = 0.
    add2_time   = 0.
    thrust_time = 0.

    istat = cudaEventCreate(startEvent)
    istat = cudaEventCreate(stopEvent)

        #if _TIMING == 1
    Total_time1 = mpi_wtime()
        #endif

    open(30,file='Debug.'//trim(adjustl(ranknum))//'.txt')
!--------------------------------------------------------------------
    ! Read inputs, set up domain, write the log file
    ! open SLIM input .txt file
    open (10,file='slimin.txt')

    ! read SLIM run name
    read(10,*) runname

    ! read ParFlow run name
    read(10,*) pname

    ! read DEM file name
    read(10,*) DEMname

    if(rank == 0) then
        ! open/create/write the output log.txt file. If doesn't exist, it's created.
        open(11,file=trim(runname)//'_log.txt')
        write(11,*) '### EcoSLIM Log File'
        write(11,*)
        write(11,*) 'run name:',trim(runname)
        write(11,*)
        write(11,*) 'ParFlow run name:',trim(pname)
        write(11,*)
        if (DEMname /= '') then
            write(11,*) 'ParFlow DEM name:',trim(DEMname)
        else
            write(11,*) 'Not reading ParFlow DEM'
        end if
        write(11,*)
    endif ! rank = 0, write logfile through channel 11

    ! read domain number of cells and number of particles to be injected
    read(10,*) nx_c
    nx = nx_c
    read(10,*) ny_c
    ny = ny_c
    read(10,*) nz_c
    nz = nz_c
    read(10,*) nCLMsoil
    read(10,*) ppx
    read(10,*) qqy
    read(10,*) transfer
    read(10,*) separate
    read(10,*) LB
    ! read in number of particles for IC (if np_ic = -1 then restart from a file)
    read(10,*) np_ic

    ! read in the number of particles total
    read(10,*) np

    if(rank == 0) then
        ! check to make sure we don't assign more particles for IC than we have allocated
        ! in total
        if (np_ic > np) then
            write(11,*) 'warning NP_IC greater than IC'
            np = np_ic
        end if
        ! write nx, ny, nz, and np in the log file
        write(11,*) 'Grid information'
        write(11,*) 'nx:',nx_c
        write(11,*) 'ny:',ny_c
        write(11,*) 'nz:',nz_c
        write(11,*)
        write(11,*) 'Particle IC Information'
        write(11,*) 'np IC:',np_ic
        if (np_ic == -1) &
        write(11,*) 'Reading particle restart file:',trim(runname)//'_particle_restart.bin'
        write(11,*) 'np:',np
    endif ! rank=0

    ! nCLMsoil = 10 ! number of CLM soil layers over the root zone !this doesn't matter
    nzclm = 13 + nCLMsoil ! CLM output is 13+nCLMsoil layers for different variables not domain NZ,
                          ! e.g. 23 for 10 soil layers (default) and 17 for 4 soil layers (Noah soil
                          ! layer setup)
    n_constituents = 9

    allocate(dz(nz_c),dz_de(nz_c))
    ! have to do this here since the following read of dz

    ! read dx, dy as scalars
    read(10,*) dx_c
    dx = dx_c
    read(10,*) dy_c
    dy = dy_c
    ! read dz as an array
    read(10,*) dz(1:nz_c)
    ! read in (constant for now) ParFlow dt
    read(10,*) pfdt_c
    pfdt = pfdt_c
    ! read in parflow start and stop times
    read(10,*) pft1
    read(10,*) pft2
    read(10,*) tout1
    read(10,*) n_cycle
    read(10,*) add_f

    pfnt = n_cycle*(pft2-pft1+1)
    outkk = tout1 + 1

    ! IO control, each value is a timestep interval, e.g. 1= every timestep, 2=every other, 0 = no writing
    read(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
    read(10,*) ibinpntswrite  !  controls VTK, binary output of particle locations and attributes
    read(10,*) etwrite        !  controls ASCII ET output
    read(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
                              ! ages are mapped to a grid and written every N timesteps

    ! allocate and assign timesteps
    ! this can be a scalar and update every timestep
    allocate(Time_Next(pfnt-tout1))
    Time_Next = 0.d0
    do kk = 1, pfnt-tout1
        Time_Next(kk) = dble(kk+tout1)*pfdt
    end do
    Time_first = dble(outkk-1)*pfdt

    ! read in velocity multiplier
    read(10,*) V_mult
    ! do we read in clm evap trans?
    read(10,*) clmtrans
    ! do we read in clm output file?
    read(10,*) clmfile
    ! read in IC number of particles for flux
    read(10,*) iflux_p_res
    ! read in density h2o
    read(10,*) denh2o_c
    denh2o = denh2o_c
    ! read in diffusivity
    ! moldiff = (1.15e-9)*3600.d0
    read(10,*) moldiff_c
    moldiff = moldiff_c
    ! fraction of dx/Vx
    read(10,*) dtfrac_c
    dtfrac = dtfrac_c

    if(rank == 0) then
        ! wite out log file
        write(11,*)
        write(11,*) 'Grid Dimensions'
        write(11,'(" dx:",e12.5)') dx_c
        write(11,'(" dy:",e12.5)') dy_c
        write(11,'(" dz:",*(e12.5,", "))') dz(1:nz_c)
        write(11,*)
        write(11,*) 'Timestepping Information'
        write(11,'(" ParFlow delta-T, pfdt:",e12.5)') pfdt_c
        write(11,'(" ParFlow timesteps, pfnt:",i12)') pfnt
        write(11,'(" ParFlow start step, pft1:",i12)') pft1
        write(11,'(" ParFlow end step, pft2:",i12)') pft2
        write(11,'(" Output step start:",i12)') outkk
        write(11,'(" Time loops, cycles, n_cycle:",i12)') n_cycle
        write(11,'(" Total time steps:",i12)') pfnt

        write(11,*)
        write(11,*) 'V mult: ',V_mult,' for forward/backward particle tracking'
        write(11,*) 'CLM Trans: ',clmtrans,' adds / removes particles based on LSM fluxes'
        write(11,*)
        write(11,*) 'Physical Constants'
        write(11,*) 'denh2o: ',denh2o_c,' M/L^3'
        write(11,*) 'Molecular Diffusivity: ',moldiff_c,' '
        !write(11,*) 'Fractionation: ',Efract,' '
        write(11,*)
        write(11,*) 'Numerical Stability Information'
        write(11,'(" dtfrac: ",e12.5," fraction of dx/Vx")') dtfrac_c
    endif

    read(10,*) nind_c
    nind = nind_c
    read(10,*) Indname
    if(rank == 0) then
        write(11,*)
        write(11,*) 'Indicator File'
        write(11,*) nind_c, 'Indicators'
        flush(11)
        close(11)
    endif

    ! end of SLIM input
    close(10)

!--------------------------------------------------------------------
    call alloc_arrays_const()
    ! now we know the total dimension

    allocate(Saturation_cpu(nx_c,ny_c,nz_c),Porosity_cpu(nx_c,ny_c,nz_c))
    allocate(EvapTrans_cpu(nx_c,ny_c,nz_c),CLMvars_cpu(nx_c,ny_c,nzclm))
    allocate(Vx_cpu(nx_c+1,ny_c,nz_c),Vy_cpu(nx_c,ny_c+1,nz_c),Vz_cpu(nx_c,ny_c,nz_c+1))

    allocate(rq1(t_rank))

    allocate(Zonet_new(-buff+1:nx_c+buff,-buff+1:ny_c+buff))
    allocate(p_num(nx_c,ny_c),c_sum(max(nx_c,ny_c)))
    grid = 0
    Zonet_new = -1
    Zonet_new(1:nx_c,1:ny_c) = 0
    p_num = 1

    call global_xyz()
    ! restart file info inside, so have to call it before restart.

!--------------------------------------------------------------------
    ! define the dynamic working group
    do i = 1, t_rank
        work_ranks(i) = i - 1
    end do
    m_rank = ppx*qqy - 1  ! the maximum rank number of current work group
    call MPI_COMM_GROUP(MPI_COMM_WORLD,world_group,ierr)
    call MPI_GROUP_INCL(world_group,m_rank+1,work_ranks(1:m_rank+1),work_group,ierr)
    call MPI_COMM_CREATE(MPI_COMM_WORLD,work_group,work_comm,ierr)

    do i = 1, ppx*qqy
        t_GPUs(i) = 1
        l_GPUs(i,t_GPUs(i)) = i-1
        c_GPU(i) = l_GPUs(i,t_GPUs(i))
    end do

    if(rank < ppx*qqy) then
        map_sub = rank
    else
        map_sub = -1
        ! to make sure the new GPU started correctly
    end if
!--------------------------------------------------------------------
    if(np_ic /= -1) call gridinfo()
    ! if not restart, we do decompostion and build topology
    ! if restart, read necessary arrays in

    if (rank <= m_rank) then
        if(np_ic /= -1) then

            call copy_grid(map_sub) ! doing this on workers
                ! open(10,file='topology.'//trim(adjustl(ranknum)))
                !     write(10,'(8(i3,1x))') ix1,iy1,nnx1,nny1,ix2,iy2,nnx2,nny2
                !     write(10,'(8(i3,1x))') ix2-ix1+1,ix2-ix1+nnx2,iy2-iy1+1,iy2-iy1+nny2
                ! close(10)

            call alloc_arrays_temp() ! doing this on workers
                ! open(10,file='Zone_de_before.'//trim(adjustl(ranknum)))
                !     write(10,'(8(12(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                ! close(10)

            Zone_de(-buff+1:nnx1+buff,-buff+1:nny1+buff) = &
            Zonet_new(-buff+ix1+1:ix1+nnx1+buff,-buff+iy1+1:iy1+nny1+buff)

                open(10,file='Zone_de_after.'//trim(adjustl(ranknum))//'.txt')
                    !if(rank == 0) &
                    !    write(10,'(462(256(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    !if(rank == 1) &
                    !    write(10,'(463(256(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    !if(rank == 2) &
                    !    write(10,'(462(257(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    !if(rank == 3) &
                    !    write(10,'(463(257(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    if(rank == 0) &
                    write(10,'(18(34(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    if(rank == 1) &
                    write(10,'(18(34(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    if(rank == 2) &
                    write(10,'(18(34(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                    if(rank == 3) &
                    write(10,'(18(34(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                close(10)

        end if

        !--------------------------------------------------------------------
        call local_xyz()
            ! open(10,file='Local_range.'//trim(adjustl(ranknum))//'.txt')
            !     write(10,'(6(f15.2,1x))') Xgmin, Xgmax, Ygmin, Ygmax, Zgmin, Zgmax
            !     write(10,'(6(f15.2,1x))') Xmin1, Xmax1, Ymin1, Ymax1, Zmin1, Zmax1
            !     write(10,'(6(f15.2,1x))') Xmin2, Xmax2, Ymin2, Ymax2, Zmin2, Zmax2
            !     write(10,'(6(f15.2,1x))') Xmin3, Xmax3, Ymin3, Ymax3, Zmin3, Zmax3
            ! close(10)

        !--------------------------------------------------------------------
        ! Porosity = 0.3d0
        fname = trim(adjustl(pname))//'.out.porosity.pfb'
        call pfb_read(Porosity_cpu,fname,nx_c,ny_c,nz_c)
        Porosity(1+ix2-ix1:nnx2+ix2-ix1,1+iy2-iy1:nny2+iy2-iy1,:) = &
        Porosity_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

        if(np_ic /= -1) then
            ! Saturation = 0.8d0
            write(filenum,'(i5.5)') pft1-1
            fname = trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(Saturation_cpu,fname,nx_c,ny_c,nz_c)
            Saturation(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
            Saturation_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)
                ! allocate(h_cpu(np))
                ! h_cpu = h
                ! write(*,*) rank,'h_before',h_cpu(1)
            call createRand_init<<< ceiling(dble(nnx1*nny1*nz_c)/tPB),tPB >>> (nx_c,ny_c,nz_c, &
                rank,np_ic,nnx1,nny1)
                ! h_cpu = h
                ! write(*,*) rank,'h_after',h_cpu(1)
        endif

        ! dz_de  = dz
        ! P_de = P
        Ind_de = Ind
        Porosity_de   = Porosity
        Saturation_de = Saturation

        EvapTrans_T  => EvapTrans_de
        Saturation_T => Saturation_de
        Porosity_T   => Porosity_de
        Vx_T  => Vx_de
        Vy_T  => Vy_de
        Vz_T  => Vz_de
        Ind_T => Ind_de
        Zone_T => Zone_de
        ! dz_T  => dz_de
        ! P_T   => P_de
        ! d_isValid_T => d_isValid
        ! d_indices_T => d_indices

        ! Define initial particles' locations and mass
        if (np_ic > 0)  then
            np_active = 0
            pid = 0

            call scan_init_particles<<< ceiling(dble(nnx1*nny1*nz_c)/tPB), &
            tPB >>> (nnx1,nny1)

            call thrustscan(d_isValid,nnx1*nny1*nz_c,d_indices)

            np_active = d_indices(nnx1*nny1*nz_c)
            np_active = np_ic*np_active
            pid = np_active

            if (np_active >= np) then
                write(17,*) ' **Warning IC input but no paricles left'
                write(17,*) ' **Exiting code gracefully writing restart'
                goto 9090
            endif

            call add_init_particles<<< ceiling(dble(nnx1*nny1*nz_c)/tPB),tPB >>> &
            (np_ic,tout1,nnx1,nny1,ix1,iy1)

        end if

        call scan_zone<<< ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
            tPB >>> (nnx1,nny1,buff,neigh_list,rank,ppx*qqy)

            ! open(10,file='Neighbor.'//trim(adjustl(ranknum)))
            !     write(10,*) neigh_list
            !     write(10,*) N_recv
            ! close(10)

    end if

    open(20,file='Log_particles.'//trim(adjustl(ranknum))//'.txt')
    write(20,*) ' **** Transient Simulation Particle Accounting ****'
    write(20,*) ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass &
                PrecipIn ETOut NP_PrecipIn NP_ETOut NP_QOut NP_filtered'

    open(19,file='Exited_particles.'//trim(adjustl(ranknum))//'.bin')

    if(rank == 0) then

        open(13,file=trim(runname)//'_ET_output.txt')
        write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

        open(15,file=trim(runname)//'_flow_output.txt')
        write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

        open(16,file=trim(runname)//'_PET_balance.txt')
        write(16,*) 'TIME P[kg] ET[kg]'

    endif

    ! Intialize cuRand device API and this part need to check through again.
    call createRand_loop<<< ceiling(dble(np)/tPB),tPB >>> (np, rank, pfnt)

    if(rank >= m_rank+1 .and. rank < t_rank) then
        call MPI_IRECV(map_sub,1,MPI_INTEGER,t_rank-1,40,MPI_COMM_WORLD,rq2,ierr)
    end if

    call MPI_Barrier(MPI_COMM_WORLD,ierr)
!--------------------------------------------------------------------
    pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

    do kk = outkk, pfnt

    if(rank <= m_rank) then

        ! reset ParFlow counter for cycles
        if (mod((kk-1),(pft2-pft1+1)) == 0)  pfkk = pft1 - 1

        ! adjust the file counters
        pfkk = pfkk + 1

            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

        ! Read the velocities computed by ParFlow
        write(filenum,'(i5.5)') pfkk

        !fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vx,1)
        !Vx = 240.d0
        fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vx_cpu,fname,nx_c+1,ny_c,nz_c)
        Vx(ix2-ix1+1:ix2-ix1+nnx2+1,iy2-iy1+1:iy2-iy1+nny2,1:nz_c) = &
        Vx_cpu(ix2+1:ix2+nnx2+1,iy2+1:iy2+nny2,1:nz_c)

        !fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vy,2)
        !Vy = 240.d0
        fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vy_cpu,fname,nx_c,ny_c+1,nz_c)
        Vy(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2+1,1:nz_c) = &
        Vy_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2+1,1:nz_c)

        !fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vz,3)
        !Vz = 0.d0
        fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vz_cpu,fname,nx_c,ny_c,nz_c+1)
        Vz(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,1:nz_c+1) = &
        Vz_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,1:nz_c+1)

        !fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Saturation,0)
        !Saturation = 0.8d0
        fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Saturation_cpu,fname,nx_c,ny_c,nz_c)
        Saturation(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
        Saturation_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

        if (clmtrans) then
            ! Read in the Evap_Trans
            ! fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.h5'
            ! call read_h5_file(EvapTrans,0)
            ! EvapTrans = 0.d0
            ! EvapTrans(2,:,1) = 1.d-8
            fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(EvapTrans_cpu,fname,nx_c,ny_c,nz_c)
            EvapTrans(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
            EvapTrans_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

            if (mod((kk-1),add_f) == 0) EvapTrans_da = 0.d0
            where (EvapTrans > 0.d0) EvapTrans_da = EvapTrans_da + EvapTrans
            ! LB has to be n*add_f??? yes!

            if (clmfile) then
                ! Read in CLM output file @RMM to do make this input
                ! fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.h5'
                ! call read_h5_file(CLMvars,5)
                CLMvars = 0.d0
                ! fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.pfb'
                ! call pfb_read(CLMvars_cpu,fname,nx,ny,nzclm)
                ! CLMvars(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
                ! CLMvars_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

            end if
        end if

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(IO_time_read,startEvent,stopEvent)
            #endif

        ! Determine whether to perform forward or backward patricle tracking
        Vx = Vx * V_mult
        Vy = Vy * V_mult
        Vz = Vz * V_mult

            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

        Vx_de = Vx
        Vy_de = Vy
        Vz_de = Vz
        Saturation_de = Saturation
        EvapTrans_de  = EvapTrans
        if (mod(kk,add_f) == 0) EvapTrans_da_de = EvapTrans_da
        CLMvars_de    = CLMvars(:,:,11)
        ! probably this can be deallocated after adding of particles
        ! Now just think about the hourly add of particles.

        out_age_de  = 0.d0
        out_mass_de = 0.d0
        out_comp_de = 0.d0
        out_np_de   = 0

        et_age_de  = 0.d0
        et_mass_de = 0.d0
        et_comp_de = 0.d0
        et_np_de   = 0

        mean_age_de    = 0.d0
        mean_comp_de   = 0.d0
        total_mass_de  = 0.d0
        PET_balance_de = 0.d0

        C_de = 0.d0
        ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
        ! the following adding of particles.

        N_send = 0
        N_recv = 0

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(h2d_time,startEvent,stopEvent)
            #endif

        !----------------------------------------

        if (clmtrans) then
            if (np_active < np) then
                if (rank == c_GPU(map_sub+1)) then
                        #if _TIMING == 1
                        istat = cudaEventRecord(startEvent,0)
                        #endif

                    call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz_c)/tPB),tPB >>> ( &
                        PET_balance_de,nnx1,nny1)

                        #if _TIMING == 1
                        istat = cudaEventRecord(stopEvent,0)
                        istat = cudaEventSynchronize(stopEvent)
                        istat = cudaEventElapsedTime(scan_time,startEvent,stopEvent)
                        #endif

                    ! d_isValid has the length as P or of np which is assigned in input script
                    ! so nnx1*nny1*nz should be smaller than np
                    ! we don't care about the buffer zone for add of particles
                    ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                    ! in main program and pass arguments to other subroutines.

                    if (mod(kk,add_f) == 0) then
                        ! so the frequency to write restart file should be 24*n
                        ! i am not sure why i used 1752 previously but I feel it is not necessary

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif

                        ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                        call thrustscan(d_isValid,nnx1*nny1,d_indices)

                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(thrust_time,startEvent,stopEvent)
                            #endif

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif

                        temp = d_indices(nnx1*nny1)
                        i_added_particles = temp * iflux_p_res ! should be legal?

                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(copy_time,startEvent,stopEvent)
                            #endif

                        if(np_active + i_added_particles >= np) then
                            write(17,*) ' **Warning rainfall input but no paricles left'
                            write(17,*) ' **Exiting code gracefully writing restart'
                            goto 9090
                        endif

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif

                        if(i_added_particles > 0) &
                        call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                            tPB >>> (iflux_p_res,np_active,pid, &
                            nnx1,nny1,ix1,iy1,kk,tout1)
                        !!! ****you have to think about if i_added_particles == 0 ???

                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(add2_time,startEvent,stopEvent)
                            #endif

                        np_active = np_active + i_added_particles
                        pid = pid + i_added_particles
                    end if
                end if

                    PET_balance = PET_balance_de
                    call MPI_Barrier(work_comm,ierr)

                        #if _TIMING == 1
                        ! istat = cudaEventRecord(startEvent,0)
                        t1 = mpi_wtime()
                        #endif

                    call MPI_ALLReduce(MPI_IN_PLACE,PET_balance,2,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
                    ! be carefull???

                        #if _TIMING == 1
                        !istat = cudaEventRecord(stopEvent,0)
                        !istat = cudaEventSynchronize(stopEvent)
                        !istat = cudaEventElapsedTime(reduce_time,startEvent,stopEvent)
                        t2 = mpi_wtime()
                        reduce_time = (t2 - t1)*1000
                        #endif
            end if
        end if

        !----------------------------------------
        ! If you don't physically separate the particles, the interior and peripherical particles
        ! are mixed in the array, the thrustscan and movement of interior particles may happen at
        ! the same time, but they are all read-only operations, so it doesn't matter.
        !!!!! ****since add_f is large, after new GPU is added, in the following add_f steps, in fact,
        !!!!! ****there are no particles on this new GPU, so we need if condition for the following work.

            if(separate .and. np_active > 0) then
                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call particles_separation<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    d_isValid,xmin3,ymin3,xmax3,ymax3,np_active,ix1,iy1)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time1,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call thrustscan(d_isValid,np_active,d_indices)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time2,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                N_inte = d_indices(np_active)
                N_peri = np_active - N_inte
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time3,startEvent,stopEvent)
                    #endif

                if(np_active + N_peri >= np) then
                    write(17,*) ' **Warning particles separation out of bound'
                    write(17,*) ' **Exiting code gracefully writing restart'
                    goto 9090
                endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                allocate(holes(N_peri))
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time4,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,np_active)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time5,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call select2np_active<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                    holes,np_active,N_peri)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time6,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,0,np_active)
                    ! if no compaction, you will need d_isValid and if condition to judge
                    ! during the interior particles' movement
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time7,startEvent,stopEvent)
                    #endif

                !        #if _TIMING == 1
                !        istat = cudaEventRecord(startEvent,0)
                !        #endif
                !    !N_inte = d_indices(np_active);
                !        #if _TIMING == 1
                !        istat = cudaEventRecord(stopEvent,0)
                !        istat = cudaEventSynchronize(stopEvent)
                !        istat = cudaEventElapsedTime(sort_time8,startEvent,stopEvent)
                !        #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                deallocate(holes)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time9,startEvent,stopEvent)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                call connect_recv<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                    N_inte,N_peri)
                    ! if not connect, ther will be the risk that it is out of particles when do receiving
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(sort_time10,startEvent,stopEvent)
                    #endif
            else
                N_peri = np_active
                N_inte = 0
            endif
            !----------------------------------------
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif
            ! peripheric particles
            call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
                xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                xmin2,ymin2,zmin2,xmax2,ymax2,zmax2, &
                N_inte,N_peri,ix1,iy1,reflect,map_sub)

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(peri_time,startEvent,stopEvent)
                #endif
            !----------------------------------------
            if(transfer == 1) call particle_exchange1()
            if(transfer == 2) call particle_exchange2()
            if(np_active > 0) call Update_Sort()
        !----------------------------------------
        mean_age    = mean_age_de
        mean_comp   = mean_comp_de
        total_mass  = total_mass_de
        if(total_mass(1) > 0.d0) mean_age(1)  = mean_age(1)/total_mass(1)
        if(total_mass(1) > 0.d0) mean_comp(1) = mean_comp(1)/total_mass(1)
        mean_mass = total_mass/dble(np_active)
        ET_np_cpu   = ET_np_de
        out_np_cpu  = out_np_de

        write(20,'(3(i8),7(1x,e12.5,1x),3(i8),6(i12))') kk,pfkk,outkk,Time_Next(kk),mean_age,mean_comp, &
                                                      mean_mass,total_mass,PET_balance(1), &
                                                      PET_balance(2),i_added_particles,ET_np_cpu(1), &
                                                      Out_np_cpu(1),np_active,N_peri,N_inte,N_exit, &
                                                      sum(N_send),sum(N_recv)
                                                      ! here???

        write(17,'(3(i8),31(1x,e12.5,1x))') kk,pfkk,np_active,IO_time_read,h2d_time,scan_time,thrust_time,copy_time, &
                                            add2_time,reduce_time,sort_time1,sort_time2,sort_time3,sort_time4, &
                                            sort_time5,sort_time6,sort_time7,sort_time8,sort_time9,sort_time10, &
                                            peri_time,tran_time1,tran_time2,tran_time3,tran_time4,tran_time5, &
                                            tran_time6,tran_time7,tran_time8,tran_time9,inte_time, &
                                            tran_time10,C_time,exit_time

        flush(17);flush(20)
    end if

    ! gather np_active on the maximum rank
    if(LB > 0 .and. mod(kk,LB) == 0) then

        m_rank_o = m_rank
        call MPI_ALLGather(np_active,1,MPI_INTEGER,nump,1,MPI_INTEGER,MPI_COMM_WORLD,ierr)

        if(rank == t_rank - 1) then
            do i = 1, ppx*qqy

                temp = nump(l_GPUs(i,1) + 1)
                c_GPU(i) = l_GPUs(i,1)
                if(t_GPUs(i) > 1) then
                    do j = 2, t_GPUs(i)
                        if(nump(l_GPUs(i,j) + 1) < temp) then
                            ! l_GPUs store the ranks of the GPUs, so plus one
                            temp = nump(l_GPUs(i,j) + 1)
                            c_GPU(i) = l_GPUs(i,j)
                        end if
                    end do
                end if

                if(temp > 1e5 .and. (m_rank+1) < t_rank) then
                    m_rank = m_rank + 1
                    t_GPUs(i) = t_GPUs(i) + 1
                    l_GPUs(i,t_GPUs(i)) = m_rank
                    c_GPU(i) = m_rank
                    call MPI_ISEND(l_GPUs(i,1),1,MPI_INTEGER,m_rank,40,MPI_COMM_WORLD,rq1(m_rank),ierr)
                end if

            end do
        end if

        call MPI_BCAST(m_rank,1,MPI_INTEGER,t_rank-1,MPI_COMM_WORLD,ierr)
        call MPI_BCAST(t_GPUs,ppx*qqy,MPI_INTEGER,t_rank-1,MPI_COMM_WORLD,ierr)
        call MPI_BCAST(l_GPUs,ppx*qqy*t_rank,MPI_INTEGER,t_rank-1,MPI_COMM_WORLD,ierr)
        call MPI_BCAST(c_GPU,ppx*qqy,MPI_INTEGER,t_rank-1,MPI_COMM_WORLD,ierr)

        if (m_rank > m_rank_o) then

            if(rank <= m_rank_o) call MPI_COMM_FREE(work_comm,ierr)
            call MPI_GROUP_FREE(work_group,ierr)
            call MPI_GROUP_INCL(world_group,m_rank+1,work_ranks(1:m_rank+1),work_group,ierr)
            call MPI_COMM_CREATE(MPI_COMM_WORLD,work_group,work_comm,ierr)

            if(rank >= m_rank_o+1 .and. rank <= m_rank) then

                call MPI_TEST(rq2,flag,status,ierr)

                if(flag == .true. .or. map_sub >= 0) then

                    call copy_grid(map_sub)
                    ! make sure grid is on all ranks by bcast

                    call alloc_arrays_temp()

                    Zone_de(-buff+1:nnx1+buff,-buff+1:nny1+buff) = &
                    Zonet_new(-buff+ix1+1:ix1+nnx1+buff,-buff+iy1+1:iy1+nny1+buff)

                    call local_xyz()

                    ! Porosity = 0.3d0
                    fname = trim(adjustl(pname))//'.out.porosity.pfb'
                    call pfb_read(Porosity_cpu,fname,nx_c,ny_c,nz_c)
                    Porosity(1+ix2-ix1:nnx2+ix2-ix1,1+iy2-iy1:nny2+iy2-iy1,:) = &
                    Porosity_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

                    Ind_de = Ind
                    Porosity_de   = Porosity
                    EvapTrans_T  => EvapTrans_de
                    Saturation_T => Saturation_de
                    Porosity_T   => Porosity_de
                    Vx_T  => Vx_de
                    Vy_T  => Vy_de
                    Vz_T  => Vz_de
                    Ind_T => Ind_de
                    Zone_T => Zone_de

                    call scan_zone<<< ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
                    tPB >>> (nnx1,nny1,buff,neigh_list,map_sub,ppx*qqy)

                end if
            end if
        end if
    end if

    call MPI_Barrier(MPI_COMM_WORLD,ierr)

    end do
    9090 continue

    if(rank <= m_rank) then
        nullify(EvapTrans_T,Saturation_T,Porosity_T)
        nullify(Vx_T,Vy_T,Vz_T,Ind_T,dz_T,P_T,Zone_T)
        nullify(d_isValid_T,d_indices_T)
    end if

    #if _TIMING == 1
    Total_time2 = mpi_wtime()
    #endif
    istat = cudaEventDestroy(startEvent)
    istat = cudaEventDestroy(stopEvent)

    write(17,*)
    write(17,*) '###  Execution Finished'
    write(17,*)
    write(17,*) 'Simulation Timing and Profiling:'
    write(17,'("Total Execution Time (s):",e12.5)') Total_time2 - Total_time1
    !write(17,'("File IO Time Read (s):",e12.5)') IO_time_read
    !write(17,'("Host to device data transfer (s):",e12.5)') h2d_time
    !write(17,'("Add new particles (s):",e12.5)') add_time
    !write(17,'("Peripheric particles (s):",e12.5)') peri_time/1000.
    !write(17,'("Interior particles (s):",e12.5)') inte_time/1000.
    !write(17,'("Update C array (s):",e12.5)') C_time/1000.
    !write(17,'("Exit particles (s):",e12.5)') exit_time/1000.
    !write(17,'("Particles separation (s):",e12.5)') sort_time
    !write(17,'("Transfer particles (s):",e12.5)') transfer_time/1000.
    !write(17,'("scan (s):",e12.5)') scan_time/1000.
    !write(17,'("thrust (s):",e12.5)') thrust_time/1000.
    !write(17,'("copy (s):",e12.5)') copy_time/1000.
    !write(17,'("add (s):",e12.5)') add2_time/1000.
    !Write(17,'("File IO Time Write (s):",e12.5)') float(IO_time_write)/1000.
    !Write(17,'("Parallel Particle Time (s):",e12.5)') float(parallel_time)/1000.
    !write(17,*)

    flush(13);close(13)
    flush(15);close(15)
    flush(16);close(16)
    flush(17);close(17)
    ! flush(18);close(18)
    flush(19);close(19)
    flush(20);close(20)
    flush(30);close(30)
    ! be careful here? some ranks might not have these files

    ! call close_hdf5_interface()
    if(rank <= m_rank) call MPI_COMM_FREE(work_comm, ierr)
    call MPI_GROUP_FREE(work_group, ierr)
    call MPI_GROUP_FREE(world_group, ierr)
    call MPI_FINALIZE(ierr)

end program EcoSLIM

!open(10,file='P_new.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
!    write(10,*) np_active
!    write(10,*) pid
!    P = P_de
!    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind_c),i=1,np_active)
!close(10)

!open(10,file='P_separa.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
!    write(10,*) 'rank',rank,N_peri,N_inte
!    write(10,*) np_active
!    write(10,*) pid
!    P = P_de
!    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind_c),i=1,np_active)
!close(10)

!open(10,file='P_peri.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
!    write(10,*) np_active
!    write(10,*) pid
!    P = P_de
!    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind_c),i=1,np_active)
!close(10)