! Last Change:  2020-11-12 17:43:44
!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels.  This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths.  It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!--------------------------------------------------------------------
! 06/26/2021 GPU version, Chen Yang
!--------------------------------------------------------------------
program EcoSLIM
! use mpi
! use cudafor
use mrand
! use thrust
use utilities
use particle_loop
use mpiDeviceUtil
use variable_list
! use hdf5_file_read
! use hdf5_file_write
use subdomain_bound
use create_subdomain
use add_particles
use compact_array
! use exchange_zone
use exchange_particles

implicit none
! integer(8),allocatable,pinned:: p_num_cpu(:,:)
! type(curandStateXORWOW),allocatable,pinned:: h_cpu(:)
real(8),allocatable:: Saturation_cpu(:,:,:),Porosity_cpu(:,:,:)
real(8),allocatable:: EvapTrans_cpu(:,:,:),CLMvars_cpu(:,:,:)
real(8),allocatable:: Vx_cpu(:,:,:),Vy_cpu(:,:,:),Vz_cpu(:,:,:)
type(cudaEvent):: startEvent,stopEvent
integer:: istat, temp
real:: peri_time,peri_time_step
!real:: scan_time,scan_time_step
!real:: thrust_time,thrust_time_step
!real:: copy_time,copy_time_step
!real:: add2_time,add2_time_step
!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)
    write(ranknum,'(i5.5)') rank

    ! call open_hdf5_interface()
!--------------------------------------------------------------------
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    ! how to do timing is also a new question when asyn later.
    ! Set up timing
    Total_time1 = 0.d0
    Total_time2 = 0.d0
    t1 = 0.d0
    t2 = 0.d0
    IO_time_read = 0.d0
    IO_time_write = 0.d0
    parallel_time = 0.d0
    sort_time = 0.d0
    h2d_time = 0.d0
    d2h_time = 0.d0
    add_time = 0.d0
    peri_time= 0.
    transfer_time= 0.
    inte_time= 0.
    exit_time= 0.
    C_time   = 0.
    !scan_time   = 0.
    !copy_time   = 0.
    !add2_time   = 0.
    !thrust_time   = 0.

    istat = cudaEventCreate(startEvent)
    istat = cudaEventCreate(stopEvent)
!--------------------------------------------------------------------
    ! Read inputs, set up domain, write the log file
    ! open SLIM input .txt file
    open (10,file='slimin.txt')

    ! read SLIM run name
    read(10,*) runname

    ! read ParFlow run name
    read(10,*) pname

    ! read DEM file name
    read(10,*) DEMname

    if(rank == 0) then
        ! open/create/write the output log.txt file. If doesn't exist, it's created.
        open(11,file=trim(runname)//'_log.txt')
        write(11,*) '### EcoSLIM Log File'
        write(11,*)
        write(11,*) 'run name:',trim(runname)
        write(11,*)
        write(11,*) 'ParFlow run name:',trim(pname)
        write(11,*)
        if (DEMname /= '') then
            write(11,*) 'ParFlow DEM name:',trim(DEMname)
        else
            write(11,*) 'Not reading ParFlow DEM'
        end if
        write(11,*)
    endif ! rank = 0, write logfile through channel 11

    ! read domain number of cells and number of particles to be injected
    read(10,*) nx
    read(10,*) ny
    read(10,*) nz
    read(10,*) nCLMsoil
    read(10,*) ppx
    read(10,*) qqy
    ! read in number of particles for IC (if np_ic = -1 then restart from a file)
    read(10,*) np_ic

    ! read in the number of particles total
    read(10,*) np

    if(rank == 0) then
        ! check to make sure we don't assign more particles for IC than we have allocated
        ! in total
        if (np_ic > np) then
        write(11,*) 'warning NP_IC greater than IC'
        np = np_ic
        end if
        ! write nx, ny, nz, and np in the log file
        write(11,*) 'Grid information'
        write(11,*) 'nx:',nx
        write(11,*) 'ny:',ny
        write(11,*) 'nz:',nz
        write(11,*)
        write(11,*) 'Particle IC Information'
        write(11,*) 'np IC:',np_ic
        if (np_ic == -1) &
        write(11,*) 'Reading particle restart file:',trim(runname)//'_particle_restart.bin'
        write(11,*) 'np:',np
    endif ! rank=0

    ! nCLMsoil = 10 ! number of CLM soil layers over the root zone !this doesn't matter
    nzclm = 13 + nCLMsoil ! CLM output is 13+nCLMsoil layers for different variables not domain NZ,
                          ! e.g. 23 for 10 soil layers (default) and 17 for 4 soil layers (Noah soil
                          ! layer setup)
    n_constituents = 9

    allocate(dz(nz),dz_de(nz))
    ! have to do this here since the following read of dz

    ! read dx, dy as scalars
    read(10,*) dx_c
    dx = dx_c
    read(10,*) dy_c
    dy = dy_c
    ! read dz as an array
    read(10,*) dz(1:nz)
    ! dz2 = dz
    ! read in (constant for now) ParFlow dt
    read(10,*) pfdt_c
    pfdt = pfdt_c
    ! read in parflow start and stop times
    read(10,*) pft1
    read(10,*) pft2
    read(10,*) tout1
    read(10,*) n_cycle
    read(10,*) add_f

    pfnt = n_cycle*(pft2-pft1+1)
    outkk = tout1 + 1

    ! IO control, each value is a timestep interval, e.g. 1= every timestep, 2=every other, 0 = no writing
    read(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
    read(10,*) ibinpntswrite  !  controls VTK, binary output of particle locations and attributes
    read(10,*) etwrite        !  controls ASCII ET output
    read(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
                              ! ages are mapped to a grid and written every N timesteps

    ! allocate and assign timesteps
    ! this can be a scalar and update every timestep
    allocate(Time_Next(pfnt-tout1))
    Time_Next = 0.d0
    do kk = 1, pfnt-tout1
        Time_Next(kk) = float(kk+tout1)*pfdt
    end do
    Time_first = float(outkk-1)*pfdt

    ! read in velocity multiplier
    read(10,*) V_mult
    ! do we read in clm evap trans?
    read(10,*) clmtrans
    ! do we read in clm output file?
    read(10,*) clmfile
    ! read in IC number of particles for flux
    read(10,*) iflux_p_res
    ! read in density h2o
    read(10,*) denh2o
    ! read in diffusivity
    ! moldiff = (1.15e-9)*3600.d0
    read(10,*) moldiff
    ! fraction of dx/Vx
    read(10,*) dtfrac

    if(rank == 0) then
        ! wite out log file
        write(11,*)
        write(11,*) 'Grid Dimensions'
        write(11,'(" dx:",e12.5)') dx_c
        write(11,'(" dy:",e12.5)') dy_c
        write(11,'(" dz:",*(e12.5,", "))') dz(1:nz)
        write(11,*)
        write(11,*) 'Timestepping Information'
        write(11,'(" ParFlow delta-T, pfdt:",e12.5)') pfdt_c
        write(11,'(" ParFlow timesteps, pfnt:",i12)') pfnt
        write(11,'(" ParFlow start step, pft1:",i12)') pft1
        write(11,'(" ParFlow end step, pft2:",i12)') pft2
        write(11,'(" Output step start:",i12)') outkk
        write(11,'(" Time loops, cycles, n_cycle:",i12)') n_cycle
        write(11,'(" Total time steps:",i12)') pfnt

        write(11,*)
        write(11,*) 'V mult: ',V_mult,' for forward/backward particle tracking'
        write(11,*) 'CLM Trans: ',clmtrans,' adds / removes particles based on LSM fluxes'
        write(11,*)
        write(11,*) 'Physical Constants'
        write(11,*) 'denh2o: ',denh2o,' M/L^3'
        write(11,*) 'Molecular Diffusivity: ',moldiff,' '
        !write(11,*) 'Fractionation: ',Efract,' '
        write(11,*)
        write(11,*) 'Numerical Stability Information'
        write(11,'(" dtfrac: ",e12.5," fraction of dx/Vx")') dtfrac
    endif

    read(10,*) nind
    read(10,*) Indname
    if(rank == 0) then
        write(11,*)
        write(11,*) 'Indicator File'
        write(11,*) nind, 'Indicators'
    endif

    ! end of SLIM input
    close(10)
!--------------------------------------------------------------------
    call alloc_arrays_const()
    ! now we know the total dimension

    allocate(Saturation_cpu(nx,ny,nz),Porosity_cpu(nx,ny,nz))
    allocate(EvapTrans_cpu(nx,ny,nz),CLMvars_cpu(nx,ny,nzclm))
    allocate(Vx_cpu(nx+1,ny,nz),Vy_cpu(nx,ny+1,nz),Vz_cpu(nx,ny,nz+1))

    allocate(Zonet_old(-buff+1:nx+buff,-buff+1:ny+buff)) ! Zonet_old is not necessary
    allocate(Zonet_new(-buff+1:nx+buff,-buff+1:ny+buff))
    allocate(p_num(nx,ny),c_sum(max(nx,ny)))
    grid = 0
    Zonet_new = -1
    Zonet_new(1:nx,1:ny) = 0
    p_num = 1

    call global_xyz()
    ! restart file info inside, so have to call it before restart.

!--------------------------------------------------------------------
    if(np_ic /= -1) then

        ! if not restart, we do decompostion and build topology
        call gridinfo()

        call copy_grid() ! doing this on workers
            ! open(10,file='topology.'//trim(adjustl(ranknum)))
            !     write(10,'(8(i3,1x))') ix1,iy1,nnx1,nny1,ix2,iy2,nnx2,nny2
            !     write(10,'(8(i3,1x))') ix2-ix1+1,ix2-ix1+nnx2,iy2-iy1+1,iy2-iy1+nny2
            ! close(10)

        call alloc_arrays_temp() ! doing this on workers
            ! open(10,file='Zone_de_before.'//trim(adjustl(ranknum)))
            !     write(10,'(8(12(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
            ! close(10)

        Zone_de(-buff+1:nnx1+buff,-buff+1:nny1+buff) = &
        Zonet_new(-buff+ix1+1:ix1+nnx1+buff,-buff+iy1+1:iy1+nny1+buff)

            open(10,file='Zone_de_after.'//trim(adjustl(ranknum))//'.txt')
                if(rank == 0) &
                    write(10,'(22(22(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                if(rank == 1) &
                    write(10,'(23(22(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                if(rank == 2) &
                    write(10,'(22(23(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
                if(rank == 3) &
                    write(10,'(23(23(i3,1x),/))') ((Zone_de(i,j),i=-buff+1,nnx1+buff),j=-buff+1,nny1+buff)
            close(10)

    end if
!--------------------------------------------------------------------
    call local_xyz()
        ! open(10,file='Local_range.'//trim(adjustl(ranknum)))
        !     write(10,'(6(f15.2,1x))') Xgmin, Xgmax, Ygmin, Ygmax, Zgmin, Zgmax
        !     write(10,'(6(f15.2,1x))') Xmin1, Xmax1, Ymin1, Ymax1, Zmin1, Zmax1
        !     write(10,'(6(f15.2,1x))') Xmin2, Xmax2, Ymin2, Ymax2, Zmin2, Zmax2
        !     write(10,'(6(f15.2,1x))') Xmin3, Xmax3, Ymin3, Ymax3, Zmin3, Zmax3
        ! close(10)

    if(rank == 0) then
        flush(11)
        close(11)
    endif

    ! Porosity = 0.3d0
    fname = trim(adjustl(pname))//'.out.porosity.pfb'
    call pfb_read(Porosity_cpu,fname,nx,ny,nz)
    Porosity(1+ix2-ix1:nnx2+ix2-ix1,1+iy2-iy1:nny2+iy2-iy1,:) = &
    Porosity_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

    if(np_ic /= -1) then
        ! Saturation = 0.8d0
        write(filenum,'(i5.5)') pft1-1
        fname = trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Saturation_cpu,fname,nx,ny,nz)
        Saturation(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
        Saturation_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)
            ! allocate(h_cpu(np))
            ! h_cpu = h
            ! write(*,*) rank,'h_before',h_cpu(1)
        call createRand_init<<<ceiling(dble(nnx1*nny1*nz)/tPB),tPB>>>(nx,ny,nz, &
            rank,np_ic,nnx1,nny1)
            ! h_cpu = h
            ! write(*,*) rank,'h_after',h_cpu(1)
    endif

    dz_de  = dz ! I can't understand why dz doesn't work.
    Ind_de = Ind
    Porosity_de   = Porosity
    Saturation_de = Saturation
    P_de = P

    ! Define initial particles' locations and mass
    if (np_ic > 0)  then
        np_active = 0
        pid = 0
        if (np_active + np_ic*nnx1*nny1*nz >= np ) then
            !write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
            !call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            !MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

            !write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
            !call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
            !MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
            write(17,*) ' **Warning IC input but no paricles left'
            write(17,*) ' **Exiting code gracefully writing restart'
            goto 9090
        endif

        call add_init_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
        tPB >>> (P_de,np_ic,nind,denh2o,tout1,nnx1,nny1,nz,ix1,iy1,dz_de)

        np_active = np_ic*nnx1*nny1*nz
        pid = np_ic*nnx1*nny1*nz

    end if

        !open(10,file='P_init.'//trim(adjustl(ranknum))//'.txt')
        !    write(10,*) np_active
        !    write(10,*) pid
        !    P = P_de
        !    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
        !close(10)

    ! keep the following output as it is. we can figure out a better way to handle them later.
    !call MPI_FILE_OPEN(MPI_COMM_SELF,logf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh4,ierr)

    !write(message,'(a,a)') ' **** Transient Simulation Particle Accounting ****', new_line(' ')
    !call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

    !write(message,'(a,a)') ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
    !                        ETOut NP_PrecipIn NP_ETOut NP_QOut NP_filtered', new_line(' ')
    !call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

    !! open exited particle file and write header
    !call MPI_FILE_OPEN(MPI_COMM_SELF,exitedf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh3,ierr)

    open(20,file='Log_particles.'//trim(adjustl(ranknum))//'.txt')
    write(20,*) ' **** Transient Simulation Particle Accounting ****'
    write(20,*) ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
                            ETOut NP_PrecipIn NP_ETOut NP_QOut NP_filtered'

    open(19,file='Exited_particles.'//trim(adjustl(ranknum))//'.bin')

    if(rank == 0) then

        open(13,file=trim(runname)//'_ET_output.txt')
        write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

        open(15,file=trim(runname)//'_flow_output.txt')
        write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

        open(16,file=trim(runname)//'_PET_balance.txt')
        write(16,*) 'TIME P[kg] ET[kg]'

    endif

    call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
        tPB>>>(nnx1,nny1,buff,neigh_list,rank,t_rank)

    if(sum(neigh_list) > 0) then
        allocate(N_recv(sum(neigh_list)),N_send(sum(neigh_list)))
    else
        allocate(N_recv(1),N_send(1))
    end if

        ! open(10,file='Neighbor.'//trim(adjustl(ranknum)))
        !     write(10,*) neigh_list
        !     write(10,*) N_recv
        ! close(10)

    ! Intialize cuRand device API and this part need to check through again.
    call createRand_loop<<< ceiling(dble(np)/tPB),tPB >>>(np, rank, pfnt)

!--------------------------------------------------------------------
    pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

    do kk = outkk, pfnt

        ! reset ParFlow counter for cycles
        if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1

        ! adjust the file counters
        pfkk = pfkk + 1
        #ifdef TIMING
        t1 = mpi_wtime()
        #endif

        ! Read the velocities computed by ParFlow
        write(filenum,'(i5.5)') pfkk

        !fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vx,1)
        !Vx = 240.d0
        fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vx_cpu,fname,nx+1,ny,nz)
        Vx(ix2-ix1+1:ix2-ix1+nnx2+1,iy2-iy1+1:iy2-iy1+nny2,1:nz) = &
        Vx_cpu(ix2+1:ix2+nnx2+1,iy2+1:iy2+nny2,1:nz)

        !fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vy,2)
        !Vy = 240.d0
        fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vy_cpu,fname,nx,ny+1,nz)
        Vy(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2+1,1:nz) = &
        Vy_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2+1,1:nz)

        !fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Vz,3)
        !Vz = 0.d0
        fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Vz_cpu,fname,nx,ny,nz+1)
        Vz(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,1:nz+1) = &
        Vz_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,1:nz+1)

        !fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.h5'
        !call read_h5_file(Saturation,0)
        !Saturation = 0.8d0
        fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.pfb'
        call pfb_read(Saturation_cpu,fname,nx,ny,nz)
        Saturation(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
        Saturation_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

        if (clmtrans) then
            ! Read in the Evap_Trans
            ! fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.h5'
            ! call read_h5_file(EvapTrans,0)
            ! EvapTrans = 0.d0
            ! EvapTrans(2,:,1) = 1.d-8
            fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.pfb'
            call pfb_read(EvapTrans_cpu,fname,nx,ny,nz)
            EvapTrans(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
            EvapTrans_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

            if (clmfile) then
                ! Read in CLM output file @RMM to do make this input
                ! fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.h5'
                ! call read_h5_file(CLMvars,5)
                ! CLMvars = 0.d0
                fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.pfb'
                call pfb_read(CLMvars_cpu,fname,nx,ny,nzclm)
                CLMvars(ix2-ix1+1:ix2-ix1+nnx2,iy2-iy1+1:iy2-iy1+nny2,:) = &
                CLMvars_cpu(ix2+1:ix2+nnx2,iy2+1:iy2+nny2,:)

            end if
        end if

        #ifdef TIMING
        t2 = mpi_wtime()
        IO_time_read = IO_time_read + t2 - t1
        #endif

        ! Determine whether to perform forward or backward patricle tracking
        Vx = Vx * V_mult
        Vy = Vy * V_mult
        Vz = Vz * V_mult



        Vx_de = Vx
        Vy_de = Vy
        Vz_de = Vz
        Saturation_de = Saturation
        EvapTrans_de  = EvapTrans
        #ifdef TIMING
        t1 = mpi_wtime()
        #endif
        CLMvars_de    = CLMvars(:,:,11)
        ! probably this can be deallocated after adding of particles
        ! Now just think about the hourly add of particles.

        #ifdef TIMING
        t2 = mpi_wtime()
        h2d_time = h2d_time + t2 - t1
        #endif

        out_age_de  = 0.d0
        out_mass_de = 0.d0
        out_comp_de = 0.d0
        out_np_de   = 0

        et_age_de  = 0.d0
        et_mass_de = 0.d0
        et_comp_de = 0.d0
        et_np_de   = 0

        mean_age_de    = 0.d0
        mean_comp_de   = 0.d0
        total_mass_de  = 0.d0
        PET_balance_de = 0.d0

        C_de = 0.d0
        ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
        ! the following adding of particles.

!----------------------------------------
        #ifdef TIMING
        t1 = mpi_wtime()
        #endif

        if (clmtrans) then
            if (np_active < np) then

                !#ifdef TIMING
                !istat = cudaEventRecord(startEvent,0)
                !#endif

                call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>> ( &
                    PET_balance_de,d_isValid,nnx1,nny1,nz,dz_de,denh2o)

                !#ifdef TIMING
                !istat = cudaEventRecord(stopEvent,0)
                !istat = cudaEventSynchronize(stopEvent)
                !istat = cudaEventElapsedTime(scan_time_step,startEvent,stopEvent)
                !scan_time = scan_time + scan_time_step
                !#endif

                ! d_isValid has the length as P or of np which is assigned in input script
                ! so nnx1*nny1*nz should be smaller than np
                ! we don't care about the buffer zone for add of particles
                ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                ! in main program and pass arguments to other subroutines.

                !#ifdef TIMING
                !istat = cudaEventRecord(startEvent,0)
                !#endif

                ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                call thrustscan(d_isValid,nnx1*nny1,d_indices)

                !#ifdef TIMING
                !istat = cudaEventRecord(stopEvent,0)
                !istat = cudaEventSynchronize(stopEvent)
                !istat = cudaEventElapsedTime(thrust_time_step,startEvent,stopEvent)
                !thrust_time = thrust_time + thrust_time_step
                !#endif

                !#ifdef TIMING
                !istat = cudaEventRecord(startEvent,0)
                !#endif
                temp = d_indices(nnx1*nny1)
                i_added_particles = temp * iflux_p_res ! should be legal?

                !#ifdef TIMING
                !istat = cudaEventRecord(stopEvent,0)
                !istat = cudaEventSynchronize(stopEvent)
                !istat = cudaEventElapsedTime(copy_time_step,startEvent,stopEvent)
                !copy_time = copy_time + copy_time_step
                !#endif

                if(np_active + i_added_particles >= np) then
                    !write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                    !call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    !MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                    !write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                    !call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    !MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                    write(17,*) ' **Warning rainfall input but no paricles left'
                    write(17,*) ' **Exiting code gracefully writing restart'
                    goto 9090
                endif

                !#ifdef TIMING
                !istat = cudaEventRecord(startEvent,0)
                !#endif

                call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                    tPB >>> (d_indices,P_de,iflux_p_res,np_active,pid,nind, &
                    nnx1,nny1,nz,ix1,iy1,dz_de,denh2o,kk,tout1)

                !#ifdef TIMING
                !istat = cudaEventRecord(stopEvent,0)
                !istat = cudaEventSynchronize(stopEvent)
                !istat = cudaEventElapsedTime(add2_time_step,startEvent,stopEvent)
                !add2_time = add2_time + add2_time_step
                !#endif

                np_active = np_active + i_added_particles
                pid = pid + i_added_particles

            end if
        end if

            !open(10,file='P_new.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
            !    write(10,*) np_active
            !    write(10,*) pid
            !    P = P_de
            !    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            !close(10)

        call MPI_ALLReduce(MPI_IN_PLACE,PET_balance_de,2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)

        #ifdef TIMING
        t2 = mpi_wtime()
        add_time = add_time + t2 - t1
        #endif

        #ifdef TIMING
        t1 = mpi_wtime()
        #endif
        call particles_separation<<<ceiling(dble(np_active)/tPB),tPB>>> ( &
            P_de,d_isValid,xmin3,ymin3,xmax3,ymax3,np_active,ix1,iy1)
        call thrustscan(d_isValid,np_active,d_indices)
        temp = d_indices(np_active)
        N_peri = np_active - temp

        if(np_active + N_peri >= np) then
            write(17,*) ' **Warning particles separation out of bound'
            write(17,*) ' **Exiting code gracefully writing restart'
            goto 9090
        endif

        allocate(holes(N_peri))
        call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,np_active)
        call select2np_active<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            holes,P_de,np_active,N_peri)
        call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,P_de,0,np_active)
        N_inte = d_indices(np_active); deallocate(holes)
        call connect_recv<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            P_de,N_inte,N_peri)

            !open(10,file='P_separa.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
            !    write(10,*) 'rank',rank,N_peri,N_inte
            !    write(10,*) np_active
            !    write(10,*) pid
            !    P = P_de
            !    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            !close(10)
        #ifdef TIMING
        t2 = mpi_wtime()
        sort_time = sort_time + t2 - t1
        #endif

        #ifdef TIMING
        istat = cudaEventRecord(startEvent,0)
        #endif
        ! peripheric particles
        call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            et_age_de,et_mass_de,et_comp_de,et_np_de, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin2,ymin2,zmin2,xmax2,ymax2,zmax2, &
            moldiff,denh2o,dtfrac,N_inte,N_peri, &
            nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)
        #ifdef TIMING
        istat = cudaEventRecord(stopEvent,0)
        istat = cudaEventSynchronize(stopEvent)
        istat = cudaEventElapsedTime(peri_time_step,startEvent,stopEvent)
        peri_time = peri_time + peri_time_step
        #endif
            !open(10,file='P_peri.'//trim(adjustl(ranknum))//'.'//trim(adjustl(filenum))//'.txt')
            !    write(10,*) np_active
            !    write(10,*) pid
            !    P = P_de
            !    write(10,'(2000(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            !close(10)

        call particle_exchange()

        mean_age    = mean_age_de
        mean_comp   = mean_comp_de
        total_mass  = total_mass_de
        if(total_mass(1) > 0.d0) mean_age(1)  = mean_age(1)/total_mass(1)
        if(total_mass(1) > 0.d0) mean_comp(1) = mean_comp(1)/total_mass(1)
        mean_mass = total_mass/dble(np_active)
        PET_balance = PET_balance_de
        ET_np_cpu   = ET_np_de
        out_np_cpu  = out_np_de
        write(20,'(3(i8),7(1x,e12.5,1x),3(i8),6(i12))') kk,pfkk,outkk,Time_Next(kk),mean_age,mean_comp, &
                                                      mean_mass,total_mass,PET_balance(1), &
                                                      PET_balance(2),i_added_particles,ET_np_cpu(1), &
                                                      Out_np_cpu(1),np_active,N_peri,N_inte,N_exit, &
                                                      sum(N_send),sum(N_recv)
        !call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

    end do
    9090 continue

    istat = cudaEventDestroy(startEvent)
    istat = cudaEventDestroy(stopEvent)

    write(17,*)
    write(17,*) '###  Execution Finished'
    write(17,*)
    write(17,*) 'Simulation Timing and Profiling:'
    !Write(17,'("Total Execution Time (s):",e12.5)') float(Total_time2-Total_time1)/1000.
    write(17,'("File IO Time Read (s):",e12.5)') IO_time_read
    write(17,'("Host to device data transfer (s):",e12.5)') h2d_time
    write(17,'("Add new particles (s):",e12.5)') add_time
    write(17,'("Peripheric particles (s):",e12.5)') peri_time/1000.
    write(17,'("Interior particles (s):",e12.5)') inte_time/1000.
    write(17,'("Update C array (s):",e12.5)') C_time/1000.
    write(17,'("Exit particles (s):",e12.5)') exit_time/1000.
    write(17,'("Particles separation (s):",e12.5)') sort_time
    write(17,'("Transfer particles (s):",e12.5)') transfer_time/1000.
    !write(17,'("scan (s):",e12.5)') scan_time/1000.
    !write(17,'("thrust (s):",e12.5)') thrust_time/1000.
    !write(17,'("copy (s):",e12.5)') copy_time/1000.
    !write(17,'("add (s):",e12.5)') add2_time/1000.
    !Write(17,'("File IO Time Write (s):",e12.5)') float(IO_time_write)/1000.
    !Write(17,'("Parallel Particle Time (s):",e12.5)') float(parallel_time)/1000.
    !write(17,*)

    flush(13);close(13)
    flush(15);close(15)
    flush(16);close(16)
    flush(17);close(17)
    flush(18);close(18)
    flush(19);close(19)
    flush(20);close(20)

    ! call close_hdf5_interface()
    call MPI_FINALIZE(ierr)

end program EcoSLIM

