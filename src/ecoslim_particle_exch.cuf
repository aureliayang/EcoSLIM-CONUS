module exchange_particles

    use thrust
    use cudafor
    use compact_array
    use particle_loop
    use variable_list, only: N_recv, N_send, N_peri, N_inte, N_exit
    use variable_list, only: neigh_list, transfer
    ! neigh_list is allocated after getting number of total GPUs.
    ! it is 1 if neighbor exists while 0 if neighbor doesn't exist.
    ! N_send has the same length of neigh_list and was built one by one.
    ! N_recv is allocated or deallocated after initialization/update of grid.
    ! N_recv has the length of its neighbors and has been initialized as 0.
    use variable_list, only: holes, P_send, P_recv, P, P_exit
    use variable_list, only: P_de, d_isValid, d_indices
    use variable_list, only: dx, dy, dz_de, nz
    use variable_list, only: mean_age_de, mean_comp_de, total_mass_de
    use variable_list, only: ix1, iy1, nnx1, nny1, buff, reflect
    use variable_list, only: rank, t_rank, nind, tPB
    use variable_list, only: np_active, ranknum, filenum

    use variable_list, only: out_age_de,out_mass_de,out_comp_de,out_np_de
    use variable_list, only: et_age_de,et_mass_de,et_comp_de,et_np_de
    use variable_list, only: xgmin,ygmin,zgmin,xgmax,ygmax,zgmax
    use variable_list, only: xmin1,ymin1,zmin1,xmax1,ymax1,zmax1
    use variable_list, only: pfdt,moldiff,denh2o,dtfrac,kk
    use variable_list, only: transfer_time1, inte_time, exit_time, C_time
    use variable_list, only: transfer_time2, transfer_time3
    use variable_list, only: startEvent, stopEvent

contains
    subroutine particle_exchange()
        use mpi
        implicit none
        integer:: i, j, ierr, status(MPI_STATUS_SIZE,t_rank)
        integer:: nc1, nc2, nc3, nc4, nc5, nc6, rq1(t_rank), &
                  rq2(t_rank), rq3(t_rank), rq4(t_rank)
        integer:: left, right, nattri, sum_recv, sum_send
        integer:: istat, temp

        integer:: k, one = 0
        ! integer:: ik, jk, status2(MPI_STATUS_SIZE)
        integer:: d_indices_cpu(200000),d_isValid_cpu(200000)
        integer,allocatable:: holes_cpu(:)
        ! real(8):: P_cpu(19*4)
        ! we have to send and receive numbers first
        ! otherwise, when receive, we have to wait the receive of particle-number
        ! it has two-fold of meanings: 1. waiting the communication itself (copy to recv buff)
        ! 2. probably the number has not been sent by its neighbor yet since its neighbor
        ! is doing the sending one by one.
        ! but here, in order to send numbers first, we count them first in the particle
        ! loop which will consume extra time. Extra time means it can also be obtained
        ! in the thrustscan. But thrustscan can only be done one by one since d_indices
        ! is repeatedly used, so we can't get the number of all neighbors at one time.

            nattri = 17 + 2*nind
            if(transfer > 0) then
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif
                N_send = 0; N_recv = 0
                nc1 = 0; nc3 = 0
                do i = 0, t_rank-1 ! go through to send particles

                    if(neigh_list(i+1) > 0) then ! the number of GPUs, if >0 it's neighbor
                        nc1 = nc1 + 1 ! the number of neighbors
                        call prepare_neighbor<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                            P_de(N_inte+1:N_inte+N_peri,13+2*nind),d_isValid,N_peri,i)
                        ! get d_isValid, set it 0 where the particle will be sent to neighbor i

                        call thrustscan(d_isValid,N_peri,d_indices)
                        ! do prefix-sum and get d_indices
                        temp = d_indices(N_peri)
                        N_send(nc1) = N_peri - temp
                        call MPI_ISEND(N_send(nc1),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq1(nc1),ierr)

                        ! each neighbor use different sending buffer, no wait
                        if(N_send(nc1) > 0) then ! gpu number
                            nc3 = nc3 + 1 ! only the neighbor with particles
                            allocate(holes(N_send(nc1)))
                            call prepare_holes<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                                holes,d_indices,d_isValid,N_peri)
                            sum_send = sum(N_send(1:nc1-1)) ! have sent before this one
                            call select2end<<<ceiling(dble(N_send(nc1))/tPB),tPB>>>(holes, &
                                P_de,P_send,N_inte,N_send(nc1),sum_send,nattri)
                            left  = sum_send*nattri + 1
                            right = sum(N_send(1:nc1))*nattri
                            call MPI_ISEND(P_send(left:right),N_send(nc1)*nattri, &
                                MPI_DOUBLE_PRECISION,i,41,MPI_COMM_WORLD,rq3(nc3),ierr)
                            call compaction_inplace<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                                holes,d_indices,d_isValid,P_de,N_inte,N_peri)
                            N_peri = d_indices(N_peri); deallocate(holes)
                        end if
                    end if
                end do
                np_active = N_inte + N_peri
                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(transfer_time1,startEvent,stopEvent)
                #endif

                !-----------------------------------------
                ! receive part.
                !-----------------------------------------
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif
                nc2 = 0
                do i = 0, t_rank-1
                    if(neigh_list(i+1) > 0) then
                        nc2 = nc2 + 1 !neighbor number
                        call MPI_IRECV(N_recv(nc2),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq2(nc2),ierr)
                    end if
                end do
                if(nc2 > 0) call MPI_WAITALL(nc2,rq2(1:nc2),status(:,1:nc2),ierr)

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(transfer_time2,startEvent,stopEvent)
                #endif

                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif

                nc4 = 0; nc5 = 0
                do i = 0, t_rank-1
                    if(neigh_list(i+1) > 0) then
                        nc4 = nc4 + 1
                        if(N_recv(nc4) > 0) then
                            nc5 = nc5 + 1
                            left  = sum(N_recv(1:nc4-1))*nattri + 1
                            right = sum(N_recv(1:nc4))*nattri
                            call MPI_IRECV(P_recv(left:right),N_recv(nc4)*nattri, &
                                MPI_DOUBLE_PRECISION,i,41,MPI_COMM_WORLD,rq4(nc5),ierr)
                        end if
                    end if
                end do
                !-----------------------------------------
                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(transfer_time3,startEvent,stopEvent)
                #endif
                !-----------------------------------------
                if(transfer == 1) then
                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                    ! interior particles
                    #if _TEXTURE == 1
                        #if _SHARED == 1
                        call particles_independent_T<<<ceiling(dble(N_inte)/tPB),tPB,tPB*(13+nind*2)*8>>> ( &
                            P_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
                            et_age_de,et_mass_de,et_comp_de,et_np_de, &
                            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
                            moldiff,denh2o,dtfrac,0,N_inte, &
                            nind,ix1,iy1,nz,reflect,rank)
                        #else
                        call particles_independent_T<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
                            P_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
                            et_age_de,et_mass_de,et_comp_de,et_np_de, &
                            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
                            moldiff,denh2o,dtfrac,0,N_inte, &
                            nind,ix1,iy1,nz,reflect,rank)
                        #endif
                    #else
                        call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
                            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
                            et_age_de,et_mass_de,et_comp_de,et_np_de, &
                            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
                            moldiff,denh2o,dtfrac,0,N_inte, &
                            nind,ix1,iy1,nz,reflect,rank)
                    #endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(inte_time,startEvent,stopEvent)
                    #endif
                endif
                !-----------------------------------------
                if(nc2 > 0) sum_recv = sum(N_recv)
                if(nc5 > 0) then
                    call MPI_WAITALL(nc5,rq4(1:nc5),status(:,1:nc5),ierr)
                    call unpack_recv<<<ceiling(dble(sum_recv)/tPB),tPB>>>( &
                        P_recv,P_de,np_active,sum_recv,nattri)
                endif
                if(nc2 > 0) N_peri = N_peri + sum(N_recv)
                np_active = N_inte + N_peri

            endif

            !-----------------------------------------
            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

            call Update_C_Array<<< ceiling(dble(np_active)/tPB),tPB >>>( &
                P_de,dz_de,nz,ix1,iy1, &
                mean_age_de,mean_comp_de,total_mass_de,np_active)

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(C_time,startEvent,stopEvent)
            #endif

            !-----------------------------------------
            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif
            ! then separate inactive particles
            call prepare_neighbor<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                P_de(:,8),d_isValid,np_active,0)
            call thrustscan(d_isValid,np_active,d_indices)
            ! active and ET-inactive particles in overlap zone are sent
            ! out of domain, all outfolw-inactive, received ET-inactive are scanned here
            temp = d_indices(np_active)
            N_exit = np_active - temp

            if(N_exit > 0) then
                allocate(holes(N_exit),P_exit(N_exit,nattri))
                ! P_exit has to be deallocated after the use in main
                call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,d_indices,d_isValid,np_active)
                call select2exit<<<ceiling(dble(N_exit)/tPB),tPB>>>(&
                    holes,P_de,P_exit,N_exit)
                call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,d_indices,d_isValid,P_de,0,np_active)
                ! update the slots after np_active
                np_active = d_indices(np_active)
                deallocate(holes,P_exit)

            end if

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(exit_time,startEvent,stopEvent)
            #endif

    end subroutine particle_exchange

end module exchange_particles