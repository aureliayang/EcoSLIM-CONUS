module eco_particle_exch1
    use thrust
    use cudafor
    use eco_compact_util
    use eco_particle_loop
    use variable_list, only: N_recv, N_send, N_peri, N_inte, N_exit
    use variable_list, only: neigh_list, nattri_c
    ! neigh_list is 1 if neighbor exists while 0 if neighbor doesn't exist.

    use variable_list, only: holes, P_send, P_recv
    use variable_list, only: P_de, d_isValid, d_indices
    use variable_list, only: ix1, iy1, reflect
    use variable_list, only: rank, t_rank, nind_c, tPB
    use variable_list, only: np_active, ppx, qqy !, ranknum, filenum
    use variable_list, only: l_GPUs, t_GPUs, map_sub

    use variable_list, only: xgmin,ygmin,zgmin,xgmax,ygmax,zgmax
    use variable_list, only: xmin1,ymin1,zmin1,xmax1,ymax1,zmax1
    use variable_list, only: inte_time

    use variable_list, only: tran_time1, tran_time2, tran_time3, tran_time4, tran_time5
    use variable_list, only: tran_time6, tran_time7, tran_time8, tran_time9, tran_time10
    use variable_list, only: startEvent, stopEvent

contains
    subroutine particle_exchange1()
        use mpi
        implicit none
        integer:: i, j, ierr, status(MPI_STATUS_SIZE,t_rank)
        ! i, j are also in variable_list but we don't use it above
        integer:: nc1, nc2, nc3, nc4, nc5, nc6, rq1(t_rank), &
                  rq2(t_rank), rq3(t_rank), rq4(t_rank)
        integer:: left, right, sum_recv, sum_send
        integer:: istat, temp
        real:: tran_t1, tran_t2, tran_t3, tran_t4, tran_t5
        real:: tran_t6, tran_t7, tran_t8, tran_t9, tran_t10

                #if _TIMING == 1
            tran_time1=0.; tran_time2=0.; tran_time3=0.; tran_time4=0.; tran_time5=0.
            tran_time6=0.; tran_time7=0.; tran_time8=0.; tran_time9=0.; tran_time10=0.
                #endif

            nc1 = 0; nc3 = 0
            do i = 0, ppx*qqy-1 ! go through to send particles
                if(neigh_list(i+1) > 0) then ! the number of GPUs, if >0 it's neighbor
                    nc1 = nc1 + 1 ! the number of neighbors

                        #if _TIMING == 1
                        istat = cudaEventRecord(startEvent,0)
                        #endif
                    call MPI_ISEND(N_send(i+1),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq1(nc1),ierr)
                        #if _TIMING == 1
                        istat = cudaEventRecord(stopEvent,0)
                        istat = cudaEventSynchronize(stopEvent)
                        istat = cudaEventElapsedTime(tran_t2,startEvent,stopEvent)
                        tran_time2 = tran_time2 + tran_t2
                        #endif
                    ! each neighbor use different sending buffer, no wait
                    if(N_send(i+1) > 0) then ! gpu number
                        nc3 = nc3 + 1 ! only the neighbor with particles

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif
                        call prepare_neighbor<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                            P_de(N_inte+1:N_inte+N_peri,13+2*nind_c),N_peri,i)
                            ! get d_isValid, set it 0 where the particle will be sent to neighbor i
                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(tran_t1,startEvent,stopEvent)
                            tran_time1 = tran_time1 + tran_t1
                            #endif

                        call thrustscan(d_isValid,N_peri,d_indices)

                        allocate(holes(N_send(i+1)))

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif
                        call prepare_holes<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                            holes,N_peri)
                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(tran_t3,startEvent,stopEvent)
                            tran_time3 = tran_time3 + tran_t3
                            #endif

                        sum_send = sum(N_send(1:i)) ! have sent before this one

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif
                        call select2send<<<ceiling(dble(N_send(i+1))/tPB),tPB>>>(holes, &
                            P_send,N_inte,N_send(i+1),sum_send)
                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(tran_t4,startEvent,stopEvent)
                            tran_time4 = tran_time4 + tran_t4
                            #endif

                        left  = sum_send*nattri_c + 1
                        right = sum(N_send(1:i+1))*nattri_c

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif
                        call MPI_ISEND(P_send(left:right),N_send(i+1)*nattri_c, &
                            MPI_DOUBLE_PRECISION,i,41,MPI_COMM_WORLD,rq3(nc3),ierr)
                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(tran_t5,startEvent,stopEvent)
                            tran_time5 = tran_time5 + tran_t5
                            #endif

                            #if _TIMING == 1
                            istat = cudaEventRecord(startEvent,0)
                            #endif
                        call compaction_inplace<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                            holes,N_inte,N_peri)
                            #if _TIMING == 1
                            istat = cudaEventRecord(stopEvent,0)
                            istat = cudaEventSynchronize(stopEvent)
                            istat = cudaEventElapsedTime(tran_t6,startEvent,stopEvent)
                            tran_time6 = tran_time6 + tran_t6
                            #endif

                        N_peri = d_indices(N_peri); deallocate(holes)
                    end if
                end if
            end do
            np_active = N_inte + N_peri

            !-----------------------------------------
            ! receive part. Receive only on primary GPUs.
            !-----------------------------------------
            if(rank < ppx*qqy) then
                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                nc2 = 0
                do i = 0, ppx*qqy-1
                    if(neigh_list(i+1) > 0) then
                        do j = 1, t_GPUs(i+1)
                            nc2 = nc2 + 1 !neighbor number
                            call MPI_IRECV(N_recv(nc2),1,MPI_INTEGER,l_GPUs(i+1,j), &
                            40,MPI_COMM_WORLD,rq2(nc2),ierr)
                        end do
                    end if
                end do
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(tran_t7,startEvent,stopEvent)
                    tran_time7 = tran_time7 + tran_t7
                    #endif
                !-----------------------------------------
                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif
                if(nc2 > 0) call MPI_WAITALL(nc2,rq2(1:nc2),status(:,1:nc2),ierr)
                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(tran_t8,startEvent,stopEvent)
                    tran_time8 = tran_time8 + tran_t8
                    #endif
                !-----------------------------------------
                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif

                nc4 = 0; nc5 = 0
                do i = 0, ppx*qqy-1
                    if(neigh_list(i+1) > 0) then
                        do j = 1, t_GPUs(i+1)
                            nc4 = nc4 + 1
                            if(N_recv(nc4) > 0) then
                                nc5 = nc5 + 1
                                left  = sum(N_recv(1:nc4-1))*nattri_c + 1
                                right = sum(N_recv(1:nc4))*nattri_c
                                call MPI_IRECV(P_recv(left:right),N_recv(nc4)*nattri_c, &
                                    MPI_DOUBLE_PRECISION,l_GPUs(i+1,j),41,MPI_COMM_WORLD,rq4(nc5),ierr)
                            end if
                        end do
                    end if
                end do

                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(tran_time9,startEvent,stopEvent)
                    #endif
            end if
        !-----------------------------------------
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif

            ! interior particles
            call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
                xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
                0,N_inte,ix1,iy1,reflect,map_sub)

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(inte_time,startEvent,stopEvent)
                #endif
        !-----------------------------------------
            if(rank < ppx*qqy) then

                    #if _TIMING == 1
                    istat = cudaEventRecord(startEvent,0)
                    #endif

                if(nc5 > 0) then
                    sum_recv = sum(N_recv(1:nc4))
                    call MPI_WAITALL(nc5,rq4(1:nc5),status(:,1:nc5),ierr)
                    call unpack_recv<<<ceiling(dble(sum_recv)/tPB),tPB>>>( &
                        P_recv,np_active,sum_recv)
                    N_peri = N_peri + sum_recv
                    np_active = N_inte + N_peri
                endif

                    #if _TIMING == 1
                    istat = cudaEventRecord(stopEvent,0)
                    istat = cudaEventSynchronize(stopEvent)
                    istat = cudaEventElapsedTime(tran_time10,startEvent,stopEvent)
                    #endif
            end if
    end subroutine particle_exchange1

end module eco_particle_exch1