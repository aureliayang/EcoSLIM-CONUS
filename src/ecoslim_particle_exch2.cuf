module eco_particle_exch2
    use thrust
    use cudafor
    use eco_compact_util
    use eco_particle_loop
    use variable_list, only: N_recv, N_send, N_peri, N_inte, N_exit
    use variable_list, only: neigh_list, nattri_c, clmtrans
    ! neigh_list is 1 if neighbor exists while 0 if neighbor doesn't exist.

    use variable_list, only: holes, P_send, P_recv
    use variable_list, only: P_de, d_isValid, d_indices
    use variable_list, only: ix1_c, iy1_c, reflect, kk, transfer
    use variable_list, only: rank, t_rank, nind_c, tPB
    use variable_list, only: np_active, ppx, qqy !, ranknum, filenum
    use variable_list, only: l_GPUs, t_GPUs, map_sub

    use variable_list, only: xmin1_c,ymin1_c,zmin1_c,xmax1_c,ymax1_c,zmax1_c
    use variable_list, only: inte_time

    use variable_list, only: tran_time1, tran_time2, tran_time3, tran_time4, tran_time5
    use variable_list, only: tran_time6, tran_time7, tran_time8, tran_time9, tran_time10
    use variable_list, only: tran_time11, tran_time12, tran_time13, tran_time14, tran_time15
    use variable_list, only: startEvent, stopEvent

contains
    subroutine particle_exchange2()
        use mpi
        implicit none
        integer:: i, j, k, ierr, status(MPI_STATUS_SIZE,tPB)
        integer:: nc1, nc2, nc3, nc4, nc5, nc6, rq1(tPB), &
                  rq2(tPB), rq3(tPB), rq4(tPB)
        integer:: left, right, sum_recv, sum_send
        integer:: istat, temp
        real:: tran_t1, tran_t2, tran_t3, tran_t4, tran_t5
        real:: tran_t6, tran_t7, tran_t8, tran_t9, tran_t10
        real:: tran_t11, tran_t12, tran_t13, tran_t14, tran_t15
        real(8):: P_send_cpu(tPB*nattri_c)

            #if _TIMING == 1
        tran_time1=0.; tran_time2=0.; tran_time3=0.; tran_time4=0.; tran_time5=0.
        tran_time6=0.; tran_time7=0.; tran_time8=0.; tran_time9=0.; tran_time10=0.
        tran_time11=0.; tran_time12=0.; tran_time13=0.; tran_time14=0.; tran_time15=0.
            #endif

        call local2global<<<ceiling(dble(N_peri)/tPB),tPB>>>(N_inte, &
        N_peri,dble(ix1_c),dble(iy1_c))

        sum_send = sum(N_send)

        nc1 = 0; nc3 = 0
        do i = 0, ppx*qqy-1 ! go through to send particles
            if(neigh_list(i+1) > 0) then ! the number of GPUs, if >0 it's neighbor
                nc1 = nc1 + 1
                call MPI_ISEND(N_send(i+1),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq1(nc1),ierr)
            end if
        end do

        if(sum_send > 0) then

            call prepare_neigh_all<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            P_de(N_inte+1:N_inte+N_peri,13+2*nind_c),N_peri,-1.d0)

            call thrustscan(d_isValid,N_peri,d_indices)

            allocate(holes(sum_send))

            call prepare_holes<<<ceiling(dble(N_peri)/tPB),tPB>>>(holes,N_peri)

            call select2send<<<ceiling(dble(sum_send)/tPB),tPB>>>(holes, &
            P_send,N_inte,sum_send,0)

            call compaction_inplace<<<ceiling(dble(N_peri)/tPB),tPB>>>(holes,N_inte,N_peri)

            N_peri = d_indices(N_peri)
            np_active = N_inte + N_peri

            P_send_cpu(1:sum_send*nattri_c) = P_send(1:sum_send*nattri_c)

            do i = 1, sum_send
                nc3 = nc3 + 1
                left  = (nc3-1)*nattri_c + 1
                right = nc3*nattri_c
                call MPI_ISEND(P_send(left:right),nattri_c,MPI_DOUBLE_PRECISION, &
                idnint(P_send_cpu(13+2*nind_c+left-1)),41,MPI_COMM_WORLD,rq3(nc3),ierr)
            end do

        end if
        !-----------------------------------------
        ! receive part.
        !-----------------------------------------
        if(rank < ppx*qqy) then
            nc2 = 0
            do i = 0, ppx*qqy-1
                if(neigh_list(i+1) > 0) then
                    do j = 1, t_GPUs(i+1)
                        nc2 = nc2 + 1 !neighbor number
                        call MPI_IRECV(N_recv(nc2),1,MPI_INTEGER,l_GPUs(i+1,j), &
                        40,MPI_COMM_WORLD,rq2(nc2),ierr)
                    end do
                endif
            enddo
            if(nc2 > 0) call MPI_WAITALL(nc2,rq2(1:nc2),status(:,1:nc2),ierr)

            nc4 = 0; nc5 = 0
            do i = 0, ppx*qqy-1
                if(neigh_list(i+1) > 0) then
                    do j = 1, t_GPUs(i+1)
                        nc4 = nc4 + 1
                        if(N_recv(nc4) > 0) then
                            do k = 1, N_recv(nc4)
                                nc5 = nc5 + 1
                                left  = (nc5-1)*nattri_c + 1
                                right = nc5*nattri_c
                                call MPI_IRECV(P_recv(left:right),nattri_c,MPI_DOUBLE_PRECISION, &
                                    l_GPUs(i+1,j),41,MPI_COMM_WORLD,rq4(nc5),ierr)
                            end do
                        end if
                    end do
                end if
            end do
        end if
        !-----------------------------------------
            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

        !interior particles
        call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
            xmin1_c,ymin1_c,zmin1_c,xmax1_c,ymax1_c,zmax1_c, &
            0,N_inte,reflect,map_sub,kk,transfer,clmtrans)

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(inte_time,startEvent,stopEvent)
            #endif
        !-----------------------------------------
        if(rank < ppx*qqy) then
                #if _TIMING == 1
                istat = cudaEventRecord(startEvent,0)
                #endif

            if(nc5 > 0) then
                sum_recv = sum(N_recv(1:nc4))
                call MPI_WAITALL(nc5,rq4(1:nc5),status(:,1:nc5),ierr)
                call unpack_recv<<<ceiling(dble(sum_recv)/tPB),tPB>>>( &
                    P_recv,np_active,sum_recv)
                N_peri = N_peri + sum_recv
                np_active = N_inte + N_peri
            endif

                #if _TIMING == 1
                istat = cudaEventRecord(stopEvent,0)
                istat = cudaEventSynchronize(stopEvent)
                istat = cudaEventElapsedTime(tran_time10,startEvent,stopEvent)
                #endif
        end if

        call global2local<<<ceiling(dble(N_peri)/tPB),tPB>>>(N_inte, &
        N_peri,dble(ix1_c),dble(iy1_c))

    end subroutine particle_exchange2

end module eco_particle_exch2