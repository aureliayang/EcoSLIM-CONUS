module eco_particle_exch2
    use thrust
    use cudafor
    use eco_compact_util
    use eco_particle_loop
    use variable_list, only: N_recv, N_send, N_peri, N_inte, N_exit
    use variable_list, only: neigh_list, nattri_c
    ! neigh_list is 1 if neighbor exists while 0 if neighbor doesn't exist.

    use variable_list, only: holes, P_send, P_recv
    use variable_list, only: P_de, d_isValid, d_indices
    use variable_list, only: ix1, iy1, nnx1, nny1, reflect
    use variable_list, only: rank, t_rank, nind_c, tPB
    use variable_list, only: np_active !, ranknum, filenum

    use variable_list, only: xgmin,ygmin,zgmin,xgmax,ygmax,zgmax
    use variable_list, only: xmin1,ymin1,zmin1,xmax1,ymax1,zmax1
    use variable_list, only: inte_time

    use variable_list, only: tran_time1, tran_time2, tran_time3, tran_time4, tran_time5
    use variable_list, only: tran_time6, tran_time7, tran_time8, tran_time9, tran_time10
    use variable_list, only: startEvent, stopEvent

contains
    subroutine particle_exchange2()
        use mpi
        implicit none
        integer:: i, j, ierr, status(MPI_STATUS_SIZE,tPB)
        integer:: nc1, nc2, nc3, nc4, nc5, nc6, rq1(tPB), &
                  rq2(tPB), rq3(tPB), rq4(tPB)
        integer:: left, right, sum_recv, sum_send
        integer:: istat, temp
        real:: tran_t1, tran_t2, tran_t3, tran_t4, tran_t5
        real:: tran_t6, tran_t7, tran_t8, tran_t9, tran_t10
        real(8):: P_send_cpu(tPB*(17+nind_c*2))

        real(8):: k, one = -1.
        ! integer:: ik, jk, status2(MPI_STATUS_SIZE)
        integer:: d_isValid_cpu(200000)
        integer,allocatable:: holes_cpu(:)
        ! real(8):: P_cpu(19*4)
        ! we have to send and receive numbers first
        ! otherwise, when receive, we have to wait the receive of particle-number
        ! it has two-fold of meanings: 1. waiting the communication itself (copy to recv buff)
        ! 2. probably the number has not been sent by its neighbor yet since its neighbor
        ! is doing the sending one by one.
        ! but here, in order to send numbers first, we count them first in the particle
        ! loop which will consume extra time. Extra time means it can also be obtained
        ! in the thrustscan. But thrustscan can only be done one by one since d_indices
        ! is repeatedly used, so we can't get the number of all neighbors at one time.

            #if _TIMING == 1
        tran_time1=0.; tran_time2=0.; tran_time3=0.; tran_time4=0.; tran_time5=0.
        tran_time6=0.; tran_time7=0.; tran_time8=0.; tran_time9=0.; tran_time10=0.
            #endif

        sum_send = sum(N_send)

        nc1 = 0; nc3 = 0
        do i = 0, t_rank-1 ! go through to send particles
            if(neigh_list(i+1) > 0) then ! the number of GPUs, if >0 it's neighbor
                nc1 = nc1 + 1
                call MPI_ISEND(N_send(i+1),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq1(nc1),ierr)
            end if
        end do

        if(sum_send > 0) then

            call prepare_neigh_all<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            P_de(N_inte+1:N_inte+N_peri,13+2*nind_c),N_peri,-1)

            call thrustscan(d_isValid,N_peri,d_indices)

            allocate(holes(sum_send))

            call prepare_holes<<<ceiling(dble(N_peri)/tPB),tPB>>>(holes,N_peri)

            call select2send<<<ceiling(dble(sum_send)/tPB),tPB>>>(holes, &
            P_send,N_inte,sum_send,0)

            call compaction_inplace<<<ceiling(dble(N_peri)/tPB),tPB>>>(holes,N_inte,N_peri)

            N_peri = d_indices(N_peri)
            np_active = N_inte + N_peri

            P_send_cpu(1:sum_send*nattri_c) = P_send(1:sum_send*nattri_c)

            do i = 1, sum_send
                nc3 = nc3 + 1
                left  = (nc3-1)*nattri_c + 1
                right = nc3*nattri_c
                call MPI_ISEND(P_send(left:right),nattri_c,MPI_DOUBLE_PRECISION, &
                idnint(P_send_cpu(13+2*nind_c+left-1)),41,MPI_COMM_WORLD,rq3(nc3),ierr)
            end do

        end if
        !-----------------------------------------
        ! receive part.
        !-----------------------------------------
        nc2 = 0; nc4 = 0
        do i = 0, t_rank-1
            if(neigh_list(i+1) > 0) then
                nc2 = nc2 + 1 !neighbor number
                call MPI_IRECV(N_recv(i+1),1,MPI_INTEGER,i,40,MPI_COMM_WORLD,rq2(nc2),ierr)
            endif
        enddo
        if(nc2 > 0) call MPI_WAITALL(nc2,rq2(1:nc2),status(:,1:nc2),ierr)

        do i = 0, t_rank-1
            if(N_recv(i+1) > 0) then
                do j = 1, N_recv(i+1)
                    nc4 = nc4 + 1
                    left  = (nc4-1)*nattri_c + 1
                    right = nc4*nattri_c
                    call MPI_IRECV(P_recv(left:right),nattri_c,MPI_DOUBLE_PRECISION, &
                        i,41,MPI_COMM_WORLD,rq4(nc4),ierr)
                end do
            end if
        end do
        !-----------------------------------------
            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

        !interior particles
        call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
            0,N_inte,ix1,iy1,reflect,rank)

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(inte_time,startEvent,stopEvent)
            #endif
        !-----------------------------------------
            #if _TIMING == 1
            istat = cudaEventRecord(startEvent,0)
            #endif

        if(nc4 > 0) then
            sum_recv = sum(N_recv)
            call MPI_WAITALL(nc4,rq4(1:nc4),status(:,1:nc4),ierr)
            call unpack_recv<<<ceiling(dble(sum_recv)/tPB),tPB>>>( &
                P_recv,np_active,sum_recv)
            N_peri = N_peri + sum_recv
            np_active = N_inte + N_peri
        endif

            #if _TIMING == 1
            istat = cudaEventRecord(stopEvent,0)
            istat = cudaEventSynchronize(stopEvent)
            istat = cudaEventElapsedTime(tran_time10,startEvent,stopEvent)
            #endif

    end subroutine particle_exchange2

end module eco_particle_exch2