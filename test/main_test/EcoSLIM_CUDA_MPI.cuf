! Last Change:  2020-11-12 17:43:44
!--------------------------------------------------------------------
! **EcoSLIM** is a Lagrangian, particle-tracking that simulates advective
! and diffusive movement of water parcels.  This code can be used to
! simulate age, diagnosing travel times, source water composition and
! flowpaths.  It integrates seamlessly with **ParFlow-CLM**.
!
! Developed by: Reed Maxwell-August 2016 (rmaxwell@mines.edu)
!
! Contributors: Laura Condon (lecondon@email.arizona.edu)
!               Mohammad Danesh-Yazdi (danesh@sharif.edu)
!               Lindsay Bearup (lbearup@usbr.gov)
!
! released under GNU LPGL, see LICENSE file for details
!--------------------------------------------------------------------
! 06/26/2021 GPU version, Chen Yang
!--------------------------------------------------------------------
program EcoSLIM
use mpi
use cudafor
use mrand
use thrust
use utilities
use particle_loop
use mpiDeviceUtil
use variable_list
use hdf5_file_read
! use hdf5_file_write
use subdomain_bound
use create_subdomain
use add_particles
use compact_array
! use exchange_zone
use exchange_particles

implicit none
integer(8),pinned,allocatable:: p_num_cpu(:,:)
type(curandStateXORWOW),allocatable,pinned:: h_cpu(:)
integer:: request2
!--------------------------------------------------------------------
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, t_rank, ierr)
    ! t_tank = ppx*qqy + 1. ppx*qqy is the total number of subdomains,
    ! and is also be the index number of manager rank. rank number of workers is 0 to ppx*qqy.
    ! one more rank for DDC, so the number of subdomains is t_rank-1
    ! be careful about this rank when doing everything

    call open_hdf5_interface()
!--------------------------------------------------------------------
    ! Get and set unique device
    call assignDevice(deviceID)
    call MPI_GET_PROCESSOR_NAME(hostname, namelength, ierr)
    write(message,"('[',i2.2 ,'] host: ', a, ', device: ', i2.2, a)") &
    rank, trim(hostname), deviceID, new_line(' ')
    offset = len(trim(message))*rank

    call MPI_FILE_OPEN(MPI_COMM_WORLD, 'EcoSLIM_Device_Utility.txt', &
    MPI_MODE_WRONLY + MPI_MODE_CREATE, MPI_INFO_NULL, fh0, ierr)
    call MPI_FILE_SEEK(fh0,offset,MPI_SEEK_SET,ierr)
    call MPI_FILE_WRITE(fh0,message,len(trim(message)),MPI_CHARACTER, &
        MPI_STATUS_IGNORE, ierr)
    call MPI_FILE_CLOSE(fh0, ierr)

!--------------------------------------------------------------------
    ! how to do timing is also a new question when asyn later.
    ! Set up timing
    Total_time1 = 0.d0
    Total_time2 = 0.d0
    t1 = 0.d0
    t2 = 0.d0
    IO_time_read = 0.d0
    IO_time_write = 0.d0
    parallel_time = 0.d0
    sort_time = 0.d0

!--------------------------------------------------------------------
    ! Read inputs, set up domain, write the log file
    ! no need to exclude the manager rank since only several scalar variables are read
    ! and these scalar variables have already been on the mananger rank due to declaration
    ! open SLIM input .txt file
    open (10,file='slimin.txt')

    ! read SLIM run name
    read(10,*) runname

    ! read ParFlow run name
    read(10,*) pname

    ! read DEM file name
    read(10,*) DEMname

    if(rank == 0) then
        ! open/create/write the output log.txt file. If doesn't exist, it's created.
        open(11,file=trim(runname)//'_log.txt')
        write(11,*) '### EcoSLIM Log File'
        write(11,*)
        write(11,*) 'run name:',trim(runname)
        write(11,*)
        write(11,*) 'ParFlow run name:',trim(pname)
        write(11,*)
        if (DEMname /= '') then
            write(11,*) 'ParFlow DEM name:',trim(DEMname)
        else
            write(11,*) 'Not reading ParFlow DEM'
        end if
        write(11,*)
    endif ! rank = 0, write logfile through channel 11

    ! read domain number of cells and number of particles to be injected
    read(10,*) nx
    read(10,*) ny
    read(10,*) nz
    read(10,*) nCLMsoil
    read(10,*) ppx
    read(10,*) qqy
    ! read in number of particles for IC (if np_ic = -1 then restart from a file)
    read(10,*) np_ic

    ! read in the number of particles total
    read(10,*) np

    if(rank == 0) then
        ! check to make sure we don't assign more particles for IC than we have allocated
        ! in total
        if (np_ic > np) then
        write(11,*) 'warning NP_IC greater than IC'
        np = np_ic
        end if
        ! write nx, ny, nz, and np in the log file
        write(11,*) 'Grid information'
        write(11,*) 'nx:',nx
        write(11,*) 'ny:',ny
        write(11,*) 'nz:',nz
        write(11,*)
        write(11,*) 'Particle IC Information'
        write(11,*) 'np IC:',np_ic
        if (np_ic == -1) &
        write(11,*) 'Reading particle restart file:',trim(runname)//'_particle_restart.bin'
        write(11,*) 'np:',np
    endif ! rank=0

    ! nCLMsoil = 10 ! number of CLM soil layers over the root zone !this doesn't matter
    nzclm = 13 + nCLMsoil ! CLM output is 13+nCLMsoil layers for different variables not domain NZ,
                          ! e.g. 23 for 10 soil layers (default) and 17 for 4 soil layers (Noah soil
                          ! layer setup)
    n_constituents = 9

    allocate(dz(nz),dz2(nz),dz_de(nz))
    ! have to do this here since the following read of dz

    ! read dx, dy as scalars
    read(10,*) dx
    read(10,*) dy
    ! read dz as an array
    read(10,*) dz(1:nz)
    dz2 = dz
    ! read in (constant for now) ParFlow dt
    read(10,*) pfdt
    ! read in parflow start and stop times
    read(10,*) pft1
    read(10,*) pft2
    read(10,*) tout1
    read(10,*) n_cycle
    read(10,*) add_f

    pfnt = n_cycle*(pft2-pft1+1)
    outkk = tout1 + 1

    ! IO control, each value is a timestep interval, e.g. 1= every timestep, 2=every other, 0 = no writing
    read(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
    read(10,*) ibinpntswrite  !  controls VTK, binary output of particle locations and attributes
    read(10,*) etwrite        !  controls ASCII ET output
    read(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
                              ! ages are mapped to a grid and written every N timesteps

    ! allocate and assign timesteps
    ! this can be a scalar and update every timestep
    allocate(Time_Next(pfnt))
    Time_Next=0.d0
    do kk = outkk, pfnt
        Time_Next(kk) = float(kk)*pfdt
    end do
    Time_first = float(outkk-1)*pfdt

    ! read in velocity multiplier
    read(10,*) V_mult
    ! do we read in clm evap trans?
    read(10,*) clmtrans
    ! do we read in clm output file?
    read(10,*) clmfile
    ! read in IC number of particles for flux
    read(10,*) iflux_p_res
    ! read in density h2o
    read(10,*) denh2o
    ! read in diffusivity
    ! moldiff = (1.15e-9)*3600.d0
    read(10,*) moldiff
    ! fraction of dx/Vx
    read(10,*) dtfrac

    if(rank == 0) then
        ! wite out log file
        write(11,*)
        write(11,*) 'Grid Dimensions'
        write(11,'(" dx:",e12.5)') dx
        write(11,'(" dy:",e12.5)') dy
        write(11,'(" dz:",*(e12.5,", "))') dz(1:nz)
        write(11,*)
        write(11,*) 'Timestepping Information'
        write(11,'(" ParFlow delta-T, pfdt:",e12.5)') pfdt
        write(11,'(" ParFlow timesteps, pfnt:",i12)') pfnt
        write(11,'(" ParFlow start step, pft1:",i12)') pft1
        write(11,'(" ParFlow end step, pft2:",i12)') pft2
        write(11,'(" Output step start:",i12)') outkk
        write(11,'(" Time loops, cycles, n_cycle:",i12)') n_cycle
        write(11,'(" Total time steps:",i12)') pfnt

        write(11,*)
        write(11,*) 'V mult: ',V_mult,' for forward/backward particle tracking'
        write(11,*) 'CLM Trans: ',clmtrans,' adds / removes particles based on LSM fluxes'
        write(11,*)
        write(11,*) 'Physical Constants'
        write(11,*) 'denh2o: ',denh2o,' M/L^3'
        write(11,*) 'Molecular Diffusivity: ',moldiff,' '
        !write(11,*) 'Fractionation: ',Efract,' '
        write(11,*)
        write(11,*) 'Numerical Stability Information'
        write(11,'(" dtfrac: ",e12.5," fraction of dx/Vx")') dtfrac
    endif

    read(10,*) nind
    read(10,*) Indname
    if(rank == 0) then
        write(11,*)
        write(11,*) 'Indicator File'
        write(11,*) nind, 'Indicators'
    endif

    ! end of SLIM input
    close(10)
!--------------------------------------------------------------------
    print *, 'Read ok1'
    ! build new communicators for communication
    call MPI_COMM_GROUP(MPI_COMM_WORLD, world_group, ierr)
    manage_ranks = ppx*qqy ! only one rank used to manage
    call MPI_GROUP_EXCL(world_group, 1, manage_ranks, work_group, ierr)
    call MPI_GROUP_INCL(world_group, 1, manage_ranks, manage_group, ierr)
    call MPI_COMM_CREATE(MPI_COMM_WORLD, work_group, work_comm, ierr)
    call MPI_COMM_CREATE(MPI_COMM_WORLD, manage_group, manage_comm, ierr)
!--------------------------------------------------------------------
    ! debug
    write(ranknum,'(i5.5)') rank
    open(10,file='Read_check.'//trim(adjustl(ranknum)))
        write(10,*) runname
        write(10,*) pname
        write(10,*) DEMname
        write(10,*) nx
        write(10,*) ny
        write(10,*) nz
        write(10,*) nCLMsoil
        write(10,*) ppx
        write(10,*) qqy
        write(10,*) np_ic
        write(10,*) np
        write(10,*) dx
        write(10,*) dy
        write(10,*) dz(1:nz)
        write(10,*) pfdt
        write(10,*) pft1
        write(10,*) pft2
        write(10,*) tout1
        write(10,*) n_cycle
        write(10,*) add_f
        write(10,*) ipwrite        ! controls an ASCII, .3D particle file not recommended due to poor performance
        write(10,*) ibinpntswrite  ! controls VTK, binary output of particle locations and attributes
        write(10,*) etwrite        ! controls ASCII ET output
        write(10,*) icwrite        ! controls VTK, binary grid based output where particle masses, concentrations,
        write(10,*) V_mult
        write(10,*) clmtrans
        write(10,*) clmfile
        write(10,*) iflux_p_res
        write(10,*) denh2o
        write(10,*) moldiff
        write(10,*) dtfrac
        write(10,*) nind
        write(10,*) Indname
    close(10)

!--------------------------------------------------------------------
    if(rank == ppx*qqy) then

        allocate(grid(ppx*qqy,4),grid_old(ppx*qqy,4))
        allocate(Zonet_old(-buff+1:nx+buff,-buff+1:ny+buff))
        allocate(Zonet_new(-buff+1:nx+buff,-buff+1:ny+buff))
        allocate(p_num(nx,ny),c_sum(max(nx,ny)),d_indices(np))
        grid = 0; Zonet_new = -1; Zonet_new(1:nx,1:ny) = 0
            allocate(p_num_cpu(nx,ny))
            ! p_num_cpu = 1_8 ! check such an initialization. Checked and should be right.
        p_num = 1
            p_num_cpu = 0
            write(*,'(8(9(i3,1x),/))') ((p_num_cpu(i,j),i=1,nx),j=1,ny)
            p_num_cpu = p_num
            write(*,'(8(9(i3,1x),/))') ((p_num_cpu(i,j),i=1,nx),j=1,ny)
            write(*,'(10(11(i3,1x),/))') ((Zonet_old(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
            write(*,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
            write(*,'(4(4(i3,1x),/))') ((grid(i,j),i=1,ppx*qqy),j=1,4)

    end if   ! set the manager
    print *, 'Set manager ok2'
!--------------------------------------------------------------------
    call global_xyz() ! on workers
    ! restart file info inside, so have to call it before restart.
    call alloc_arrays_const() ! on workers, and P array needs nind
    ! the above subroutines will work when setting manager.
!--------------------------------------------------------------------
    if(np_ic /= -1) then

        ! if not restart, we do decompostion and build topology
        call gridinfo()

        ! doing this on manager, and then we get subdomain info in grid array
        ! for init, we don't have to build p_sum on each rank, just use that for
        ! the whole domain with init value of 1.
        ! if restart, no need to do this part, directly read necessary info from restart files
            if(rank /= ppx*qqy)then
                open(10,file='grid.'//trim(adjustl(ranknum)))
                    write(10,'(4(4(i3,1x),/))') ((grid(i,j),j=1,4),i=1,ppx*qqy)
                close(10)
            else
                open(10,file='grid.'//trim(adjustl(ranknum)))
                    write(10,'(10(11(i3,1x),/))') ((Zonet_old(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                    write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                    write(10,'(4(4(i3,1x),/))') ((grid(i,j),j=1,4),i=1,ppx*qqy)
                    write(10,'(4(4(i3,1x),/))') ((grid_old(i,j),j=1,4),i=1,ppx*qqy)
                close(10)
            endif

        call MPI_BARRIER(MPI_COMM_WORLD, ierr)
        call copy_grid() ! doing this on workers
            open(10,file='topology.'//trim(adjustl(ranknum)))
                write(10,'(8(i3,1x))') ix1,iy1,nnx1,nny1,ix2,iy2,nnx2,nny2
            close(10)

        call alloc_arrays_temp() ! doing this on workers
            if(rank /= ppx*qqy)then
                open(10,file='Zone_de_before.'//trim(adjustl(ranknum)))
                    write(10,*) Zone_de
                close(10)
            endif

        call receive_Zone_de(request2) ! this needs all the ranks, both manager and workers
        if(rank /= ppx*qqy) call MPI_WAIT(request2,status,ierr)
            if(rank /= ppx*qqy)then
                open(10,file='Zone_de_after.'//trim(adjustl(ranknum)))
                    write(10,*) Zone_de
                close(10)
            endif
    end if
    ! for following, if there is [if (rank /= ppx*qqy)] in the subroutine, please
    ! just keep it there now, and later we can find a better way.
!--------------------------------------------------------------------
    if(rank /= ppx*qqy)then
        call local_xyz()

        if(rank == 0) then
            flush(11)
            close(11)
        endif

        ! Porosity = 0.3d0
        ! Read porosity values
        fname=trim(adjustl(pname))//'.out.porosity.h5'
        call read_h5_file(Porosity,0)

        if(np_ic /= -1) then

            ! Saturation = 0.8d0
            pfkk = pft1 - 1
            write(filenum,'(i5.5)') pfkk
            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Saturation,0)

                allocate(h_cpu(np))
                h_cpu = h
                write(*,*) rank,'h_before',h_cpu(1)
            call createRand_init<<<ceiling(dble(nnx1*nny1*nz)/tPB),tPB>>>(nx,ny,nz, &
                rank,np_ic,nnx1,nny1)
                h_cpu = h
                write(*,*) rank,'h_after',h_cpu(1)
        endif

        print *, 'ok3'
        dz_de  = dz2 ! I can't understand why dz doesn't work.
        Ind_de = Ind
        Porosity_de   = Porosity
        Saturation_de = Saturation
        P_de = P

        open(10,file='Variable_init.'//trim(adjustl(ranknum)))
            write(10,*) dz2
            write(10,*) Ind
            write(10,*) Porosity
            write(10,*) Saturation
            write(10,'(19(f5.2,1x))') ((P(i,j),j=1,19),i=1,10)
        close(10)

        ! Define initial particles' locations and mass
        if (np_ic > 0)  then
            np_active = 0
            pid = 0
            if (np_active + np_ic*nnx1*nny1*nz >= np ) then
                write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                goto 9090
            endif

            call add_init_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
            tPB >>> (P_de,np_ic,nind,denh2o,tout1,nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de)

            np_active = np_ic*nnx1*nny1*nz
            pid = np_ic*nnx1*nny1*nz

        end if
            open(10,file='P_init.'//trim(adjustl(ranknum))//'.txt')
                write(10,*) np_active
                write(10,*) pid
                P = P_de
                write(10,'(200(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            close(10)

        !--------------------------------------------------------------------
            ! keep the following output as it is. we can figure out a better way to handle them later.
            call MPI_FILE_OPEN(MPI_COMM_SELF,logf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh4,ierr)

            write(message,'(a,a)') ' **** Transient Simulation Particle Accounting ****', new_line(' ')
            call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

            write(message,'(a,a)') ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
                                    ETOut NP_PrecipIn NP_ETOut NP_QOut NP_active_old NP_filtered', new_line(' ')
            call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

            ! open exited particle file and write header
            call MPI_FILE_OPEN(MPI_COMM_SELF,exitedf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh3,ierr)
            print *, 'ok4'
            if(rank == 0) then

                ! if(ipwrite < 0) then
                ! ! open/create/write the 3D output file which will write particles out each timestemp, very slowly in parallel
                !     open(214,file=trim(runname)//'_total_particle_trace.3D')
                !     write(214,*) 'X Y Z TIME'
                ! end if ! ipwrite < 0
                ! this write is originally in the particle loop. now particle loop is on GPU. This can not be reserved.

                open(13,file=trim(runname)//'_ET_output.txt')
                write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

                open(15,file=trim(runname)//'_flow_output.txt')
                write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

                open(16,file=trim(runname)//'_PET_balance.txt')
                write(16,*) 'TIME P[kg] ET[kg]'

            endif

            call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
            tPB>>>(nnx1,nny1,buff,neigh_list,rank,t_rank-1)
            ! ix2, iy2, nnx2, and nny2 are only used for reading files
                print *, 'ok5'
            if(sum(neigh_list) > 0) then
                allocate(N_recv(sum(neigh_list)),N_send(sum(neigh_list)))
                N_send = 0; N_recv = 0
            end if
            ! don't have to do this every timestep, only after the update of decomposition

            open(10,file='Neighbor.'//trim(adjustl(ranknum)))
                write(10,*) neigh_list
                write(10,*) N_recv
            close(10)

            ! Intialize cuRand device API and this part need to check through again.
            call createRand_loop<<< ceiling(dble(np_active)/tPB),tPB >>>(np, rank, pfnt)
    end if
    print *, 'ok6'
!--------------------------------------------------------------------
    pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

    do kk = outkk, pfnt

        if (rank /= ppx*qqy) then
            ! reset ParFlow counter for cycles
            if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1
            ! adjust the file counters
            pfkk = pfkk + 1

            ! Read the velocities computed by ParFlow

            write(filenum,'(i5.5)') pfkk

            fname=trim(adjustl(pname))//'.out.velx.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vx,1)

            fname=trim(adjustl(pname))//'.out.vely.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vy,2)

            fname=trim(adjustl(pname))//'.out.velz.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Vz,3)

            fname=trim(adjustl(pname))//'.out.satur.'//trim(adjustl(filenum))//'.h5'
            call read_h5_file(Saturation,0)

            if (clmtrans) then
                ! Read in the Evap_Trans
                fname=trim(adjustl(pname))//'.out.evaptrans.'//trim(adjustl(filenum))//'.h5'
                call read_h5_file(EvapTrans,0)

                if (clmfile) then
                    ! Read in CLM output file @RMM to do make this input
                    fname=trim(adjustl(pname))//'.out.clm_output.'//trim(adjustl(filenum))//'.C.h5'
                    call read_h5_file(CLMvars,5)
                end if
            end if

            ! Determine whether to perform forward or backward patricle tracking
            Vx = Vx * V_mult
            Vy = Vy * V_mult
            Vz = Vz * V_mult

            out_age_cpu  = 0.d0
            out_mass_cpu = 0.d0
            out_comp_cpu = 0.d0
            out_np_cpu   = 0
            et_age_cpu   = 0.d0
            et_mass_cpu  = 0.d0
            et_comp_cpu  = 0.d0
            et_np_cpu    = 0
            mean_age     = 0.d0
            mean_comp    = 0.d0
            total_mass   = 0.d0

            Vx_de = Vx
            Vy_de = Vy
            Vz_de = Vz
            Saturation_de = Saturation
            EvapTrans_de  = EvapTrans
            CLMvars_de    = CLMvars(:,:,11)
            ! probably this can be deallocated after adding of particles
            ! Now just think about the hourly add of particles.
            out_age_de  = out_age_cpu
            out_mass_de = out_mass_cpu
            out_comp_de = out_comp_cpu
            out_np_de   = out_np_cpu

            et_age_de  = et_age_cpu
            et_mass_de = et_mass_cpu
            et_comp_de = et_comp_cpu
            et_np_de   = et_np_cpu

            mean_age_de   = mean_age
            mean_comp_de  = mean_comp
            total_mass_de = total_mass

            C    = 0.d0
            C_de = C
            ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
            ! the following adding of particles.
            !----------------------------------------
            if (clmtrans) then
                if (np_active < np) then

                    call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>> ( &
                    PET_balance_de,d_isValid,nnx1,nny1,nz,dx,dy,dz_de,pfdt,denh2o)
                    ! d_isValid has the length as P or of np which is assigned in input script
                    ! so nnx1**nny1*nz should be smaller than np
                    ! we don't care about the buffer zone for add of particles
                    ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                    ! in main program and pass arguments to other subroutines.

                    ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                    call thrustscan(d_isValid,nnx1*nny1,d_indices)

                    i_added_particles = d_indices(nnx1*nny1) * iflux_p_res ! should be legal?
                    if(np_active + i_added_particles >= np) then
                        write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                        write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                        call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                        MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                        goto 9090
                    endif

                    call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                    tPB >>> (d_indices,P_de,iflux_p_res,np_active,pid,nind, &
                    nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de,pfdt,denh2o,kk,tout1)

                    np_active = np_active + d_indices(nnx1*nny1) * iflux_p_res
                    pid = pid + d_indices(nnx1*nny1) * iflux_p_res

                end if
            end if

            open(10,file='P_new.'//trim(adjustl(ranknum))//'.txt')
                write(10,*) np_active
                write(10,*) pid
                P = P_de
                write(10,'(200(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            close(10)

            PET_balance = PET_balance_de
            call MPI_ALLReduce(MPI_IN_PLACE,PET_balance,2,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
            ! print to log file, or PET_balance can be a long array and do print at last
            print *, rank, 'ok7', PET_balance

            call particles_separation<<<ceiling(dble(np_active)/tPB),tPB>>> ( &
                P_de,d_isValid,xmin3,ymin3,xmax3,ymax3,np_active,nind,ix1,iy1,dx,dy)
            call thrustscan(d_isValid,np_active,d_indices)
            N_peri = np_active - d_indices(np_active)
            allocate(holes(N_peri))
            call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                holes,d_indices,d_isValid,np_active)
            call select2np_active<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                holes,P_de,np_active,N_peri)
            call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                holes,d_indices,d_isValid,P_de,0,np_active)
            N_inte = d_indices(np_active); deallocate(holes)
            call connect_recv<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                P_de,N_inte,N_peri)

            print *, 'rank',rank,N_peri,N_inte
            open(10,file='P_separa.'//trim(adjustl(ranknum))//'.txt')
                write(10,*) np_active
                write(10,*) pid
                P = P_de
                write(10,'(200(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
            close(10)

            ! peripheric particles
            call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
                P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
                et_age_de,et_mass_de,et_comp_de,et_np_de, &
                xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
                xmin2,ymin2,zmin2,xmax2,ymax2,zmax2, &
                dx,dy,pfdt,moldiff,denh2o,dtfrac,N_inte,N_peri, &
                nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

                open(10,file='P_peri.'//trim(adjustl(ranknum))//'.txt')
                    write(10,*) np_active
                    write(10,*) pid
                    P = P_de
                    write(10,'(200(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
                close(10)

            call particle_exchange(nc5,rq4,status2)

            !! interior particles
            !call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
            !    P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            !    et_age_de,et_mass_de,et_comp_de,et_np_de, &
            !    xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            !    xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
            !    dx,dy,pfdt,moldiff,denh2o,dtfrac,0,N_inte, &
            !    nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

            print *, rank, 'ok8'

            if(nc5 > 0) call MPI_WAITALL(nc5,rq4(1:nc5),status2(:,1:nc5),ierr)
            ! call Update_C_Array<<< ceiling(dble(np_active)/tPB),tPB >>>( &
            ! P_de,dx,dy,dz_de,nz,ix1,iy1,nnx1,nny1,buff, &
            ! mean_age_de,mean_comp_de,total_mass_de,np_active)

            ! then separate inactive particles
            call prepare_neighbor<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                P_de(:,8),d_isValid,np_active,0)
            call thrustscan(d_isValid,np_active,d_indices)
            ! active and ET-inactive particles in overlap zone are sent
            ! out of domain, all outfolw-inactive, received ET-inactive are scanned here
            N_exit = np_active - d_indices(np_active)

            if(N_exit > 0) then

                allocate(holes(N_exit),P_exit(N_exit,17+2*nind))
                ! P_exit has to be deallocated after the use in main
                call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,d_indices,d_isValid,np_active)
                call select2exit<<<ceiling(dble(N_exit)/tPB),tPB>>>(&
                    holes,P_de,P_exit,N_exit)
                call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
                    holes,d_indices,d_isValid,P_de,0,np_active)
                ! update the slots after np_active
                np_active = d_indices(np_active)
                deallocate(holes,P_exit)

            end if

        end if
    end do ! timesteps and cycles

    9090 continue

    call close_hdf5_interface()
    call MPI_FINALIZE(ierr)

end program EcoSLIM

