module exchange_particles
    ! subroutine particle_exchange
    use thrust
    use cudafor
    use compact_array
    use variable_list, only: N_recv, N_send, N_exit, N_peri, N_inte
    use variable_list, only: neigh_list
    ! N_exit is a cpu scalar
    ! neigh_list is allocated after getting number of total GPUs.
    ! it is 1 if neighbor exists while 0 if neighbor doesn't exist.
    ! N_send has the same length of neigh_list and was built one by one.
    ! N_recv is allocated or deallocated after initialization/update of grid.
    ! N_recv has the length of its neighbors and has been initialized as 0.
    use variable_list, only: holes, P_exit
    use variable_list, only: P_de, d_isValid, d_indices, C_de
    use variable_list, only: dx, dy, dz_de, nz, np
    use variable_list, only: mean_age_de, mean_comp_de, total_mass_de
    use variable_list, only: ix1, iy1, nnx1, nny1, buff
    use variable_list, only: rank, t_rank, nind, tPB, work_comm
    use variable_list, only: np_active

contains
    subroutine particle_exchange(nc5,rq4,status)
        use mpi
        implicit none
        integer:: i, j, ierr, status(MPI_STATUS_SIZE,t_rank-1)
        integer:: nc1, nc2, nc3, nc4, nc5, nc6, rq1(t_rank-1), &
                  rq2(t_rank-1), rq3(t_rank-1), rq4(t_rank-1)
        integer:: left, right, nattri, sum_recv, sum_send, istat
        ! we have to send and receive numbers first
        ! otherwise, when receive, we have to wait the receive of particle-number
        ! it has two-fold of meanings: 1. waiting the communication itself (copy to recv buff)
        ! 2. probably the number has not been sent by its neighbor yet since its neighbor
        ! is doing the sending one by one.
        ! but here, in order to send numbers first, we count them first in the particle
        ! loop which will consume extra time. Extra time means it can also be obtained
        ! in the thrustscan. But thrustscan can only be done one by one since d_indices
        ! is repeatedly used, so we can't get the number of all neighbors at one time.

        nattri = 17 + 2*nind
        nc1 = 0; nc3 = 0
        do i = 0, t_rank-2 ! go through to send particles
            if(neigh_list(i+1) > 0) then ! the number of GPUs, if >0 it's neighbor
                nc1 = nc1 + 1 ! the number of neighbors
                call prepare_neighbor<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                    P_de(N_inte+1:N_inte+N_peri,13+2*nind),d_isValid,N_peri,i)
                ! get d_isValid, set it 0 where the particle will be sent to neighbor i
                call thrustscan(d_isValid,N_peri,d_indices)
                ! do prefix-sum and get d_indices
                N_send(nc1) = N_peri - d_indices(N_peri)
                call MPI_ISEND(N_send(nc1),1,MPI_INTEGER,i,40,work_comm,rq1(nc1),ierr)
                ! each neighbor use different sending buffer, no wait
                if(N_send(nc1) > 0) then ! gpu number
                    nc3 = nc3 + 1 ! only the neighbor with particles
                    allocate(holes(N_send(nc1)))
                    call prepare_holes<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                        holes,d_indices,d_isValid,N_peri)
                    sum_send = sum(N_send(1:nc1-1)) ! have sent before this one
                    call select2end<<<ceiling(dble(N_peri)/tPB),tPB>>>(holes, &
                        P_de,N_inte,N_send(nc1),sum_send,np)
                    left  = np - sum(N_send(1:nc1)) + 1
                    right = np - sum(N_send(1:nc1-1))
                    call MPI_ISEND(P_de(left:right,:),N_send(nc1)*nattri, &
                        MPI_DOUBLE_PRECISION,i,41,work_comm,rq3(nc3),ierr)
                    call compaction_inplace<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
                        holes,d_indices,d_isValid,P_de,N_inte,N_peri)
                    N_peri = d_indices(N_peri); deallocate(holes)
                end if
            end if
        end do
        !-----------------------------------------
        ! receive part.
        !-----------------------------------------
        nc2 = 0
        do i = 0, t_rank-2
            if(neigh_list(i+1) > 0) then
                nc2 = nc2 + 1 !neighbor number
                call MPI_IRECV(N_recv(nc2),1,MPI_INTEGER,i,40,work_comm,rq2(nc2),ierr)
            end if
        end do
        call MPI_WAITALL(nc2,rq2(1:nc2),status(:,1:nc2),ierr)
        nc4 = 0; nc5 = 0
        do i = 0, t_rank-2
            if(neigh_list(i+1) > 0) then
                nc4 = nc4 + 1
                if(N_recv(nc4) > 0) then
                    nc5 = nc5 + 1
                    left  = N_inte + N_peri + sum(N_recv(1:nc4-1)) + 1
                    right = N_inte + N_peri + sum(N_recv(1:nc4))
                    call MPI_IRECV(P_de(left:right,:),N_recv(nc4)*nattri, &
                    MPI_DOUBLE_PRECISION,i,41,work_comm,rq4(nc5),ierr)
                end if
            end if
        end do
        !-----------------------------------------
        N_peri = N_peri + sum(N_recv)
        np_active = N_inte + N_peri

    end subroutine particle_exchange

end module exchange_particles