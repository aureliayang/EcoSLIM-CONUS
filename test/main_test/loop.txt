

    if(rank /= ppx*qqy)then
        open(10,file='Restart_before.'//trim(adjustl(ranknum)))
            write(10,'(8(i3,1x))') ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
            write(10,'(19(f15.3,1x))') P(1,:)
        close(10)
    else
        open(10,file='Restart_before.'//trim(adjustl(ranknum)))
            write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
            write(10,*) grid
        close(10)
    endif
!--------------------------------------------------------------------
    if (np_ic == -1) then
        ! if np_ic = -1 then we read a restart file
        ! move restart here is due to we want to do the allocate of temp here which can keep
        ! a clear structure, and then we can read the necessary files such as Ind etc.
        ! But if we do allocate, it needs the subdomain dim etc., so we read restart info here.

        call read_restarts()
        ! this is in the subdomain_bound module, which is in the ecoslim_subdomain_bc file

        ! 1. read ix1 etc. on worker. specific files or in restarts with P
        ! 2. read zones on workers   ???? copy to cpu
        ! 3. read grid and Zonet_old on manager??? Zonet_new
        call alloc_arrays_temp()

        if(rank /= ppx*qqy)then
            open(10,file='Restart_after.'//trim(adjustl(ranknum)))
                write(10,'(8(i3,1x))') ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
                write(10,'(19(f15.3,1x))') P(1,:)
                write(10,*) Zone_de
            close(10)
        else
            open(10,file='Restart_after.'//trim(adjustl(ranknum)))
                write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                write(10,*) grid
            close(10)
        endif

        call read_Zone_de()
        if(rank /= ppx*qqy)then
            open(10,file='Restart_after2_Zone_de.'//trim(adjustl(ranknum)))
                write(10,*) Zone_de
            close(10)
        endif
    end if
!--------------------------------------------------------------------

pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

do kk = outkk, pfnt
    if (rank /= ppx*qqy) then

        ! reset ParFlow counter for cycles
        if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1
        ! adjust the file counters
        pfkk = pfkk + 1
        ! Read the velocities computed by ParFlow
        write(filenum,'(i5.5)') pfkk
        Vx = 1.d-2
        Vy = 1.d-2
        Vz = 1.d-2
        Saturation = 0.8d0

        if (clmtrans) then
            ! Read in the Evap_Trans
            EvapTrans = 0.d0
            EvapTrans(2,:,5) = 1.d-8

            if (clmfile) then
                CLMvars = 0.d0
            end if
        end if

        ! Determine whether to perform forward or backward patricle tracking
        Vx = Vx * V_mult
        Vy = Vy * V_mult
        Vz = Vz * V_mult

        out_age_cpu  = 0.d0
        out_mass_cpu = 0.d0
        out_comp_cpu = 0.d0
        out_np_cpu   = 0
        et_age_cpu   = 0.d0
        et_mass_cpu  = 0.d0
        et_comp_cpu  = 0.d0
        et_np_cpu    = 0
        mean_age     = 0.d0
        mean_comp    = 0.d0
        total_mass   = 0.d0

        Vx_de = Vx
        Vy_de = Vy
        Vz_de = Vz
        Saturation_de = Saturation
        EvapTrans_de  = EvapTrans
        CLMvars_de    = CLMvars(:,:,11)
        ! probably this can be deallocated after adding of particles
        ! Now just think about the hourly add of particles.
        out_age_de  = out_age_cpu
        out_mass_de = out_mass_cpu
        out_comp_de = out_comp_cpu
        out_np_de   = out_np_cpu

        et_age_de  = et_age_cpu
        et_mass_de = et_mass_cpu
        et_comp_de = et_comp_cpu
        et_np_de   = et_np_cpu

        mean_age_de   = mean_age
        mean_comp_de  = mean_comp
        total_mass_de = total_mass

        C    = 0.d0
        C_de = C
        ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
        ! the following adding of particles.
        !----------------------------------------
        if (clmtrans) then
            if (np_active < np) then

                call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>> ( &
                PET_balance_de,d_isValid,nnx1,nny1,nz,dx,dy,dz_de,pfdt,denh2o)
                ! d_isValid has the length as P or of np which is assigned in input script
                ! so nnx1**nny1*nz should be smaller than np
                ! we don't care about the buffer zone for add of particles
                ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                ! in main program and pass arguments to other subroutines.

                ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                call thrustscan(d_isValid,nnx1*nny1,d_indices)

                i_added_particles = d_indices(nnx1*nny1) * iflux_p_res ! should be legal?
                if(np_active + i_added_particles >= np) then
                    write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                    write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                    goto 9090
                endif

                call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                tPB >>> (d_indices,P_de,iflux_p_res,np_active,pid,nind, &
                nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de,pfdt,denh2o,kk,tout1)

                np_active = np_active + d_indices(nnx1*nny1) * iflux_p_res
                pid = pid + d_indices(nnx1*nny1) * iflux_p_res

            end if
        end if

        ! this has to be checked later.
        ! call MPI_ALLReduce(MPI_IN_PLACE, PET_balance_da(kk,:), 2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        PET_balance = PET_balance_de
        call MPI_ALLReduce(MPI_IN_PLACE,PET_balance,2,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
        ! print to log file, or PET_balance can be a long array and do print at last

        print *, rank, 'ok7', PET_balance

        call particles_separation<<<ceiling(dble(np_active)/tPB),tPB>>> ( &
            P_de,d_isValid,xmin3,ymin3,xmax3,ymax3,np_active,nind,ix1,iy1,dx,dy)
        call thrustscan(d_isValid,np_active,d_indices)
        N_peri = np_active - d_indices(np_active)
        allocate(holes(N_peri))
        call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,np_active)
        call select2np_active<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            holes,P_de,np_active,N_peri)
        call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,P_de,0,np_active)
        N_inte = d_indices(np_active); deallocate(holes)
        call connect_recv<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            P_de,N_inte,N_peri)

        ! peripheric particles
        call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            et_age_de,et_mass_de,et_comp_de,et_np_de, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin2,ymin2,zmin2,xmax2,ymax2,zmax2, &
            dx,dy,pfdt,moldiff,denh2o,dtfrac,N_inte,N_peri, &
            nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

        call particle_exchange()

        ! interior particles
        call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            et_age_de,et_mass_de,et_comp_de,et_np_de, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
            dx,dy,pfdt,moldiff,denh2o,dtfrac,0,N_inte, &
            nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

        print *, rank, 'ok8'

    end if

    if (mod(kk,(pft2-pft1+1)) == 0)  then
        if(rank /= ppx*qqy) then
            call MPI_FILE_OPEN(MPI_COMM_SELF,restartf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                            MPI_INFO_NULL,fh2,ierr)
            call MPI_FILE_WRITE(fh2,np_active,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_WRITE(fh2,pid,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_WRITE(fh2,P(1:np_active,1:nind*2+17),np_active*(nind*2+17), &
                                MPI_DOUBLE_PRECISION,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_CLOSE(fh2, ierr)

            open(17,file='topology_restart.'//trim(adjustl(ranknum)),FORM='unformatted',access='stream')
                write(17) ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
            close(17)  ! Topology

            open(18,file='Zone_de_restart.'//trim(adjustl(ranknum)),FORM='unformatted',access='stream')
                write(18) Zone_de
            close(18)  ! Zone_de
        else
            ! grid and Zonet_new
            open(19,file='manager_grid_zonet_new',FORM='unformatted',access='stream')
                write(19) grid
                write(19) Zonet_new
            close(19)  ! manager
        end if
    end if

end do !! timesteps and cycles