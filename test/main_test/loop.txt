
!--------------------------------------------------------------------
    call global_xyz() ! on workers
    ! restart file info inside, so have to call it before restart.
    call alloc_arrays_const() ! on workers, and P array needs nind
    ! the above subroutines will work when setting manager.
!--------------------------------------------------------------------
    if(np_ic /= -1) then

        ! if not restart, we do decompostion and build topology
        call gridinfo()

        ! doing this on manager, and then we get subdomain info in grid array
        ! for init, we don't have to build p_sum on each rank, just use that for
        ! the whole domain with init value of 1.
        ! if restart, no need to do this part, directly read necessary info from restart files
            if(rank /= ppx*qqy)then
                open(10,file='grid.'//trim(adjustl(ranknum)))
                    write(10,'(4(4(i3,1x),/))') ((grid(i,j),j=1,4),i=1,ppx*qqy)
                close(10)
            else
                open(10,file='grid.'//trim(adjustl(ranknum)))
                    write(10,'(10(11(i3,1x),/))') ((Zonet_old(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                    write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                    write(10,'(4(4(i3,1x),/))') ((grid(i,j),j=1,4),i=1,ppx*qqy)
                close(10)
            endif

        call MPI_BARRIER(MPI_COMM_WORLD, ierr)
        call copy_grid() ! doing this on workers
            open(10,file='topology.'//trim(adjustl(ranknum)))
                write(10,'(8(i3,1x))') ix1,iy1,nnx1,nny1,ix2,iy2,nnx2,nny2
            close(10)

        call alloc_arrays_temp() ! doing this on workers
            if(rank /= ppx*qqy)then
                open(10,file='Zone_de_before.'//trim(adjustl(ranknum)))
                    write(10,*) Zone_de
                close(10)
            endif

        call receive_Zone_de() ! this needs all the ranks, both manager and workers
            if(rank /= ppx*qqy)then
                open(10,file='Zone_de_after.'//trim(adjustl(ranknum)))
                    write(10,*) Zone_de
                close(10)
            endif
    end if
    ! for following, if there is [if (rank /= ppx*qqy)] in the subroutine, please
    ! just keep it there now, and later we can find a better way.

    if(rank /= ppx*qqy)then
        open(10,file='Restart_before.'//trim(adjustl(ranknum)))
            write(10,'(8(i3,1x))') ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
            write(10,'(19(f15.3,1x))') P(1,:)
        close(10)
    else
        open(10,file='Restart_before.'//trim(adjustl(ranknum)))
            write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
            write(10,*) grid
        close(10)
    endif
!--------------------------------------------------------------------
    if (np_ic == -1) then
        ! if np_ic = -1 then we read a restart file
        ! move restart here is due to we want to do the allocate of temp here which can keep
        ! a clear structure, and then we can read the necessary files such as Ind etc.
        ! But if we do allocate, it needs the subdomain dim etc., so we read restart info here.

        call read_restarts()
        ! this is in the subdomain_bound module, which is in the ecoslim_subdomain_bc file

        ! 1. read ix1 etc. on worker. specific files or in restarts with P
        ! 2. read zones on workers   ???? copy to cpu
        ! 3. read grid and Zonet_old on manager??? Zonet_new
        call alloc_arrays_temp()

        if(rank /= ppx*qqy)then
            open(10,file='Restart_after.'//trim(adjustl(ranknum)))
                write(10,'(8(i3,1x))') ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
                write(10,'(19(f15.3,1x))') P(1,:)
                write(10,*) Zone_de
            close(10)
        else
            open(10,file='Restart_after.'//trim(adjustl(ranknum)))
                write(10,'(10(11(i3,1x),/))') ((Zonet_new(i,j),i=-buff+1,nx+buff),j=-buff+1,ny+buff)
                write(10,*) grid
            close(10)
        endif

        call read_Zone_de()
        if(rank /= ppx*qqy)then
            open(10,file='Restart_after2_Zone_de.'//trim(adjustl(ranknum)))
                write(10,*) Zone_de
            close(10)
        endif
    end if
!--------------------------------------------------------------------
    if (rank /= ppx*qqy) then
        call local_xyz()

        if(rank == 0) then
            flush(11)
            close(11)
        endif

        Porosity = 0.5d0

        if(np_ic /= -1) then
            Saturation = 0.8d0
            call createRand_init<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>>(nx,ny,nz, &
            rank,np_ic,nnx1,nny1)
        endif

        print *, 'ok2'
        dz_de  = dz2 ! I can't understand why dz doesn't work.
        Ind_de = Ind
        Porosity_de   = Porosity
        Saturation_de = Saturation
        P_de = P

        ! dz2  = dz_de ! I can't understand why dz doesn't work.
        ! Ind = Ind_de
        ! Porosity   = Porosity_de
        ! Saturation = Saturation_de
        ! P = P_de
        ! open(10,file='vari.'//trim(adjustl(ranknum)))
        !     write(10,*) dz2
        !     write(10,*) Ind
        !     write(10,*) Porosity
        !     write(10,*) Saturation
        !     write(10,*) P(1:10,:)
        ! close(10)

        ! Define initial particles' locations and mass
        if (np_ic > 0)  then
            np_active = 0
            pid = 0
            if (np_active + np_ic*nnx1*nny1*nz >= np ) then
                write(message,'(a,a)') ' **Warning IC input but no paricles left', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                write(message,'(a,a)') ' **Exiting code gracefully writing restart', new_line(' ')
                call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                goto 9090
            endif

            call add_init_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB), &
            tPB >>> (P_de,np_ic,nind,denh2o,tout1,nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de)

            np_active = np_ic*nnx1*nny1*nz
            pid = np_ic*nnx1*nny1*nz

        end if

        open(10,file='P_init.'//trim(adjustl(ranknum)))
            write(10,*) np_active
            write(10,*) pid
            P = P_de
            write(10,'(200(19(f20.5,1x),/))') ((P(i,j),j=1,17+2*nind),i=1,np_active)
        close(10)
        !--------------------------------------------------------------------
        ! keep the following output as it is. we can figure out a better way to handle them later.
        call MPI_FILE_OPEN(MPI_COMM_SELF,logf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh4,ierr)

        write(message,'(a,a)') ' **** Transient Simulation Particle Accounting ****', new_line(' ')
        call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

        write(message,'(a,a)') ' Timestep PFTimestep OutStep Time Mean_Age Mean_Comp Mean_Mass Total_Mass PrecipIn &
                                ETOut NP_PrecipIn NP_ETOut NP_QOut NP_active_old NP_filtered', new_line(' ')
        call MPI_FILE_WRITE(fh4,trim(message),len(trim(message)),MPI_CHARACTER,MPI_STATUS_IGNORE,ierr)

        ! open exited particle file and write header
        call MPI_FILE_OPEN(MPI_COMM_SELF,exitedf,MPI_MODE_WRONLY+MPI_MODE_CREATE,MPI_INFO_NULL,fh3,ierr)
        print *, 'ok3'
        if(rank == 0) then

            ! if(ipwrite < 0) then
            ! ! open/create/write the 3D output file which will write particles out each timestemp, very slowly in parallel
            !     open(214,file=trim(runname)//'_total_particle_trace.3D')
            !     write(214,*) 'X Y Z TIME'
            ! end if ! ipwrite < 0
            ! this write is originally in the particle loop. now particle loop is on GPU. This can not be reserved.

            open(13,file=trim(runname)//'_ET_output.txt')
            write(13,*) 'TIME ET_age ET_comp1 ET_comp2 ET_comp3 ET_mass ET_Np'

            open(15,file=trim(runname)//'_flow_output.txt')
            write(15,*) 'TIME Out_age Out_comp1 outcomp2 outcomp3 Out_mass Out_NP'

            open(16,file=trim(runname)//'_PET_balance.txt')
            write(16,*) 'TIME P[kg] ET[kg]'

        endif

        call scan_zone<<<ceiling(dble((nnx1+2*buff)*(nny1+2*buff))/tPB), &
        tPB>>>(nnx1,nny1,buff,d_isValid,rank,t_rank-1)
        ! ix2, iy2, nnx2, and nny2 are only used for reading files
        print *, 'ok4'
        neigh_list = d_isValid(1:t_rank-1)
        print *, 'ok5'
        if(sum(neigh_list) > 0) then
            allocate(N_recv(sum(neigh_list)),N_send(sum(neigh_list)))
            N_send = 0; N_recv = 0
        end if
        ! don't have to do this every timestep, only after the update of decomposition

        open(10,file='neighbor.'//trim(adjustl(ranknum)))
            write(10,*) neigh_list
            write(10,*) N_recv
        close(10)

        ! Intialize cuRand device API and this part need to check through again.
        call createRand_loop<<< ceiling(dble(np_active)/tPB),tPB >>>(np, rank, pfnt)

    endif
    print *, 'ok6'
!--------------------------------------------------------------------

pfkk = mod((outkk-1),(pft2-pft1+1))+pft1-1    ! outkk is tout1+1

do kk = outkk, pfnt
    if (rank /= ppx*qqy) then

        ! reset ParFlow counter for cycles
        if (mod((kk-1),(pft2-pft1+1)) == 0 )  pfkk = pft1 - 1
        ! adjust the file counters
        pfkk = pfkk + 1
        ! Read the velocities computed by ParFlow
        write(filenum,'(i5.5)') pfkk
        Vx = 1.d-2
        Vy = 1.d-2
        Vz = 1.d-2
        Saturation = 0.8d0

        if (clmtrans) then
            ! Read in the Evap_Trans
            EvapTrans = 0.d0
            EvapTrans(2,:,5) = 1.d-8

            if (clmfile) then
                CLMvars = 0.d0
            end if
        end if

        ! Determine whether to perform forward or backward patricle tracking
        Vx = Vx * V_mult
        Vy = Vy * V_mult
        Vz = Vz * V_mult

        out_age_cpu  = 0.d0
        out_mass_cpu = 0.d0
        out_comp_cpu = 0.d0
        out_np_cpu   = 0
        et_age_cpu   = 0.d0
        et_mass_cpu  = 0.d0
        et_comp_cpu  = 0.d0
        et_np_cpu    = 0
        mean_age     = 0.d0
        mean_comp    = 0.d0
        total_mass   = 0.d0

        Vx_de = Vx
        Vy_de = Vy
        Vz_de = Vz
        Saturation_de = Saturation
        EvapTrans_de  = EvapTrans
        CLMvars_de    = CLMvars(:,:,11)
        ! probably this can be deallocated after adding of particles
        ! Now just think about the hourly add of particles.
        out_age_de  = out_age_cpu
        out_mass_de = out_mass_cpu
        out_comp_de = out_comp_cpu
        out_np_de   = out_np_cpu

        et_age_de  = et_age_cpu
        et_mass_de = et_mass_cpu
        et_comp_de = et_comp_cpu
        et_np_de   = et_np_cpu

        mean_age_de   = mean_age
        mean_comp_de  = mean_comp
        total_mass_de = total_mass

        C    = 0.d0
        C_de = C
        ! We can think about asyn transfer of data to GPU here to hide the time cost when doing
        ! the following adding of particles.
        !----------------------------------------
        if (clmtrans) then
            if (np_active < np) then

                call scan_new_particles<<< ceiling(dble(nnx1*nny1*nz)/tPB),tPB >>> ( &
                PET_balance_de,d_isValid,nnx1,nny1,nz,dx,dy,dz_de,pfdt,denh2o)
                ! d_isValid has the length as P or of np which is assigned in input script
                ! so nnx1**nny1*nz should be smaller than np
                ! we don't care about the buffer zone for add of particles
                ! nnx1, nnx2 etc. subdomain info can be put in its own module. Then use module
                ! in main program and pass arguments to other subroutines.

                ! after call the kernel ‘scan_new_particles’, using d_isValid to scan
                call thrustscan(d_isValid,nnx1*nny1,d_indices)

                i_added_particles = d_indices(nnx1*nny1) * iflux_p_res ! should be legal?
                if(np_active + i_added_particles >= np) then
                    write(message,'(A,A)') ' **Warning rainfall input but no paricles left', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)

                    write(message,'(A,A)') ' **Exiting code gracefully writing restart', new_line(' ')
                    call MPI_FILE_WRITE(fh1, trim(message), len(trim(message)), &
                    MPI_CHARACTER, MPI_STATUS_IGNORE, ierr)
                    goto 9090
                endif

                call add_new_particles<<< ceiling(dble(nnx1*nny1)/tPB), &
                tPB >>> (d_indices,P_de,iflux_p_res,np_active,pid,nind, &
                nnx1,nny1,nz,ix1,iy1,dx,dy,dz_de,pfdt,denh2o,kk,tout1)

                np_active = np_active + d_indices(nnx1*nny1) * iflux_p_res
                pid = pid + d_indices(nnx1*nny1) * iflux_p_res

            end if
        end if

        ! this has to be checked later.
        ! call MPI_ALLReduce(MPI_IN_PLACE, PET_balance_da(kk,:), 2,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr)
        PET_balance = PET_balance_de
        call MPI_ALLReduce(MPI_IN_PLACE,PET_balance,2,MPI_DOUBLE_PRECISION,MPI_SUM,work_comm,ierr)
        ! print to log file, or PET_balance can be a long array and do print at last

        print *, rank, 'ok7', PET_balance

        call particles_separation<<<ceiling(dble(np_active)/tPB),tPB>>> ( &
            P_de,d_isValid,xmin3,ymin3,xmax3,ymax3,np_active,nind,ix1,iy1,dx,dy)
        call thrustscan(d_isValid,np_active,d_indices)
        N_peri = np_active - d_indices(np_active)
        allocate(holes(N_peri))
        call prepare_holes<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,np_active)
        call select2np_active<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            holes,P_de,np_active,N_peri)
        call compaction_inplace<<<ceiling(dble(np_active)/tPB),tPB>>>( &
            holes,d_indices,d_isValid,P_de,0,np_active)
        N_inte = d_indices(np_active); deallocate(holes)
        call connect_recv<<<ceiling(dble(N_peri)/tPB),tPB>>>( &
            P_de,N_inte,N_peri)

        ! peripheric particles
        call particles_independent<<<ceiling(dble(N_peri)/tPB),tPB>>> ( &
            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            et_age_de,et_mass_de,et_comp_de,et_np_de, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin2,ymin2,zmin2,xmax2,ymax2,zmax2, &
            dx,dy,pfdt,moldiff,denh2o,dtfrac,N_inte,N_peri, &
            nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

        call particle_exchange()

        ! interior particles
        call particles_independent<<<ceiling(dble(N_inte)/tPB),tPB>>> ( &
            P_de,dz_de,out_age_de,out_mass_de,out_comp_de,out_np_de, &
            et_age_de,et_mass_de,et_comp_de,et_np_de, &
            xgmin,ygmin,zgmin,xgmax,ygmax,zgmax, &
            xmin1,ymin1,zmin1,xmax1,ymax1,zmax1, &
            dx,dy,pfdt,moldiff,denh2o,dtfrac,0,N_inte, &
            nind,ix1,iy1,nnx1,nny1,nz,reflect,rank)

        print *, rank, 'ok8'

    end if

    if (mod(kk,(pft2-pft1+1)) == 0)  then
        if(rank /= ppx*qqy) then
            call MPI_FILE_OPEN(MPI_COMM_SELF,restartf,MPI_MODE_WRONLY+MPI_MODE_CREATE, &
                            MPI_INFO_NULL,fh2,ierr)
            call MPI_FILE_WRITE(fh2,np_active,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_WRITE(fh2,pid,1,MPI_INTEGER,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_WRITE(fh2,P(1:np_active,1:nind*2+17),np_active*(nind*2+17), &
                                MPI_DOUBLE_PRECISION,MPI_STATUS_IGNORE,ierr)
            call MPI_FILE_CLOSE(fh2, ierr)

            open(17,file='topology_restart.'//trim(adjustl(ranknum)),FORM='unformatted',access='stream')
                write(17) ix1, iy1, nnx1, nny1, ix2, iy2, nnx2, nny2
            close(17)  ! Topology

            open(18,file='Zone_de_restart.'//trim(adjustl(ranknum)),FORM='unformatted',access='stream')
                write(18) Zone_de
            close(18)  ! Zone_de
        else
            ! grid and Zonet_new
            open(19,file='manager_grid_zonet_new',FORM='unformatted',access='stream')
                write(19) grid
                write(19) Zonet_new
            close(19)  ! manager
        end if
    end if

end do !! timesteps and cycles